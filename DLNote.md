# 深度学习笔记
## 线性回归
线性回归，个人理解为从一组数据(称为训练数据集或训练集)中抽取某个特定或随机的数据(称为样本或数据点)，每个样本可以表示为一个向量的形式，各个分量表示样本中的某个特征。通过给予每个分量一定的权重（在向量运算中，其实是计算权重向量和样本向量的内积，结果是一个标量），来逼近样本的真实值。如果从输入输出的角度来看待，那么输入就可以看作是一个样本，输出则是这个样本所对应的价值（或者说，在线性回归函数中的$\hat{y}$值）。
输入可以看待为一组向量的形式，形式为:
$$
x^{(i)}=[x_1^{(i)}, x_2^{(i)}]^{T}
$$
通过这样的输入，我们可以得到相应的输出$y^{(i)}$。整个式子可以表达为:
$$
\hat{y} = w_{(1)}x_{(1)} + w_{(2)}x_{(2)} + ... + w_{(n)}x_{(n)} + b
$$
其中，$w$就是权重，b就是偏置量。为什么一定需要偏置量呢？是因为我们需要考虑，在输入向量的所有取值都为0时，即使现实情况中可能不会出现这样的极端取值，我们仍需要考虑线性回归在极端条件下的表现。换句话说，如果没有偏置项，我们的模型的表达能力将受到限制。
> 在线性回归中，我们希望找到一个线性函数来描述自变量和因变量之间的关系。这个线性函数的形式可以表示为：
$$
\hat{y} = w_{(1)}x_{(1)} + w_{(2)}x_{(2)} + ... + w_{(n)}x_{(n)} + b
$$
\
其中$y$是因变量，$x1, x2, ..., xn$是自变量，$w1, w2, ..., wn$是对应的权重，$b$是偏置项。
\
这个式子还可以用向量的形式来表达，表达为:
$$
\hat{y}=w^{T}x + b
$$
\
偏置项b在这个方程中的作用是调整模型的截距，它表示在自变量为0时的因变量的值。在实际问题中，往往不存在所有自变量都为0的情况，但是偏置项可以保证我们的模型在所有自变量都为0时也有一个合理的预测结果。
因此，偏置项在线性回归模型中是必不可少的。

注意到，我们的权重向量$w$（也许这就是参数向量？）是通用的。也就是说，如果我们针对某个数据集求得了合适的$w$，那么这个$w$对于数据集中的所有样本的总误差应该都最小。如果将数据集记为$\Bbb X$，即对于任意的样本$x$，都有$x \in \Bbb X$。那么我们可以通过如下公式来求得整个数据集对应的输出结果$\Bbb{\hat{Y}}$:
$$
\Bbb{\hat{Y}} = \Bbb{X}w+b
$$
<font color="red">再次说明，线性回归的目标就是找到一个合适的权重向量$w$和一个合适的偏置量(这是一个标量)$b$，使得对于任意$x \in \Bbb X$，它的输出误差尽可能小。</font>

### 损失函数
假设我们从数据集中抽取样本$x^{(i)}$，通过线性回归后输出是$\hat{y^{(i)}}$，其对应的真实价值是$y^{(i)}$。那么均方误差可以定义为如下的公式:
$$
l^{(i)}(w, b) = \frac{1}{2}(\hat{y^{(i)}} - y^{(i)})^2
$$
我们需要对这个函数进行一些解释。$l^{(i)}(w, b)$是我们的均方误差损失函数，它的参数就是我们的权重向量$w$和偏置项$b$，这些参数都被用来计算我们的估计值$\hat{y^{(i)}}$。\
随后，$\frac{1}{2}$是一个常数项，它其实不会改变我们对误差的估计。因为如果我们对每一项求均方误差时都乘以这个系数，那么最终误差和的大小和各项之间的误差大小关系其实并不会有改变，引入这个系数的原因在于。<font color="red">在我们最小化均方误差时，我们需要对损失函数求导。而均方误差损失函数包含了一个平方项，引入该系数后可以在求导后使得系数为1，方便计算。</font>\
最后面一部分就是我们的估计值和真实值之间的误差了，这里利用了最小二乘法的思想，计算结果必然是一个非负整数，我们用其来表达误差，不论估计值相对真实值是大是小。
> 您的理解基本正确，不过还有一些补充。
1. 均方误差损失函数是一个凸函数，这意味着在参数空间中只存在一个全局最小值。这也是为什么使用均方误差作为损失函数的一个重要原因。
2. 在实际使用中，我们通常会对所有样本的损失函数取平均或者求和，得到整个数据集上的损失函数。这样可以更好地反映整体的误差情况，同时也避免了样本数量不同导致的问题。\
3. 最小二乘法是一种求解线性回归的方法，它的目的是找到能够最小化所有样本的均方误差的权重向量 $w$ 和偏置项 $b$。我们通过求解损失函数的梯度，使用梯度下降等优化算法来更新模型参数，以达到最小化损失函数的目的。

---
如果希望度量我们的线性回归模型在整个数据集上面的表现，则需要求得数据集中$n$的样本的误差和，根据总体误差和的大小来判断模型的表现:
$$
\mathcal{L}(w,b)={\frac{1}{n}\sum_{i=1}^{n}{l^{(i)}(w,b)}}=
{\frac{1}{n}\sum_{i=1}^{n}{\frac{1}{2}(w^{T}x^{(i)}+b-y^{(i)})^2}}
$$
<font color="green">有趣的是，在强化学习中损失函数的计算中。如果我们将一个状态的特征看作一个样本。一个环境中整个状态空间中的所有状态的特征看作是数据集。那么除了上述的$\frac{1}{n}$计算误差和的形式。由于不同的状态在不同的策略下具有不同的重要性，所以强化学习中使用了平稳分布的概念，给予每个状态的权重不同。</font>

### 线性回归函数的解析解
如果一个解能够用一个公式简单的表达出来，那么称其为解析解。回顾一下我们线性函数的矩阵向量形式:
$$
\Bbb{\hat{Y}} = \Bbb{X}w+b
$$
如果将偏置项b作为一个列向量添加到参数向量w的第二列，使其成为一个$n + 1$行的行向量，将数据集$\Bbb{X}$增加一列，将这一行全部置为1。则线性回归模型可以写作:
$$
\Bbb{\hat{Y}} = \Bbb{X}w
$$
其中，$\Bbb{X}$和$w$分别是:
$$
\Bbb{X}=
\left(
\begin{matrix}
{x_1^1} & {x_2^{(1)}} & \cdots & {x_n^{(1)}} & 1\\
\vdots & \vdots & \vdots & \vdots & 1\\
{x_1^{(n)}} & {x_2^{(n)}} & \cdots & {x_n^{(n)}} & 1\\
\end{matrix}
\right)
\quad 
w=
\left(
\begin{matrix}
w_1  \\
\vdots \\
w_n \\
b
\end{matrix}
\right)
$$
那么，我们的预测问题的目的就是最小化$||Y- \Bbb{\hat{Y}}||^2$，也就是最小化$||Y-Xw||^2$。由于损失函数是一个凸函数，所以它只具有一个全局极小值点。
><font color="red">为什么损失函数是一个凸函数?</font>
在线性回归中，我们通常使用均方误差（MSE）作为损失函数，其定义为：
$$
L(w,b) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - \hat{y}^{(i)})^2 = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - (w^Tx^{(i)} + b))^2
$$
其中，$n$是样本数，$x^{(i)}$是第$i$个样本的特征向量，$y^{(i)}$是第$i$个样本的真实标签，$\hat{y}^{(i)}$是线性模型对第$i$个样本的预测值，$w$和$b$是模型的参数。
\
现在我们来证明一下MSE是一个凸函数。为了方便证明，我们令$\hat{y}^{(i)} = w^Tx^{(i)} + b$。
\
首先，我们可以计算损失函数关于$w$的Hessian矩阵：
$$
H(w,b) = \begin{bmatrix} \frac{\partial^2 L(w,b)}{\partial w_1^2} & \frac{\partial^2 L(w,b)}{\partial w_1 \partial w_2} & \cdots & \frac{\partial^2 L(w,b)}{\partial w_1 \partial b} \\ \frac{\partial^2 L(w,b)}{\partial w_2 \partial w_1} & \frac{\partial^2 L(w,b)}{\partial w_2^2} & \cdots & \frac{\partial^2 L(w,b)}{\partial w_2 \partial b} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 L(w,b)}{\partial b \partial w_1} & \frac{\partial^2 L(w,b)}{\partial b \partial w_2} & \cdots & \frac{\partial^2 L(w,b)}{\partial b^2} \end{bmatrix}
$$
\
其中，$H_{ij}(w,b) = \frac{\partial^2 L(w,b)}{\partial w_i \partial w_j}$。
\
我们可以发现，$H(w,b)$是一个对称矩阵。现在我们来证明$H(w,b)$是一个半正定矩阵。
\
对于任意向量$v=[v_1, v_2, \cdots, v_n]^T$，我们可以计算：
$$
v^T H(w,b) v = \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_i v_j
$$
根据二次型的定义，我们可以将上式变为：
$$
v^T H(w,b) v = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_i v_j + \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_j v_i
$$
好的，接下来我会继续完成证明过程。

根据偏导数的定义，我们可以得到：
$$
\frac{\partial^2 L(w,b)}{\partial w_i \partial w_j} = \frac{\partial}{\partial w_j} \frac{\partial L(w,b)}{\partial w_i} = \frac{\partial}{\partial w_j} \frac{1}{2m} \sum_{k=1}^{m} (\hat{y}^{(k)} - y^{(k)}) x_j^{(k)} x_i^{(k)} \\
= \frac{1}{2m} \sum_{k=1}^{m} x_j^{(k)} \frac{\partial \hat{y}^{(k)}}{\partial w_i} = \frac{1}{m} \sum_{k=1}^{m} x_j^{(k)} x_i^{(k)}
$$
其中，我们使用了$\hat{y}^{(k)} = w^Tx^{(k)} + b$以及$L(w,b)$的定义。因此，$H_{ij}(w,b) = \frac{1}{m} \sum_{k=1}^{m} x_j^{(k)} x_i^{(k)}$。

接下来，我们继续计算$v^T H(w,b) v$：
$$
v^T H(w,b) v = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_i v_j + \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_j v_i \\
= \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{1}{m} \sum_{k=1}^{m} x_j^{(k)} x_i^{(k)} v_i v_j + \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{1}{m} \sum_{k=1}^{m} x_j^{(k)} x_i^{(k)} v_j v_i \\
= \frac{1}{m} \sum_{k=1}^{m} \sum_{i=1}^{n} \sum_{j=1}^{n} x_j^{(k)} x_i^{(k)} \frac{v_i v_j + v_j v_i}{2} \\​
= \frac{1}{m} \sum_{k=1}^{m} \sum_{i=1}^{n} \sum_{j=1}^{n} x_j^{(k)} x_i^{(k)} (v_i v_j) \\
= \frac{1}{m} \sum_{k=1}^{m} (\sum_{i=1}^{n} x_i^{(k)} v_i)^2 \geq 0
$$
接下来，我们使用了$x_j^{(k)}$和$v_j$的对称性质，将上式进一步简化为：
$$
v^T H(w,b) v = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) (v_i v_j + v_j v_i)
$$
注意到对于任意的$i,j$，有$H_{ij}(w,b) = H_{ji}(w,b)$，所以可以继续化简：
$$
v^T H(w,b) v = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) 2v_i v_j = \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_i v_j
$$
因此，$v^T H(w,b) v = \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij}(w,b) v_i v_j \geq 0$，证明了$H(w,b)$是一个半正定矩阵。因此，MSE损失函数是凸函数。