---
title: "Reinforcement Learning note"
author: Yu Xia
date: March 28, 2023
output: pdf_document
---

> 以下公式都仅针对于**马尔科夫决策过程**

# 状态价值与行为价值的关系
## 状态序列
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3}+... 
$$
## 状态价值
$$
\begin{align}
v_\pi(s) &= \Bbb E[G_t\mid S_t=s] \\
&= \Bbb E[R_{t+1} + \gamma v_{\pi}(S_{t+1})\mid S_t = s] \\
\end{align}\\
如果用矩阵的形式来描述这个式子，那么就有:\\
\\
v=R + \gamma Pv\\
其中P就代表了对于后续状态价值求期望 \\
进行矩阵运算即可得到:\\
v=(I-\gamma P)^{-1}R
$$

## 行为价值
$$
\begin{align}
q_{\pi}(s,a) &= \Bbb E[G_t \mid S_t = s, A_t = a]\\
&=\Bbb E[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) \mid S_t = s, A_t = a]
\end{align}
$$

## 状态价值与行为价值
$$
一个状态的状态价值可以用该状态下所有行为价值来表达:\\
v_\pi(s) = \sum_{a \in A}{\pi(a|s)q_{\pi}(s,a)} \\
一个行为的行为价值可以用该行为能达到的后续状态价值来表达:\\
q_{\pi}(s,a)=R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a{v_{\pi}(s^{'})}}\\
那么，一个状态的状态价值可以表示为:\\
v_\pi(s) = \sum_{a \in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a{v_{\pi}(s^{'})}})\\
一个行为的行为价值可以表示为:\\
q_{\pi}(s,a)=R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}(\sum_{a \in A}{\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'})})
$$
具体的描述一下第三个公式，首先它需要几个条件:
1. 这个状态的状态值基于策略$\pi$
2. 该策略下基于状态$s$有一到多个行为$a$，采取这些行为后到达的状态$s^{'}$是具有概率性的，也就是说采取行为后到达的状态是不确定的。

那么，最后这个公式可以描述为:
一个状态的状态价值基于给定的策略$\pi$，它表示该策略下所有可能的动作获得的即时奖励，加上采取该动作后到达下一步状态的概率乘以下一步状态的状态价值乘以折现率。
再换句话说，一个状态的状态价值等于它当前所有可能行为的即时奖励，加上采取某一行为后后续状态价值的期望。

