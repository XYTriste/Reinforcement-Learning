---
title: "Reinforcement Learning note"
author: Yu Xia
date: March 28, 2023
output: pdf_document
---

> 以下公式都仅针对于**马尔科夫决策过程**

# 状态价值与行为价值的关系
## 状态序列
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3}+... 
$$
## 状态价值
$$
\begin{align}
v_\pi(s) &= \Bbb E[G_t\mid S_t=s] \\
&= \Bbb E[R_{t+1} + \gamma v_{\pi}(S_{t+1})\mid S_t = s] \\
\end{align}\\
如果用矩阵的形式来描述这个式子，那么就有:\\
\\
v=R + \gamma Pv\\
其中P就代表了对于后续状态价值求期望 \\
进行矩阵运算即可得到:\\
v=(I-\gamma P)^{-1}R
$$

## 行为价值
$$
\begin{align}
q_{\pi}(s,a) &= \Bbb E[G_t \mid S_t = s, A_t = a]\\
&=\Bbb E[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) \mid S_t = s, A_t = a]
\end{align}
$$

## 状态价值与行为价值
$$
一个状态的状态价值可以用该状态下所有行为价值来表达:\\
v_\pi(s) = \sum_{a \in A}{\pi(a|s)q_{\pi}(s,a)} \\
一个行为的行为价值可以用该行为能达到的后续状态价值来表达:\\
q_{\pi}(s,a)=R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a{v_{\pi}(s^{'})}}\\
那么，一个状态的状态价值可以表示为:\\
v_\pi(s) = \sum_{a \in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a{v_{\pi}(s^{'})}})\\
一个行为的行为价值可以表示为:\\
q_{\pi}(s,a)=R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}(\sum_{a \in A}{\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'})})
$$
具体的描述一下第三个公式，首先它需要几个条件:
1. 这个状态的状态值基于策略$\pi$
2. 该策略下基于状态$s$有一到多个行为$a$，采取这些行为后到达的状态$s^{'}$是具有概率性的，也就是说采取行为后到达的状态是不确定的。

那么，最后这个公式可以描述为:
一个状态的状态价值基于给定的策略$\pi$，它表示该策略下所有可能的动作获得的即时奖励，加上采取该动作后到达下一步状态的概率乘以下一步状态的状态价值乘以折现率。
再换句话说，一个状态的状态价值等于它当前所有可能行为的即时奖励，加上采取某一行为后后续状态价值的期望。

> 总结:
> 一个状态的状态价值，等于该状态采取某个行为获得的即时奖励加上以该状态为起点的后续所有可能的状态的价值的期望。
> 一个行为的行为价值，同样可以看作是行为获得的即时奖励，加上该行为到达某状态的概率乘以该状态的状态价值。

## 最优状态价值函数与最优行为价值函数
### 最优状态价值函数
最优状态价值函数指的是在状态$s$下，所有的策略中产生的状态价值中最大者:
$$
v_* = \max_{\pi}v_{\pi}(s)
$$
换句话说，如果某个策略$\pi$使得对于任意状态$s$，都有$v_{\pi}(s) >= 其他任意策略下的状态价值$，那么就称这个策略是最优策略，使用了最优策略的状态价值函数称为最优状态价值函数。

## 最优行为价值函数
最优行为价值函数指的是在状态$s$下，所有策略产生的行为中行为价值最大者。
$$
q_*(s,a) = \max_{\pi}q_{\pi}(s,a)
$$
> 再提一次，行为价值依赖于状态$s$下采取的行为获得的即时奖励，以及该行为可能达到的所有后续状态的概率(由状态转移概率矩阵确定)乘以该后续状态的状态价值。

从公式中可以看出，最优行为价值函数也和策略的选择息息相关。最优行为价值来自于最优的策略，最优策略使得在任何一个状态$s$下选择某个行动$a$获得的行为价值最大。
同时，最优策略可以通过最大化行为价值函数来获得。也就是说，假如我们处于任意状态$s$，此时采取行动$a$获得的行为价值$q(s,a)$均是最大的。那么我们的最优策略$\pi$就应该在这个状态下总是选择这个行为，即$\pi(s,a) = 1$.
解决强化学习问题意味着需要寻找一个最优策略，使得该策略下的return最大。而我们最优策略可以通过最大化行为价值函数来获得，因此解决强化学习问题就变成了求解最优行为价值函数的问题。

最优行为价值函数可以换一种表示形式，表示为：
$$
q_*(s,a) = \Bbb E[R_{t+1} + \gamma v_*(S_{t+1})\mid S_t = s,A_t = a] 
$$
也就是说，最优行为价值可以用该行为获得的即时奖励加上后续达到的状态价值的期望得到。

# 动态规划寻找最优策略
预测是对给定策略的评估过程，控制是寻找一个最优策略的过程。\
<span id = "predAndCon"></span>
**预测($prediction$)**:已知一个马尔科夫决策过程$<S,A,P,R, \gamma>$和一个策略$\pi$，或者是给定一个马尔科夫奖励过程$<S,P_\pi,R_\pi, \gamma>$，求解基于该策略的价值函数$v_\pi$
**控制($control$)**:已知一个马尔科夫决策过程$<S,A,P,R, \gamma>$，求解最优价值函数$v_*$和最优策略$\pi_*$。 

## 策略评估
策略评估指的是计算给定策略下状态价值函数的过程。\
使用上一个迭代周期$k$内的后续状态价值来计算更新当前迭代周期$k+1$内某状态$s$的价值(听起来像雅可比迭代法):
$$
v_{k+1}(s)=\sum_{a \in A}{\pi(a|s)(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}v_{k}(s^{'}))}
$$

## 策略迭代
策略迭代指的是完成一定轮次的策略评估之后，此时我们得到了所有的状态的状态价值。那么我们就可以通过新的状态价值来更新我们的策略，使得我们的策略在某个状态时更倾向于前往状态价值更大的状态。然后，我们使用更新后的策略进行策略评估，更新我们的状态价值函数，重复这个过程。最终，它们都会收敛到最优价值函数和最优策略。

## 价值迭代
一个策略，如果是最优策略，那么它在某个状态下一定能够产生当前状态下的最优行为。而且通过当前状态下的最优行为到达的后续状态。该策略同样是最优的。
或者换句话说，如果一个策略对于某个状态产生的行为它不是最优行为，那么这个策略就不是一个最佳策略。
>一个策略能够获得某状态的最优价值当且仅当该策略也同时获得状态所有可能的后续状态的最优价值。
>对于这句话的理解就是，一个策略获得某状态的最优价值。说明该策略在该状态下产生的行为是一个最优价值行为，最优价值行为就是由当前行为的奖励加上行为导致的后续状态价值的期望组合而成。
也就是有:
$$
v_{*}(s)=\max_{a\in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}v_{*}(s^{'}))
$$

### 价值迭代与策略迭代的区别
**价值迭代算法中迭代的是状态价值函数 $v(s)$，以逼近最优状态价值函数 $v^*(s)$。**

>与策略迭代不同，价值迭代算法**并不需要显式地维护一个策略**。在每次更新状态价值函数后，我们都会自动进行一次最优化策略改进，即选择当前能够使下一个状态价值最大化的行动作为最优策略的行动。因此，价值迭代算法通常会比策略迭代更加高效，因为它将策略评估和策略改进两个过程集成到了一个算法中，无需反复地进行迭代。
>另外，与策略迭代相比，价值迭代算法通常更容易实现和理解，并且可以处理具有连续状态和行动空间的问题。但它也有一些缺点，例如可能会收敛得比较慢，特别是在状态空间很大的情况下。此外，如果状态或者行动空间非常大，那么存储和计算状态价值函数的成本也可能会很高。
>
>书上描述的二者的差距：策略迭代每次迭代仅计算相关状态的价值，一次计算即得到最优状态价值。后者在每次迭代时要更新所有状态的价值。
>这里的"一次计算即得到最优状态价值"指的是基于当前的所有策略中选择了最优的策略计算出的最优状态价值，但是策略迭代同时也需要改善策略。所以根据最优的状态价值还会更新策略继续下一次的策略迭代。
>而价值迭代是基于各个状态各自能够采取的行动，采取最优行为来更新自己的状态价值函数。也就是说，每次迭代完毕后，我们根据各个状态状态价值的大小选择出来的路径就是最优策略。
---
>chatGPT给出的描述:
>在策略迭代中，首先进行一定次数的策略评估得到当前策略下状态的价值函数，然后通过策略改进更新策略，再重新进行策略评估得到新的状态价值函数。这个过程不断重复，直到算法收敛为止。策略迭代的优点是它能够针对特定的策略进行优化，逐步提高策略的质量。
而在价值迭代中，每轮迭代会更新所有状态的价值函数，不需要显式地保留策略。价值迭代通常需要多次迭代才能达到最优解，但是一旦完成，从任意状态出发直接选择状态价值最大的状态就可以得到最佳策略的轨迹。因此，价值迭代较为适用于解决具有有限动作空间、可离散化状态空间的强化学习问题。
总之，策略迭代和价值迭代都是常用的增量式策略优化方法，在具体应用时需要根据问题的特性选择合适的算法。
---
价值迭代过程中，价值函数更新的公式为:
$$
v_{k+1}(s)=\max_{a \in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}v_{k}(s^{'}))
$$
这个公式和策略评估的公式非常相似，区别在于策略评估需要根据策略考虑当前状态下所有行为的概率，而价值迭代中仅考虑行为价值最大的行为。
以上就是同步动态规划的内容，迭代法策略评估属于[预测](#predAndCon)问题，使用贝尔曼期望方程进行求解。而策略迭代和价值迭代属于[控制](#predAndCon)问题。

# 蒙特卡洛强化学习
蒙特卡洛强化学习的特点就是在我们不了解状态转移概率的情况下，通过得到一个**完整的状态序列**，来估计状态的真实价值，并认为**某个状态的价值**等于**在多个状态序列中以该状态为起点得到的所有收获的平均**。

> **完整的状态序列**指的是:当状态序列的最后一个状态是终止状态时，称该状态序列是一个**完整的状态序列**。
> 这段话的意思是，我们首先通过某种方法得到一个完整的状态序列。然后，我们计算状态序列中所有以非结束状态为起点的状态的return，并用这个return来更新这个起点状态的状态价值。

例如如下的一个状态序列:
$$
S_1,A_1,R_1,S_2,A_2,...,S_t,A_t,R_{t+1},...,S_k
$$
对于该状态序列，它在$t$时刻的状态$S_t$的$return$可以表示为:
$$
G_t=R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1}R_T
$$
该策略下某一状态$s$的价值为:
$$
v_\pi(s)=\Bbb E_\pi[G_t\mid S_t = s]
$$
> 再次提醒，这里的$G_t$仍然是不固定的，因为从状态$S_t$能够转移到的后续状态可能不止一个。
> 所以最后这个公式估计的是，以状态$S_t$为起点所有的$G_t$的return求**平均值**。
> (例如我们有两条完整的状态序列，在状态$s$时的return分别是$1$和$2$，那么状态$s$的价值就是它们的均值，即:$(1 + 2) / 2 = 1.5$)

注意到，在同一个状态序列中，某一个状态可能出现多次（例如浏览手机状态选择继续浏览的动作，则下一个状态仍然是浏览手机。出现多次的状态不一定连续）。显然，不同位置出现的相同状态为起点的return也是不一样的。既然我们需要使用某状态的return更新它的状态价值，那么需要应对这两种情况，所以就有:
1. 首次访问($first\ visit$):仅统计状态序列中首次出现该状态的位置的return，使用它来更新状态价值，后续再次出现该状态时则忽略。
2. 每次访问($every\ visit$):针对一个状态序列中每次出现的该状态，都计算其return值并更新状态价值。

蒙特卡洛方法中一个状态的状态价值是根据给定的所有状态序列中每条序列中该状态的return求和再平均得到。这就意味着我们最终得到一个状态的状态价值时需要记录下该状态所有的return，这样未免太麻烦。所以我们有一种名叫**累进更新平均值**($incremental\ mean$)的方法，在每次计算出新的某状态的return时，用它来更新状态价值，则无需记录历史所有状态价值求和再平均:
$$
\mu_k=\mu_{k-1}+{1\over k}(x_k-\mu_{k-1})
$$
其中，$\mu_k$表示更新后的状态价值，$\mu_{k-1}$表示更新前的状态价值。${1/k}(x_k-\mu_{k-1})$则是将第$k$次某状态的return减去更新前的状态价值，再除以一个该状态更新的次数$k$。
换句话说，累进更新平均值利用前一次的平均值和当前某状态return以及以该状态为起点的return的个数来计算新的平均值。\
如果把式子中的$\mu_{k-1}$看作状态的价值，$x_k$看作一个新的该状态下的return，那么该公式就成为了递增式的蒙特卡洛法更新状态价值:
$$
N(S_t)\leftarrow N(S_t) + 1 \\
V(S_t)\leftarrow V(S_t) + {1\over N(S_t)}(G_t - V(S_t))
$$
在一些实时或者无法准确统计某个状态的出现次数的情况下，可以使用一个系数$\alpha$来代替状态计数的倒数:
$$
V(S_t)\leftarrow V(S_t) + {\alpha}(G_t - V(S_t))
$$

# 时序差分强化学习
时序差分强化学习($temporal-difference\ reinforcecment learning, TD学习$)，指的是从采样中得到的不完整的状态序列进行学习。先**通过引导**(bootsrapping)估计某个状态在状态序列完整的情况下可能的return。再使用累进更新平均值的方法来更新该状态的价值，然后通过不断的采样持续更新该状态的价值。
> 一言以蔽之，时序差分和蒙特卡洛的区别就在于时序差分在来到某状态时就估计它的return来更新，蒙特卡洛则是得到整个状态序列并得到所有状态的return再更新。
具体而言，TD学习使用**该状态采取动作的即时奖励**加上**下一时刻状态**$S_{t+1}$**的预估状态价值乘以衰减系数**$\gamma$组成。
$$
V(S_t)\leftarrow V(S_t) + {\alpha}(R_{t+1}+\gamma V(S_{t+1}) - V(S_t))
$$
其中，$R_{t+1}+\gamma V(S_{t+1})$称为TD目标值，它减去$V(S_t)$称为TD误差。
TD误差在期望意义上应该为零。用语言描述的话，TD目标值反应的是即时奖励加上下一个时刻的状态价值减去当前的状态价值，得到td error。
<font color = "red">注意到，即时奖励加下一个时刻的状态价值就是当前状态的状态价值，它是由贝尔曼方程推导得到的:</font>
$$
V(S_t) \leftarrow R + \gamma V(S_{t + 1})
$$
在td学习中，我们通过经历多个$episode$，在每个$episode$中得到$td\ error$，并用它乘以学习率来更新我们的状态价值，并据此收敛到最优解。

引导(bootstrapping):用TD目标值代替$G_t$的过程。


假设你正在玩一个你从未玩过的迷宫游戏，你想学习如何在游戏中取得高分。在每个状态（即游戏的每个位置）下，你需要采取一个动作（如向上、向下、向左、向右等）来移动到下一个状态。你不知道下一个状态的价值，但你知道当前状态的价值，即你的当前位置。

假设你现在处于状态$S_t$，你采取了一个动作$A_t$，你到达了下一个状态$S_{t+1}$并得到了一个即时奖励$R_{t+1}$。你需要更新状态$S_t$的价值函数估计值$v(S_t)$，即通过估计下一个状态$S_{t+1}$的价值函数$v(S_{t+1})$来估计当前状态的状态价值。

TD算法采用的方法是通过当前状态$S_t$的估计值$v(S_t)$和下一个状态$S_{t+1}$的估计值$v(S_{t+1})$来更新状态$S_t$的价值函数估计值$v(S_t)$。TD目标值$target$可以通过如下公式得到：
$$
target = R_{t+1} + \gamma v(S_{t+1})
$$
其中，$R_{t+1}$是当前状态$S_t$采取动作$A_t$后得到的即时奖励，$\gamma$是折扣因子，用于调节未来奖励的重要性。根据TD目标值，可以用下式更新当前状态$S_t$的价值函数估计值$v(S_t)$：
$$
v(S_t) \leftarrow v(S_t) + \alpha(target - v(S_t))
$$
其中，$\alpha$是学习率，用于调节每次更新的幅度。通过这种方式，TD算法可以逐步更新状态价值函数的估计值。
> 关于学习率
<font color = "red">学习率反应了我们的td error和当前状态价值在更新时各自占的比重关系。</font>
这是什么意思呢？我们观察td学习中更新状态价值函数的公式:
$$
\begin{align}
V(S_{t}) &\leftarrow V(S_t) + \alpha (R + V(S_{t+1}) - V(S_t)) \\
&=V(S_t) + \alpha(V^{'}(S_{t}) - V(S_t)) \\
&=(1 - \alpha)V(S_t) + \alpha V^{'}(S_{t})
\end{align}
$$
这里的式子可以直观的反映出学习率对于价值更新的影响。如果学习率变大，那么原来经验得出的状态价值的比重就变小。每次经历新的episode时如果得到了新的状态价值估计值，则它会对状态价值造成较大幅度的更新。这有可能导致错过最优解，但是如果学习率太小每次更新的幅度太小，收敛又会太慢，所以需要选择合适的学习率。



我们可以考虑以下这个简单的迷宫，其中数字表示状态，S为起点，G为终点，X表示障碍物。

```
S 0 1 2
3 X 4 5
6 7 G 8
```

假设我们使用TD(0)算法来估计每个状态的价值。首先需要初始化所有状态的价值为0。

我们从起点S开始，向右移动到状态0，并得到即时奖励-1。根据TD(0)的更新公式，我们可以更新状态S的价值：
$$
V(S) \gets V(S) + \alpha [R + \gamma V(0) - V(S)]
$$
假设我们使用步长参数 $\alpha=0.1$ 和折扣因子 $\gamma=0.9$。因为0是S的下一个状态，且V(0)的初始值也为0，因此可以计算出更新值：
$$
V(S) \gets 0 + 0.1[-1 + 0.9 \cdot 0 - 0] = -0.1
$$
后续的计算与上面相似。
> 注意:在TD算法中，我们通常只根据实际转移到的下一个状态来更新当前状态的状态价值，不需要对所有可能转移到的状态的状态价值进行加权平均。这是因为TD算法是一种在线学习方法，它通过不断地从当前状态转移到下一个状态来更新状态价值，而不是等待所有可能的状态序列都出现再进行更新。因此，TD算法只更新已经出现的状态转移对应的状态价值，而不考虑未来可能出现的状态转移。

TD算法首先根据已有经验估计状态间的转移概率:
$$
\hat{P_{s,s^{'}}^a}={1\over N(s,a)}\sum_{k=1}^{k}{\sum_{t=1}^{T_k}{1(S_t^k,a_t^k,s_{t+1}^k) = s,a,s^{'}}}
$$
同时估计某一个状态的即时奖励：
$$
\hat{R_{s}^a}={1\over N(s,a)}\sum_{k=1}^{k}{\sum_{t=1}^{T_k}{1(S_t^k,a_t^k) r^k_t}}
$$
最后计算状态价值函数。

For each step of the episode,t = T-1,T-2,...0, do
$$
g \leftarrow \gamma g+r_{t+1}
$$
if $(s_t,a_t)$ does not appear in $(s_0,a_0,s_1,a_1,...,s_{t-1},a_{t-1})$,then
$$
Returns(s_t,a_t) \leftarrow Returns(s_t,a_t) + g
$$





MC算法可以用来寻找最优策略，一般可以按以下步骤进行：

初始化策略 $\pi$ 和状态价值函数 $V(s)$；

对于每一次实验（episode），根据当前策略 $\pi$ 生成状态序列 $s_1,a_1,r_2,s_2,a_2,...,r_T,s_T$，其中 $T$ 是实验结束时的时间步数；

对于每一个时间步 $t=1,2,...,T$，计算该时间步的回报 $G_t$（从时间步 $t$ 开始的折扣累积奖励），即 $G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+...+\gamma^{T-t-1} r_T$，其中 $\gamma$ 是折扣因子；

对于每一个状态-动作对 $(s_t,a_t)$，计算其累积回报 $G(s_t,a_t)$，即从该状态-动作对出发，到达实验结束时的状态 $s_T$ 所获得的累积奖励，然后更新该状态-动作对的出现次数 $N(s_t,a_t)$ 和累积回报 $S(s_t,a_t)$，即：
$$
N(s_t,a_t) \leftarrow N(s_t,a_t) + 1 \\ S(s_t,a_t) \leftarrow S(s_t,a_t) + G_t
$$​


对于每一个状态-动作对 $(s,a)$，更新其价值函数 $Q(s,a)$，即：
$$
Q(s,a) \leftarrow \frac{S(s,a)}{N(s,a)}
$$
	​


根据新的价值函数 $Q(s,a)$，更新策略 $\pi$，即对于每一个状态 $s$，选择 $a=\arg\max_a Q(s,a)$ 作为下一步的动作。

重复步骤 2-6，直到价值函数 $Q(s,a)$ 收敛到最优值，或达到一定的迭代次数。

需要注意的是，MC算法是一种基于经验的算法，需要进行多次实验（episode），才能得到比较准确的价值函数和策略。同时，MC算法也需要注意控制折扣因子 $\gamma$ 的取值，过大或过小都会影响算法的收敛性和稳定性。

## n步时序差分
上面介绍的都是$TD(0)$算法，也就是在已知状态的基础上向前看一步。用采取行为的下一步的**即时奖励**和**后续状态的状态价值预估值**来计算当前状态的状态价值。
$n$步时序差分的道理类似，假设现在的状态是$S_t$，那么就是往前看到$S_{t+n-1}$步的状态以及这一路上的即时奖励，并估计状态$S_{t+n}$的状态价值。用它们来更新当前状态$S_t$的状态价值。公式为:
$$
V(S_t)=V(S_t)+\alpha(G_t^{(n)}-V(S_t))
$$
其中，$G_t^{(n)}$指的是从第$t+1$步开始的$n$步即时奖励加上第$n+1$步的状态价值。
如果向前看$\infty$步，那么就成了蒙特卡洛算法了。因为看了$\infty$步后到达了状态序列的最后一个状态，将这些状态的即时奖励乘以折现率加起来其实就是当前状态为起点的return。
从上面可知$n=1$时就是$TD(0)$，$n=\infty$时就是蒙特卡洛算法。如何找到一个合适的$n$使得充分利用两种算法的优势呢？
> 换句话说，我们做某件事的影响，可能和未来的事关系重大，这时候我们应该多考虑后续奖励取一个较大的$n$。另一件事可能影响比较小，或者说影响只有几步。这时候就可以选择取一个较小的$n$。

于是我们基于此原因引入了参数$\lambda$，它表示:从$n=1$到$\infty$的所有步收获的权重之和。任意一个$n$步收获的权重为$(1-\lambda)\lambda^{n-1}$。定义为:
$$
G_t^\lambda = (1-\lambda)\sum_{n=1}^{\infty}{\lambda^{n-1}G_t^{(n)}}
$$
> 为什么要把权重设置为$(1-\lambda)\lambda^{n-1}$？其实$\lambda$是一个$(0,1)$范围内的取值。如果它接近于0，那么就有$G_t^{\lambda} \approx G_t^{(n)}$，此时它接近TD(0)学习。如果它接近于1，那么$(1-\lambda)\lambda^{n-1}$这部分的值此时类似$\gamma * \frac{1}{N(S_t)}$的作用，此时它接近蒙特卡洛学习。
其中，$(1-\lambda)$是一个比例因子，控制n步之前的奖励对当前状态值的影响；$\lambda^{n-1}$是一个指数因子，控制当前状态值对n步之前的奖励的影响。

### 前向认识与反向认识
#### 前向认识TD($\lambda$)
如果从前向后看，一个状态的状态价值$V(S_t)$依赖于$G_t^{\lambda}$，而$G_t^{\lambda}$又由后续的奖励和状态价值才能知道。这与MC算法的要求一致，都必须经过完整的状态序列才能得到每一个$G_t$。
#### 反向认识TD($\lambda$)
如果要了解反向认识，需要先知道**效用迹**的概念。
>效用迹（eligibility trace）是强化学习中一种用于描述历史状态对当前状态的影响程度的方法。具体来说，效用迹是一个和状态空间相同大小的向量，其中每个元素表示在某个时间步之前，某个状态对当前状态的影响程度。效用迹可以被看作是一种记忆单元，记录了历史状态对当前状态的影响。

在基于TD(lambda)的学习算法中，效用迹被用来计算状态值函数或动作值函数的更新量。每当智能体到达一个新状态时，与该状态相关的效用迹会被增加，而其他状态的效用迹则会逐渐衰减。这样，历史状态对当前状态的影响就可以被合理地考虑到，从而提高学习的效率和准确性。

通常情况下，效用迹可以通过多种方式来计算，比如累加式效用迹、替换式效用迹等等。

在强化学习中，效用迹是一种用于跟踪过去决策的方式，可以帮助智能体更好地学习长期的策略。在状态价值的概念中，每个状态都有一个对应的价值，该价值表示在该状态下采取各种行动的期望回报。当智能体处于某个状态时，它可以通过比较可用行动的预期价值来选择下一步应该采取的行动。

效用迹与状态价值类似，都用于表示某种形式的累积信息。不过**效用迹更加关注于轨迹上的信息**，它的基本思想是对轨迹上的每个状态都进行更新，而不仅仅是最终状态。在更新过程中，效用迹会在每个状态上留下一个痕迹，这个痕迹会随着时间的推移逐渐消失。这种随时间消失的痕迹可以被看作是对过去决策的记忆，可以影响当前的决策。

举个生活中的例子，假设你每天都会去吃饭。你对于各个餐馆的评价可以看作是状态的价值，表示在该状态下采取某个行动（选择该餐馆）的期望回报。而效用迹则可以看作是你对每个餐馆的印象，它随着时间的推移逐渐消失，但可以影响你未来的选择。比如，如果你在某个餐馆尝试了几次都感觉不好吃，那么该餐馆的效用迹就会逐渐减少，影响你在未来选择餐馆时对该餐馆的偏好。

其实上面说了这么多，效用迹的概念就是计算各个状态价值的时候增加了一个"影响力"的概念，我们在计算某个状态的状态价值时，除了考虑后续状态价值以外，还要考虑先前的状态的影响力。但是当某个状态上一次出现的过于久远时，显然它的影响力就微乎其微甚至可以忽略不计了，因此我们可能就不再考虑过于久远的状态的影响。
定义为:
$$
E_0(s)=0\\
E_t(s) = \gamma\lambda E_{t-1}(s)+1(S_t = s),\gamma,\lambda \in [0,1]
$$
在开始的时候，效用迹为0。随着状态的出现会不断增加然后又经由$\gamma \lambda$的影响而衰减，$+1(S_t = s)$这一部分的含义是，仅当当前状态为s时，才对当前状态的效用迹进行一个+1的操作。这样会导致某个状态连续出现时，即使有$\gamma \lambda$的影响进行衰减，它的效用迹也会维持在一个较高的数值，表明该状态对后续状态影响较大。
效用迹也是存在上限的，它的瞬时最高上限是:
$$
E_{sat}=\frac{1}{1-\gamma \lambda}
$$
如果我们更新状态价值时同时考虑效用，价值更新就变成了:
$$
\delta_{t}=R_{t+1}+\gamma (V(S_{t+1})-V(S_t))\\
V(s) \leftarrow V(s)+\alpha \delta_{t} E_t(s)
$$

# 不基于模型的控制
不基于模型的控制，指的是在不基于模型的条件下如何通过个体的**学习**来优化价值函数。同时改善自身的行为策略来最大化获得累积奖励的过程。
指导个体与环境进行实际交互的行为的策略称为行为策略，评价状态或行为价值的策略或者说待优化的策略称为目标策略。
个体学习过程中如果行为策略与目标策略为同一策略，则称为**现时策略学习**。如果优化的策略与行为策略是不同的策略时，称为**借鉴策略学习**。
> 举例说明什么是行为策略，什么是目标策略。
假设你想要减肥，你可以设计两个策略：
目标策略：每天只吃蔬菜和水果。
行为策略：每天吃三餐，不吃零食，不喝含糖饮料。
在这个例子中，目标策略是你想要实现的最终目标，即只吃蔬菜和水果。行为策略则是你实际采取的行动方式，即每天吃三餐、不吃零食、不喝含糖饮料。行为策略和目标策略不一定完全一致，但是行为策略会有助于你逐步实现目标策略。

通常情况下，目标策略可能会比较激进或者冒险，因为它是为了最大化长期回报而设计的，而这可能需要在短期内做出一些风险决策。但在实际应用中，我们可能更倾向于使用更保守的策略，以最小化风险和损失。因此，在许多情况下，我们会使用不同于目标策略的行为策略，以平衡长期回报和风险之间的权衡。此外，行为策略还可以根据当前状态和环境来进行调整，以应对突发事件和变化的情况。
在学习的过程中，**使用行为策略可以帮助我们探索新的状态和动作**，从而提高我们的学习效率。而**使用目标策略可以让我们更加注重利用已有的知识**，提高学习的效果。当我们发现目标策略中存在一些不合理之处时，我们可以通过学习来改善目标策略，从而更好地实现我们的目标。
## $\epsilon 贪婪策略$
简而言之，$\epsilon$贪婪策略在牺牲了部分最优性的基础上，使得算法保持了一定的探索性。
$$
\pi(a\mid s)=
\left
\{\begin{matrix}
\epsilon / m + 1 - \epsilon \qquad if \ a^{*} = \argmax _{a \in A} Q(s,a)\\
\epsilon / m  \qquad else \\
\end{matrix}
\right.
$$
## 实时蒙特卡洛策略控制
实时蒙特卡洛策略控制采用$\epsilon$贪婪策略进行迭代，在采样得到了一条完整的状态序列后就可以开始迭代状态行为对的价值。并持续的对策略进行评估和改善。
然而，使用该策略的实时蒙特卡洛策略控制仍然只能得到一个基于该策略的近似行为价值函数，因为该策略一直在探索，没有终止条件。
所以我们希望关注两个方面，一方面我们不想丢弃更好的信息和状态，另一方面随着我们策略的改善最终我们希望可以收敛至某个最优策略。
为此引入了一个名为$GILE(greedy\ in\ the\ limit\ with\ infinte\ exploration)$。它有两种含义，分别是:
1. 所有的状态行为对都会被无限次探索。
$$
\lim_{k \rightarrow \infty}{N_k(s,a)}=\infty
$$
2. 随着采样趋向于无穷多，策略会收敛至一个贪婪策略。
$$
\lim_{k \rightarrow \infty}{\pi_k(a\mid s)}=1(a=\argmax _{a^{'} \in A}{Q_k{(s,a^{'})}})
$$
## 实时策略时序差分控制
### Sarsa算法
相比TD算法更新状态价值，Sarsa算法的思想是：使用后续的行为价值，更新当前状态-行为对的行为价值。
更具体的说，针对一个状态$s$，个体通过行为策略产生一个行为$a$，执行该行为进而产生一个状态-行为对$(s,a)$，并基于此获得一个即时奖励$R_s^a$并进入状态$s^{'}$。在新的状态$s^{'}$下，个体遵循策略产生一个新的行为$a^{'}$。但是此时个体并不立即执行该行为，而是通过行为价值函数得到$q_\pi(s^{'},a^{'})$的价值。利用即时奖励$R_s^a$和这个$q_\pi(s^{'},a^{'})$来更新前一个状态-行为对$(s,a)$的价值。 \
\
相较于MC算法，Sarsa算法在单个状态序列的每一个时间步，在状态$s$下采取一个行为$a$到达状态$s^{'}$后都要更新状态行为对$(s,a)$的价值$q_\pi(s^{'},a^{'})$，这一过程中同样使用$\epsilon-贪婪策略$进行策略迭代。
$$
Q(s,a) \leftarrow Q(s,a)+\alpha(R_s^{a}+\gamma Q(s^{'},a^{'})-Q(s,a))
$$

### Sarsa($\lambda$)算法
类似于n步TD学习，Sarsa算法也有一个n步的概念。$n=1$时就是我们常规的Sarsa算法。
n步Sarsa是强化学习中的一种算法，是Sarsa算法的变体。在n步Sarsa算法中，智能体不需要等到当前状态的值函数被完全更新才能采取下一个行动，而是采取一系列行动，然后在一次更新中一次性更新n步之前的状态-行动值函数。

具体来说，n步Sarsa算法中，在每个时间步，智能体采取一个行动并观察到下一个状态以及对应的奖励，同时将这个状态-行动对加入n步缓存。如果当前时间步大于等于n步，则从缓存中取出之前n步的状态-行动对来进行更新，具体更新公式如下：
$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(G_{t:t+n}-Q(S_t,A_t))
$$
其中，$G_{t:t+n}$是从时间步$t$开始，连续$n$步的回报值，可以使用下面的公式计算：
$$
G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n},A_{t+n})
$$
相比于标准的Sarsa算法，n步Sarsa算法的优势在于可以更快地进行更新，同时也可以减小由于单步更新带来的方差过大的问题。
类似于TD($\lambda$)，可以给n步Sarsa中的Q收获的每一步分配一个权重，并按照权重对每一步的Q收获求和，那么就得到$q^{\lambda}$收获:
$$
q^{\lambda}_t=(1-\lambda)\sum_{n=1}^{\infty}{\lambda}^{n-1}q_t^{(n)}
$$
如果用某一个状态的$q^{\lambda}_t$来更新状态行为对的Q值，那么可以表示成如下的形式:
$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(q^{(\lambda)}_{t}-Q(S_t,A_t))
$$
上述公式也是对于$Sarsa(\lambda)$的前向认识，使用它更新Q价值需要遍历完整的状态序列。
我们同样可以反向认识$Sarsa(\lambda)$，同样引入效用追迹。E值针对的不是一个状态，而是一个状态行为对:
$$
E_0(s,a)=0 \\
E_t(s,a)=\gamma \lambda E_{t-1}(s,a)+1(S_t=s,A_t=a), \gamma,\lambda \in [0,1]
$$
基于反向认识$的Sarsa(\lambda)$算法可以有效地在线学习，数据学习完毕即可丢弃。
值得注意的是，效用迹针对的仅仅只是状态序列，因此每当一条状态序列结束时需要将其重新置零。而算法更新的Q和E则是针对个体掌握的整个状态空间和行为空间产生的Q和E。
## 借鉴策略$Q-learning$算法
借鉴策略学习中，产生指导自身行为的策略$\mu(a\mid s)$和更新状态行为对价值时使用的目标策略$\pi(a\mid s)$不是同一个策略。
具体的说，个体通过策略$\mu(a\mid s)$和环境产生实际交互，而使用目标策略$\pi(a\mid s)$来更新状态-行为对的价值。
目标策略$\pi(a\mid s)$多数是已经具备了一定“能力”的策略，也就是已有的经验。
借鉴学习TD学习任务就是使用TD方法在目标策略$\pi(a\mid s)$的基础上更新行为价值，进而优化行为策略：
$$
V(s_t) \leftarrow V(s_t)+\alpha \bigg (\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}\big [R_{t+1}+\gamma V(S_{t+1})-V(S_t)\big ] \bigg )
$$
其中我们关注的重点是这部分:
$$
\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}
$$
这是目标策略与行为策略之间的比值。\
当这个比值接近于1时，说明目标策略与行为策略在该状态下采取该动作的概率差不多，也就是说两个策略都支持这样的行为。\
当$\mu(A_t\mid S_t)$更大时，比值会小于1。这时可以认为我们的目标策略$\pi(A_t\mid S_t)$并不是很支持这样的行为，因此对于价值函数的更新需要打一些折扣。
当$\mu(A_t\mid S_t)$更小时，比值会大于1。这时可以认为我们的目标策略$\pi(A_t\mid S_t)$对于我们的行为策略更新的效果并不是非常满意。于是会增大更新的程度，鼓励更大胆的更新当前的价值。
### $Q-learning$
借鉴策略TD学习中的一个**典型的行为策略**$\mu(a\mid s)$是**基于行为价值函数**$Q(s,a)$的$\epsilon -greedy\ policy$，而**目标策略**则是基于$Q(s,a)$的完全贪婪策略，这种学习方式称为$Q-learning$。
$Q-learning$的目标是得到最优价值$Q(s,a)$，在这过程中，$t$时刻与环境进行实际交互的行为$A_t$由策略$\mu$产生:
$$
A_t \sim \mu(·|S_t)
$$
而$t+1$时刻用来更新Q值的行为$A_{t+1}^{'}$的行为由目标策略$\pi(a\mid s)$产生:
$$
A_{t+1}^{'} \sim \pi(·|S_{t+1})
$$
注意到上述的行为策略$\mu(a\mid s)$和目标策略$\pi(a\mid s)$分别是一个$\epsilon -greedy\ policy$和一个完全贪婪策略。
$Q(S_t,A_t)$按照下面的式子进行更新:
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha \bigg( {\color{red} R_{t+1}+\gamma Q(S_{t+1},A^{'})} - Q(S_t,A_t) \bigg )
$$
其中，红色部分是基于目标策略$\pi(a,s)$产生的行为$A^{'}$得到的Q值。在这样的条件下，状态$S_t$的行为$A_t$的价值会朝着目标策略下确定的最大行为价值的方向做一定比例的更新。
> 如果行为 $A_t$ 和目标策略的行为不是同一个行为，那么在 Q-learning 算法中，$Q(S_t, A_t)$ 的值会被更新，但是更新的幅度会比较小。
具体来说，假设当前状态为 $S_t$，当前采取的行为为 $A_t$，根据 Q-learning 算法的更新公式：
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha \bigg(R_{t+1}+\gamma \max_{a^{'}}{Q(S_{t+1},A^{'})} - Q(S_t,A_t) \bigg )
$$
其中，$\max_{a^{'}}{Q(S_{t+1},A^{'})}$ 表示在下一个状态 $S_{t+1}$ 时采取所有行为 $a$ 中最大的 Q 值，即在下一个状态下的最优行为价值。
如果 $A_t$ 和目标策略的行为不同，那么目标策略下的最大行为价值不一定是 $Q(S_{t+1}, A_t)$，而是 $\max_{a} Q(S_{t+1}, a)$，因为采取的行为 $A_t$ 可能不是目标策略下的最优行为。因此，$Q(S_t, A_t)$ 的更新量就会比较小，这也是 Q-learning 算法中存在一定偏差的原因。