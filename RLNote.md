---
title: "Reinforcement Learning note"
author: Yu Xia
date: March 28, 2023
output: pdf_document
---

> 以下公式都仅针对于**马尔科夫决策过程**

# 状态价值与行为价值的关系
## 状态序列
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3}+... 
$$
## 状态价值
$$
\begin{align}
v_\pi(s) &= \Bbb E[G_t\mid S_t=s] \\
&= \Bbb E[R_{t+1} + \gamma v_{\pi}(S_{t+1})\mid S_t = s] \\
\end{align}\\
如果用矩阵的形式来描述这个式子，那么就有:\\
\\
v=R + \gamma Pv\\
其中P就代表了对于后续状态价值求期望 \\
进行矩阵运算即可得到:\\
v=(I-\gamma P)^{-1}R
$$

## 行为价值
$$
\begin{align}
q_{\pi}(s,a) &= \Bbb E[G_t \mid S_t = s, A_t = a]\\
&=\Bbb E[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) \mid S_t = s, A_t = a]
\end{align}
$$

## 状态价值与行为价值
$$
一个状态的状态价值可以用该状态下所有行为价值来表达:\\
v_\pi(s) = \sum_{a \in A}{\pi(a|s)q_{\pi}(s,a)} \\
一个行为的行为价值可以用该行为能达到的后续状态价值来表达:\\
q_{\pi}(s,a)=R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a{v_{\pi}(s^{'})}}\\
那么，一个状态的状态价值可以表示为:\\
v_\pi(s) = \sum_{a \in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a{v_{\pi}(s^{'})}})\\
一个行为的行为价值可以表示为:\\
q_{\pi}(s,a)=R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}(\sum_{a \in A}{\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'})})
$$
具体的描述一下第三个公式，首先它需要几个条件:
1. 这个状态的状态值基于策略$\pi$
2. 该策略下基于状态$s$有一到多个行为$a$，采取这些行为后到达的状态$s^{'}$是具有概率性的，也就是说采取行为后到达的状态是不确定的。

那么，最后这个公式可以描述为:
一个状态的状态价值基于给定的策略$\pi$，它表示该策略下所有可能的动作获得的即时奖励，加上采取该动作后到达下一步状态的概率乘以下一步状态的状态价值乘以折现率。
再换句话说，一个状态的状态价值等于它当前所有可能行为的即时奖励，加上采取某一行为后后续状态价值的期望。

> 总结:
> 一个状态的状态价值，等于该状态采取某个行为获得的即时奖励加上以该状态为起点的后续所有可能的状态的价值的期望。
> 一个行为的行为价值，同样可以看作是行为获得的即时奖励，加上该行为到达某状态的概率乘以该状态的状态价值。

## 最优状态价值函数与最优行为价值函数
### 最优状态价值函数
最优状态价值函数指的是在状态$s$下，所有的策略中产生的状态价值中最大者:
$$
v_* = \max_{\pi}v_{\pi}(s)
$$
换句话说，如果某个策略$\pi$使得对于任意状态$s$，都有$v_{\pi}(s) >= 其他任意策略下的状态价值$，那么就称这个策略是最优策略，使用了最优策略的状态价值函数称为最优状态价值函数。

## 最优行为价值函数
最优行为价值函数指的是在状态$s$下，所有策略产生的行为中行为价值最大者。
$$
q_*(s,a) = \max_{\pi}q_{\pi}(s,a)
$$
> 再提一次，行为价值依赖于状态$s$下采取的行为获得的即时奖励，以及该行为可能达到的所有后续状态的概率(由状态转移概率矩阵确定)乘以该后续状态的状态价值。

从公式中可以看出，最优行为价值函数也和策略的选择息息相关。最优行为价值来自于最优的策略，最优策略使得在任何一个状态$s$下选择某个行动$a$获得的行为价值最大。
同时，最优策略可以通过最大化行为价值函数来获得。也就是说，假如我们处于任意状态$s$，此时采取行动$a$获得的行为价值$q(s,a)$均是最大的。那么我们的最优策略$\pi$就应该在这个状态下总是选择这个行为，即$\pi(s,a) = 1$.
解决强化学习问题意味着需要寻找一个最优策略，使得该策略下的return最大。而我们最优策略可以通过最大化行为价值函数来获得，因此解决强化学习问题就变成了求解最优行为价值函数的问题。

最优行为价值函数可以换一种表示形式，表示为：
$$
q_*(s,a) = \Bbb E[R_{t+1} + \gamma v_*(S_{t+1})\mid S_t = s,A_t = a] 
$$
也就是说，最优行为价值可以用该行为获得的即时奖励加上后续达到的状态价值的期望得到。

# 动态规划寻找最优策略
预测是对给定策略的评估过程，控制是寻找一个最优策略的过程。\
<span id = "predAndCon"></span>
**预测($prediction$)**:已知一个马尔科夫决策过程$<S,A,P,R, \gamma>$和一个策略$\pi$，或者是给定一个马尔科夫奖励过程$<S,P_\pi,R_\pi, \gamma>$，求解基于该策略的价值函数$v_\pi$
**控制($control$)**:已知一个马尔科夫决策过程$<S,A,P,R, \gamma>$，求解最优价值函数$v_*$和最优策略$\pi_*$。 

## 策略评估
策略评估指的是计算给定策略下状态价值函数的过程。\
使用上一个迭代周期$k$内的后续状态价值来计算更新当前迭代周期$k+1$内某状态$s$的价值(听起来像雅可比迭代法):
$$
v_{k+1}(s)=\sum_{a \in A}{\pi(a|s)(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}v_{k}(s^{'}))}
$$

## 策略迭代
策略迭代指的是完成一定轮次的策略评估之后，此时我们得到了所有的状态的状态价值。那么我们就可以通过新的状态价值来更新我们的策略，使得我们的策略在某个状态时更倾向于前往状态价值更大的状态。然后，我们使用更新后的策略进行策略评估，更新我们的状态价值函数，重复这个过程。最终，它们都会收敛到最优价值函数和最优策略。

## 价值迭代
一个策略，如果是最优策略，那么它在某个状态下一定能够产生当前状态下的最优行为。而且通过当前状态下的最优行为到达的后续状态。该策略同样是最优的。
或者换句话说，如果一个策略对于某个状态产生的行为它不是最优行为，那么这个策略就不是一个最佳策略。
>一个策略能够获得某状态的最优价值当且仅当该策略也同时获得状态所有可能的后续状态的最优价值。
>对于这句话的理解就是，一个策略获得某状态的最优价值。说明该策略在该状态下产生的行为是一个最优价值行为，最优价值行为就是由当前行为的奖励加上行为导致的后续状态价值的期望组合而成。
也就是有:
$$
v_{*}(s)=\max_{a\in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}v_{*}(s^{'}))
$$

### 价值迭代与策略迭代的区别
**价值迭代算法中迭代的是状态价值函数 $v(s)$，以逼近最优状态价值函数 $v^*(s)$。**

>与策略迭代不同，价值迭代算法**并不需要显式地维护一个策略**。在每次更新状态价值函数后，我们都会自动进行一次最优化策略改进，即选择当前能够使下一个状态价值最大化的行动作为最优策略的行动。因此，价值迭代算法通常会比策略迭代更加高效，因为它将策略评估和策略改进两个过程集成到了一个算法中，无需反复地进行迭代。
>另外，与策略迭代相比，价值迭代算法通常更容易实现和理解，并且可以处理具有连续状态和行动空间的问题。但它也有一些缺点，例如可能会收敛得比较慢，特别是在状态空间很大的情况下。此外，如果状态或者行动空间非常大，那么存储和计算状态价值函数的成本也可能会很高。
>
>书上描述的二者的差距：策略迭代每次迭代仅计算相关状态的价值，一次计算即得到最优状态价值。后者在每次迭代时要更新所有状态的价值。
>这里的"一次计算即得到最优状态价值"指的是基于当前的所有策略中选择了最优的策略计算出的最优状态价值，但是策略迭代同时也需要改善策略。所以根据最优的状态价值还会更新策略继续下一次的策略迭代。
>而价值迭代是基于各个状态各自能够采取的行动，采取最优行为来更新自己的状态价值函数。也就是说，每次迭代完毕后，我们根据各个状态状态价值的大小选择出来的路径就是最优策略。
---
>chatGPT给出的描述:
>在策略迭代中，首先进行一定次数的策略评估得到当前策略下状态的价值函数，然后通过策略改进更新策略，再重新进行策略评估得到新的状态价值函数。这个过程不断重复，直到算法收敛为止。策略迭代的优点是它能够针对特定的策略进行优化，逐步提高策略的质量。
而在价值迭代中，每轮迭代会更新所有状态的价值函数，不需要显式地保留策略。价值迭代通常需要多次迭代才能达到最优解，但是一旦完成，从任意状态出发直接选择状态价值最大的状态就可以得到最佳策略的轨迹。因此，价值迭代较为适用于解决具有有限动作空间、可离散化状态空间的强化学习问题。
总之，策略迭代和价值迭代都是常用的增量式策略优化方法，在具体应用时需要根据问题的特性选择合适的算法。
---
价值迭代过程中，价值函数更新的公式为:
$$
v_{k+1}(s)=\max_{a \in A}(R_s^{a}+\gamma \sum_{s^{'}\in S}{P_{ss^{'}}^a}v_{k}(s^{'}))
$$
这个公式和策略评估的公式非常相似，区别在于策略评估需要根据策略考虑当前状态下所有行为的概率，而价值迭代中仅考虑行为价值最大的行为。
以上就是同步动态规划的内容，迭代法策略评估属于[预测](#predAndCon)问题，使用贝尔曼期望方程进行求解。而策略迭代和价值迭代属于[控制](#predAndCon)问题。

# 蒙特卡洛强化学习
蒙特卡洛强化学习的特点就是在我们不了解状态转移概率的情况下，通过得到一个**完整的状态序列**，来估计状态的真实价值，并认为**某个状态的价值**等于**在多个状态序列中以该状态为起点得到的所有收获的平均**。

> **完整的状态序列**指的是:当状态序列的最后一个状态是终止状态时，称该状态序列是一个**完整的状态序列**。
> 这段话的意思是，我们首先通过某种方法得到一个完整的状态序列。然后，我们计算状态序列中所有以非结束状态为起点的状态的return，并用这个return来更新这个起点状态的状态价值。

例如如下的一个状态序列:
$$
S_1,A_1,R_1,S_2,A_2,...,S_t,A_t,R_{t+1},...,S_k
$$
对于该状态序列，它在$t$时刻的状态$S_t$的$return$可以表示为:
$$
G_t=R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1}R_T
$$
该策略下某一状态$s$的价值为:
$$
v_\pi(s)=\Bbb E_\pi[G_t\mid S_t = s]
$$
> 再次提醒，这里的$G_t$仍然是不固定的，因为从状态$S_t$能够转移到的后续状态可能不止一个。
> 所以最后这个公式估计的是，以状态$S_t$为起点所有的$G_t$的return求**平均值**。
> (例如我们有两条完整的状态序列，在状态$s$时的return分别是$1$和$2$，那么状态$s$的价值就是它们的均值，即:$(1 + 2) / 2 = 1.5$)

注意到，在同一个状态序列中，某一个状态可能出现多次（例如浏览手机状态选择继续浏览的动作，则下一个状态仍然是浏览手机。出现多次的状态不一定连续）。显然，不同位置出现的相同状态为起点的return也是不一样的。既然我们需要使用某状态的return更新它的状态价值，那么需要应对这两种情况，所以就有:
1. 首次访问($first\ visit$):仅统计状态序列中首次出现该状态的位置的return，使用它来更新状态价值，后续再次出现该状态时则忽略。
2. 每次访问($every\ visit$):针对一个状态序列中每次出现的该状态，都计算其return值并更新状态价值。

蒙特卡洛方法中一个状态的状态价值是根据给定的所有状态序列中每条序列中该状态的return求和再平均得到。这就意味着我们最终得到一个状态的状态价值时需要记录下该状态所有的return，这样未免太麻烦。所以我们有一种名叫**累进更新平均值**($incremental\ mean$)的方法，在每次计算出新的某状态的return时，用它来更新状态价值，则无需记录历史所有状态价值求和再平均:
$$
\mu_k=\mu_{k-1}+{1\over k}(x_k-\mu_{k-1})
$$
其中，$\mu_k$表示更新后的状态价值，$\mu_{k-1}$表示更新前的状态价值。${1/k}(x_k-\mu_{k-1})$则是将第$k$次某状态的return减去更新前的状态价值，再除以一个该状态更新的次数$k$。
换句话说，累进更新平均值利用前一次的平均值和当前某状态return以及以该状态为起点的return的个数来计算新的平均值。\
如果把式子中的$\mu_{k-1}$看作状态的价值，$x_k$看作一个新的该状态下的return，那么该公式就成为了递增式的蒙特卡洛法更新状态价值:
$$
N(S_t)\leftarrow N(S_t) + 1 \\
V(S_t)\leftarrow V(S_t) + {1\over N(S_t)}(G_t - V(S_t))
$$
在一些实时或者无法准确统计某个状态的出现次数的情况下，可以使用一个系数$\alpha$来代替状态计数的倒数:
$$
V(S_t)\leftarrow V(S_t) + {\alpha}(G_t - V(S_t))
$$

# 时序差分强化学习
时序差分强化学习($temporal-difference\ reinforcecment learning, TD学习$)，指的是从采样中得到的不完整的状态序列进行学习。先**通过引导**(bootsrapping)估计某个状态在状态序列完整的情况下可能的return。再使用累进更新平均值的方法来更新该状态的价值，然后通过不断的采样持续更新该状态的价值。
> 一言以蔽之，时序差分和蒙特卡洛的区别就在于时序差分在来到某状态时就估计它的return来更新，蒙特卡洛则是得到整个状态序列并得到所有状态的return再更新。
具体而言，TD学习使用**该状态采取动作的即时奖励**加上**下一时刻状态**$S_{t+1}$**的预估状态价值乘以衰减系数**$\gamma$组成。
$$
V(S_t)\leftarrow V(S_t) + {\alpha}(R_{t+1}+\gamma V(S_{t+1}) - V(S_t))
$$
其中，$R_{t+1}+\gamma V(S_{t+1})$称为TD目标值，它减去$V(S_t)$称为TD误差。
TD误差在期望意义上应该为零。用语言描述的话，TD目标值反应的是即时奖励加上下一个时刻的状态价值减去当前的状态价值，得到td error。
<font color = "red">注意到，即时奖励加下一个时刻的状态价值就是当前状态的状态价值，它是由贝尔曼方程推导得到的:</font>
$$
V(S_t) \leftarrow R + \gamma V(S_{t + 1})
$$
在td学习中，我们通过经历多个$episode$，在每个$episode$中得到$td\ error$，并用它乘以学习率来更新我们的状态价值，并据此收敛到最优解。

引导(bootstrapping):用TD目标值代替$G_t$的过程。


假设你正在玩一个你从未玩过的迷宫游戏，你想学习如何在游戏中取得高分。在每个状态（即游戏的每个位置）下，你需要采取一个动作（如向上、向下、向左、向右等）来移动到下一个状态。你不知道下一个状态的价值，但你知道当前状态的价值，即你的当前位置。

假设你现在处于状态$S_t$，你采取了一个动作$A_t$，你到达了下一个状态$S_{t+1}$并得到了一个即时奖励$R_{t+1}$。你需要更新状态$S_t$的价值函数估计值$v(S_t)$，即通过估计下一个状态$S_{t+1}$的价值函数$v(S_{t+1})$来估计当前状态的状态价值。

TD算法采用的方法是通过当前状态$S_t$的估计值$v(S_t)$和下一个状态$S_{t+1}$的估计值$v(S_{t+1})$来更新状态$S_t$的价值函数估计值$v(S_t)$。TD目标值$target$可以通过如下公式得到：
$$
target = R_{t+1} + \gamma v(S_{t+1})
$$
其中，$R_{t+1}$是当前状态$S_t$采取动作$A_t$后得到的即时奖励，$\gamma$是折扣因子，用于调节未来奖励的重要性。根据TD目标值，可以用下式更新当前状态$S_t$的价值函数估计值$v(S_t)$：
$$
v(S_t) \leftarrow v(S_t) + \alpha(target - v(S_t))
$$
其中，$\alpha$是学习率，用于调节每次更新的幅度。通过这种方式，TD算法可以逐步更新状态价值函数的估计值。
> 关于学习率
<font color = "red">学习率反应了我们的td error和当前状态价值在更新时各自占的比重关系。</font>
这是什么意思呢？我们观察td学习中更新状态价值函数的公式:
$$
\begin{align}
V(S_{t}) &\leftarrow V(S_t) + \alpha (R + V(S_{t+1}) - V(S_t)) \\
&=V(S_t) + \alpha(V^{'}(S_{t}) - V(S_t)) \\
&=(1 - \alpha)V(S_t) + \alpha V^{'}(S_{t})
\end{align}
$$
这里的式子可以直观的反映出学习率对于价值更新的影响。如果学习率变大，那么原来经验得出的状态价值的比重就变小。每次经历新的episode时如果得到了新的状态价值估计值，则它会对状态价值造成较大幅度的更新。这有可能导致错过最优解，但是如果学习率太小每次更新的幅度太小，收敛又会太慢，所以需要选择合适的学习率。



我们可以考虑以下这个简单的迷宫，其中数字表示状态，S为起点，G为终点，X表示障碍物。

```
S 0 1 2
3 X 4 5
6 7 G 8
```

假设我们使用TD(0)算法来估计每个状态的价值。首先需要初始化所有状态的价值为0。

我们从起点S开始，向右移动到状态0，并得到即时奖励-1。根据TD(0)的更新公式，我们可以更新状态S的价值：
$$
V(S) \gets V(S) + \alpha [R + \gamma V(0) - V(S)]
$$
假设我们使用步长参数 $\alpha=0.1$ 和折扣因子 $\gamma=0.9$。因为0是S的下一个状态，且V(0)的初始值也为0，因此可以计算出更新值：
$$
V(S) \gets 0 + 0.1[-1 + 0.9 \cdot 0 - 0] = -0.1
$$
后续的计算与上面相似。
> 注意:在TD算法中，我们通常只根据实际转移到的下一个状态来更新当前状态的状态价值，不需要对所有可能转移到的状态的状态价值进行加权平均。这是因为TD算法是一种在线学习方法，它通过不断地从当前状态转移到下一个状态来更新状态价值，而不是等待所有可能的状态序列都出现再进行更新。因此，TD算法只更新已经出现的状态转移对应的状态价值，而不考虑未来可能出现的状态转移。

TD算法首先根据已有经验估计状态间的转移概率:
$$
\hat{P_{s,s^{'}}^a}={1\over N(s,a)}\sum_{k=1}^{k}{\sum_{t=1}^{T_k}{1(S_t^k,a_t^k,s_{t+1}^k) = s,a,s^{'}}}
$$
同时估计某一个状态的即时奖励：
$$
\hat{R_{s}^a}={1\over N(s,a)}\sum_{k=1}^{k}{\sum_{t=1}^{T_k}{1(S_t^k,a_t^k) r^k_t}}
$$
最后计算状态价值函数。

For each step of the episode,t = T-1,T-2,...0, do
$$
g \leftarrow \gamma g+r_{t+1}
$$
if $(s_t,a_t)$ does not appear in $(s_0,a_0,s_1,a_1,...,s_{t-1},a_{t-1})$,then
$$
Returns(s_t,a_t) \leftarrow Returns(s_t,a_t) + g
$$





MC算法可以用来寻找最优策略，一般可以按以下步骤进行：

初始化策略 $\pi$ 和状态价值函数 $V(s)$；

对于每一次实验（episode），根据当前策略 $\pi$ 生成状态序列 $s_1,a_1,r_2,s_2,a_2,...,r_T,s_T$，其中 $T$ 是实验结束时的时间步数；

对于每一个时间步 $t=1,2,...,T$，计算该时间步的回报 $G_t$（从时间步 $t$ 开始的折扣累积奖励），即 $G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+...+\gamma^{T-t-1} r_T$，其中 $\gamma$ 是折扣因子；

对于每一个状态-动作对 $(s_t,a_t)$，计算其累积回报 $G(s_t,a_t)$，即从该状态-动作对出发，到达实验结束时的状态 $s_T$ 所获得的累积奖励，然后更新该状态-动作对的出现次数 $N(s_t,a_t)$ 和累积回报 $S(s_t,a_t)$，即：
$$
N(s_t,a_t) \leftarrow N(s_t,a_t) + 1 \\ S(s_t,a_t) \leftarrow S(s_t,a_t) + G_t
$$​


对于每一个状态-动作对 $(s,a)$，更新其价值函数 $Q(s,a)$，即：
$$
Q(s,a) \leftarrow \frac{S(s,a)}{N(s,a)}
$$
	​


根据新的价值函数 $Q(s,a)$，更新策略 $\pi$，即对于每一个状态 $s$，选择 $a=\arg\max_a Q(s,a)$ 作为下一步的动作。

重复步骤 2-6，直到价值函数 $Q(s,a)$ 收敛到最优值，或达到一定的迭代次数。

需要注意的是，MC算法是一种基于经验的算法，需要进行多次实验（episode），才能得到比较准确的价值函数和策略。同时，MC算法也需要注意控制折扣因子 $\gamma$ 的取值，过大或过小都会影响算法的收敛性和稳定性。

## n步时序差分
上面介绍的都是$TD(0)$算法，也就是在已知状态的基础上向前看一步。用采取行为的下一步的**即时奖励**和**后续状态的状态价值预估值**来计算当前状态的状态价值。
$n$步时序差分的道理类似，假设现在的状态是$S_t$，那么就是往前看到$S_{t+n-1}$步的状态以及这一路上的即时奖励，并估计状态$S_{t+n}$的状态价值。用它们来更新当前状态$S_t$的状态价值。公式为:
$$
V(S_t)=V(S_t)+\alpha(G_t^{(n)}-V(S_t))
$$
其中，$G_t^{(n)}$指的是从第$t+1$步开始的$n$步即时奖励加上第$n+1$步的状态价值。
如果向前看$\infty$步，那么就成了蒙特卡洛算法了。因为看了$\infty$步后到达了状态序列的最后一个状态，将这些状态的即时奖励乘以折现率加起来其实就是当前状态为起点的return。
从上面可知$n=1$时就是$TD(0)$，$n=\infty$时就是蒙特卡洛算法。如何找到一个合适的$n$使得充分利用两种算法的优势呢？
> 换句话说，我们做某件事的影响，可能和未来的事关系重大，这时候我们应该多考虑后续奖励取一个较大的$n$。另一件事可能影响比较小，或者说影响只有几步。这时候就可以选择取一个较小的$n$。

于是我们基于此原因引入了参数$\lambda$，它表示:从$n=1$到$\infty$的所有步收获的权重之和。任意一个$n$步收获的权重为$(1-\lambda)\lambda^{n-1}$。定义为:
$$
G_t^\lambda = (1-\lambda)\sum_{n=1}^{\infty}{\lambda^{n-1}G_t^{(n)}}
$$
> 为什么要把权重设置为$(1-\lambda)\lambda^{n-1}$？其实$\lambda$是一个$(0,1)$范围内的取值。如果它接近于0，那么就有$G_t^{\lambda} \approx G_t^{(n)}$，此时它接近TD(0)学习。如果它接近于1，那么$(1-\lambda)\lambda^{n-1}$这部分的值此时类似$\gamma * \frac{1}{N(S_t)}$的作用，此时它接近蒙特卡洛学习。
其中，$(1-\lambda)$是一个比例因子，控制n步之前的奖励对当前状态值的影响；$\lambda^{n-1}$是一个指数因子，控制当前状态值对n步之前的奖励的影响。

### 前向认识与反向认识
#### 前向认识TD($\lambda$)
如果从前向后看，一个状态的状态价值$V(S_t)$依赖于$G_t^{\lambda}$，而$G_t^{\lambda}$又由后续的奖励和状态价值才能知道。这与MC算法的要求一致，都必须经过完整的状态序列才能得到每一个$G_t$。
#### 反向认识TD($\lambda$)
如果要了解反向认识，需要先知道**效用迹**的概念。
>效用迹（eligibility trace）是强化学习中一种用于描述历史状态对当前状态的影响程度的方法。具体来说，效用迹是一个和状态空间相同大小的向量，其中每个元素表示在某个时间步之前，某个状态对当前状态的影响程度。效用迹可以被看作是一种记忆单元，记录了历史状态对当前状态的影响。

在基于TD(lambda)的学习算法中，效用迹被用来计算状态值函数或动作值函数的更新量。每当智能体到达一个新状态时，与该状态相关的效用迹会被增加，而其他状态的效用迹则会逐渐衰减。这样，历史状态对当前状态的影响就可以被合理地考虑到，从而提高学习的效率和准确性。

通常情况下，效用迹可以通过多种方式来计算，比如累加式效用迹、替换式效用迹等等。

在强化学习中，效用迹是一种用于跟踪过去决策的方式，可以帮助智能体更好地学习长期的策略。在状态价值的概念中，每个状态都有一个对应的价值，该价值表示在该状态下采取各种行动的期望回报。当智能体处于某个状态时，它可以通过比较可用行动的预期价值来选择下一步应该采取的行动。

效用迹与状态价值类似，都用于表示某种形式的累积信息。不过**效用迹更加关注于轨迹上的信息**，它的基本思想是对轨迹上的每个状态都进行更新，而不仅仅是最终状态。在更新过程中，效用迹会在每个状态上留下一个痕迹，这个痕迹会随着时间的推移逐渐消失。这种随时间消失的痕迹可以被看作是对过去决策的记忆，可以影响当前的决策。

举个生活中的例子，假设你每天都会去吃饭。你对于各个餐馆的评价可以看作是状态的价值，表示在该状态下采取某个行动（选择该餐馆）的期望回报。而效用迹则可以看作是你对每个餐馆的印象，它随着时间的推移逐渐消失，但可以影响你未来的选择。比如，如果你在某个餐馆尝试了几次都感觉不好吃，那么该餐馆的效用迹就会逐渐减少，影响你在未来选择餐馆时对该餐馆的偏好。

其实上面说了这么多，效用迹的概念就是计算各个状态价值的时候增加了一个"影响力"的概念，我们在计算某个状态的状态价值时，除了考虑后续状态价值以外，还要考虑先前的状态的影响力。但是当某个状态上一次出现的过于久远时，显然它的影响力就微乎其微甚至可以忽略不计了，因此我们可能就不再考虑过于久远的状态的影响。
定义为:
$$
E_0(s)=0\\
E_t(s) = \gamma\lambda E_{t-1}(s)+1(S_t = s),\gamma,\lambda \in [0,1]
$$
在开始的时候，效用迹为0。随着状态的出现会不断增加然后又经由$\gamma \lambda$的影响而衰减，$+1(S_t = s)$这一部分的含义是，仅当当前状态为s时，才对当前状态的效用迹进行一个+1的操作。这样会导致某个状态连续出现时，即使有$\gamma \lambda$的影响进行衰减，它的效用迹也会维持在一个较高的数值，表明该状态对后续状态影响较大。
效用迹也是存在上限的，它的瞬时最高上限是:
$$
E_{sat}=\frac{1}{1-\gamma \lambda}
$$
如果我们更新状态价值时同时考虑效用，价值更新就变成了:
$$
\delta_{t}=R_{t+1}+\gamma (V(S_{t+1})-V(S_t))\\
V(s) \leftarrow V(s)+\alpha \delta_{t} E_t(s)
$$

# 不基于模型的控制
不基于模型的控制，指的是在不基于模型的条件下如何通过个体的**学习**来优化价值函数。同时改善自身的行为策略来最大化获得累积奖励的过程。
指导个体与环境进行实际交互的行为的策略称为行为策略，评价状态或行为价值的策略或者说待优化的策略称为目标策略。
个体学习过程中如果行为策略与目标策略为同一策略，则称为**现时策略学习**。如果优化的策略与行为策略是不同的策略时，称为**借鉴策略学习**。
> 举例说明什么是行为策略，什么是目标策略。
假设你想要减肥，你可以设计两个策略：
目标策略：每天只吃蔬菜和水果。
行为策略：每天吃三餐，不吃零食，不喝含糖饮料。
在这个例子中，目标策略是你想要实现的最终目标，即只吃蔬菜和水果。行为策略则是你实际采取的行动方式，即每天吃三餐、不吃零食、不喝含糖饮料。行为策略和目标策略不一定完全一致，但是行为策略会有助于你逐步实现目标策略。

通常情况下，目标策略可能会比较激进或者冒险，因为它是为了最大化长期回报而设计的，而这可能需要在短期内做出一些风险决策。但在实际应用中，我们可能更倾向于使用更保守的策略，以最小化风险和损失。因此，在许多情况下，我们会使用不同于目标策略的行为策略，以平衡长期回报和风险之间的权衡。此外，行为策略还可以根据当前状态和环境来进行调整，以应对突发事件和变化的情况。
在学习的过程中，**使用行为策略可以帮助我们探索新的状态和动作**，从而提高我们的学习效率。而**使用目标策略可以让我们更加注重利用已有的知识**，提高学习的效果。当我们发现目标策略中存在一些不合理之处时，我们可以通过学习来改善目标策略，从而更好地实现我们的目标。
## $\epsilon 贪婪策略$
简而言之，$\epsilon$贪婪策略在牺牲了部分最优性的基础上，使得算法保持了一定的探索性。
$$
\pi(a\mid s)=
\left
\{\begin{matrix}
\epsilon / m + 1 - \epsilon \qquad if \ a^{*} = \argmax _{a \in A} Q(s,a)\\
\epsilon / m  \qquad else \\
\end{matrix}
\right.
$$
## 实时蒙特卡洛策略控制
实时蒙特卡洛策略控制采用$\epsilon$贪婪策略进行迭代，在采样得到了一条完整的状态序列后就可以开始迭代状态行为对的价值。并持续的对策略进行评估和改善。
然而，使用该策略的实时蒙特卡洛策略控制仍然只能得到一个基于该策略的近似行为价值函数，因为该策略一直在探索，没有终止条件。
所以我们希望关注两个方面，一方面我们不想丢弃更好的信息和状态，另一方面随着我们策略的改善最终我们希望可以收敛至某个最优策略。
为此引入了一个名为$GILE(greedy\ in\ the\ limit\ with\ infinte\ exploration)$。它有两种含义，分别是:
1. 所有的状态行为对都会被无限次探索。
$$
\lim_{k \rightarrow \infty}{N_k(s,a)}=\infty
$$
2. 随着采样趋向于无穷多，策略会收敛至一个贪婪策略。
$$
\lim_{k \rightarrow \infty}{\pi_k(a\mid s)}=1(a=\argmax _{a^{'} \in A}{Q_k{(s,a^{'})}})
$$
## 实时策略时序差分控制
### Sarsa算法
相比TD算法更新状态价值，Sarsa算法的思想是：使用后续的行为价值，更新当前状态-行为对的行为价值。
更具体的说，针对一个状态$s$，个体通过行为策略产生一个行为$a$，执行该行为进而产生一个状态-行为对$(s,a)$，并基于此获得一个即时奖励$R_s^a$并进入状态$s^{'}$。在新的状态$s^{'}$下，个体遵循策略产生一个新的行为$a^{'}$。但是此时个体并不立即执行该行为，而是通过行为价值函数得到$q_\pi(s^{'},a^{'})$的价值。利用即时奖励$R_s^a$和这个$q_\pi(s^{'},a^{'})$来更新前一个状态-行为对$(s,a)$的价值。 \
\
相较于MC算法，Sarsa算法在单个状态序列的每一个时间步，在状态$s$下采取一个行为$a$到达状态$s^{'}$后都要更新状态行为对$(s,a)$的价值$q_\pi(s^{'},a^{'})$，这一过程中同样使用$\epsilon-贪婪策略$进行策略迭代。
$$
Q(s,a) \leftarrow Q(s,a)+\alpha(R_s^{a}+\gamma Q(s^{'},a^{'})-Q(s,a))
$$

### Sarsa($\lambda$)算法
类似于n步TD学习，Sarsa算法也有一个n步的概念。$n=1$时就是我们常规的Sarsa算法。
n步Sarsa是强化学习中的一种算法，是Sarsa算法的变体。在n步Sarsa算法中，智能体不需要等到当前状态的值函数被完全更新才能采取下一个行动，而是采取一系列行动，然后在一次更新中一次性更新n步之前的状态-行动值函数。

具体来说，n步Sarsa算法中，在每个时间步，智能体采取一个行动并观察到下一个状态以及对应的奖励，同时将这个状态-行动对加入n步缓存。如果当前时间步大于等于n步，则从缓存中取出之前n步的状态-行动对来进行更新，具体更新公式如下：
$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(G_{t:t+n}-Q(S_t,A_t))
$$
其中，$G_{t:t+n}$是从时间步$t$开始，连续$n$步的回报值，可以使用下面的公式计算：
$$
G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n},A_{t+n})
$$
相比于标准的Sarsa算法，n步Sarsa算法的优势在于可以更快地进行更新，同时也可以减小由于单步更新带来的方差过大的问题。
类似于TD($\lambda$)，可以给n步Sarsa中的Q收获的每一步分配一个权重，并按照权重对每一步的Q收获求和，那么就得到$q^{\lambda}$收获:
$$
q^{\lambda}_t=(1-\lambda)\sum_{n=1}^{\infty}{\lambda}^{n-1}q_t^{(n)}
$$
如果用某一个状态的$q^{\lambda}_t$来更新状态行为对的Q值，那么可以表示成如下的形式:
$$
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha(q^{(\lambda)}_{t}-Q(S_t,A_t))
$$
上述公式也是对于$Sarsa(\lambda)$的前向认识，使用它更新Q价值需要遍历完整的状态序列。
我们同样可以反向认识$Sarsa(\lambda)$，同样引入效用追迹。E值针对的不是一个状态，而是一个状态行为对:
$$
E_0(s,a)=0 \\
E_t(s,a)=\gamma \lambda E_{t-1}(s,a)+1(S_t=s,A_t=a), \gamma,\lambda \in [0,1]
$$
基于反向认识$的Sarsa(\lambda)$算法可以有效地在线学习，数据学习完毕即可丢弃。
值得注意的是，效用迹针对的仅仅只是状态序列，因此每当一条状态序列结束时需要将其重新置零。而算法更新的Q和E则是针对个体掌握的整个状态空间和行为空间产生的Q和E。
## 借鉴策略$Q-learning$算法
借鉴策略学习中，产生指导自身行为的策略$\mu(a\mid s)$和更新状态行为对价值时使用的目标策略$\pi(a\mid s)$不是同一个策略。
具体的说，个体通过策略$\mu(a\mid s)$和环境产生实际交互，而使用目标策略$\pi(a\mid s)$来更新状态-行为对的价值。
目标策略$\pi(a\mid s)$多数是已经具备了一定“能力”的策略，也就是已有的经验。
借鉴学习TD学习任务就是使用TD方法在目标策略$\pi(a\mid s)$的基础上更新行为价值，进而优化行为策略：
$$
V(s_t) \leftarrow V(s_t)+\alpha \bigg (\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}\big [R_{t+1}+\gamma V(S_{t+1})-V(S_t)\big ] \bigg )
$$
其中我们关注的重点是这部分:
$$
\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}
$$
这是目标策略与行为策略之间的比值。\
当这个比值接近于1时，说明目标策略与行为策略在该状态下采取该动作的概率差不多，也就是说两个策略都支持这样的行为。\
当$\mu(A_t\mid S_t)$更大时，比值会小于1。这时可以认为我们的目标策略$\pi(A_t\mid S_t)$并不是很支持这样的行为，因此对于价值函数的更新需要打一些折扣。
当$\mu(A_t\mid S_t)$更小时，比值会大于1。这时可以认为我们的目标策略$\pi(A_t\mid S_t)$对于我们的行为策略更新的效果并不是非常满意。于是会增大更新的程度，鼓励更大胆的更新当前的价值。
### $Q-learning$
借鉴策略TD学习中的一个**典型的行为策略**$\mu(a\mid s)$是**基于行为价值函数**$Q(s,a)$的$\epsilon -greedy\ policy$，而**目标策略**则是基于$Q(s,a)$的完全贪婪策略，这种学习方式称为$Q-learning$。
$Q-learning$的目标是得到最优价值$Q(s,a)$，在这过程中，$t$时刻与环境进行实际交互的行为$A_t$由策略$\mu$产生:
$$
A_t \sim \mu(·|S_t)
$$
而$t+1$时刻用来更新Q值的行为$A_{t+1}^{'}$的行为由目标策略$\pi(a\mid s)$产生:
$$
A_{t+1}^{'} \sim \pi(·|S_{t+1})
$$
注意到上述的行为策略$\mu(a\mid s)$和目标策略$\pi(a\mid s)$分别是一个$\epsilon -greedy\ policy$和一个完全贪婪策略。
$Q(S_t,A_t)$按照下面的式子进行更新:
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha \bigg( {\color{red} R_{t+1}+\gamma Q(S_{t+1},A^{'})} - Q(S_t,A_t) \bigg )
$$
其中，红色部分是基于目标策略$\pi(a,s)$产生的行为$A^{'}$得到的Q值。在这样的条件下，状态$S_t$的行为$A_t$的价值会朝着目标策略下确定的最大行为价值的方向做一定比例的更新。
> 如果行为 $A_t$ 和目标策略的行为不是同一个行为，那么在 Q-learning 算法中，$Q(S_t, A_t)$ 的值会被更新，但是更新的幅度会比较小。
具体来说，假设当前状态为 $S_t$，当前采取的行为为 $A_t$，根据 Q-learning 算法的更新公式：
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha \bigg(R_{t+1}+\gamma \max_{a^{'}}{Q(S_{t+1},A^{'})} - Q(S_t,A_t) \bigg )
$$
其中，$\max_{a^{'}}{Q(S_{t+1},A^{'})}$ 表示在下一个状态 $S_{t+1}$ 时采取所有行为 $a$ 中最大的 Q 值，即在下一个状态下的最优行为价值。
如果 $A_t$ 和目标策略的行为不同，那么目标策略下的最大行为价值不一定是 $Q(S_{t+1}, A_t)$，而是 $\max_{a} Q(S_{t+1}, a)$，因为采取的行为 $A_t$ 可能不是目标策略下的最优行为。因此，$Q(S_t, A_t)$ 的更新量就会比较小，这也是 Q-learning 算法中存在一定偏差的原因。

## 值函数近似
在之前的学习中，我们使用表格的形式来记录所有的状态或者行为的价值。例如:
```
V = np.zeros((32, 12))
```
这就可以看作是一个记录了状态价值的表格，其中状态由两个离散的值来表示，比如二维世界中智能体所在的坐标等。
然而，如果问题的规模变得更大。且不谈复杂问题，仅仅是将二维世界中的智能体换到三维，并增大空间中位置的数量为100，那么我们采用之前的方式来表示状态价值函数的话：
```
V = np.zeros((100, 100, 100))
```
可以看到这就是一个非常大的空间了，包含了一百万个不同的状态。如果我们想要对所有的状态进行更新来使得智能体具有较好的训练效果，则需要通过非常非常多次的迭代才有可能尽量的访问到所有的状态。
因此我们引入了值函数近似的概念，使用一个函数来对状态价值函数进行拟合。例如如下的形式:
$$
v_{\pi}(s)\approx as + b\quad a,b \in \Bbb R
$$
我们也可以将其写成向量的形式：
$$
v_{\pi}(s) \approx as + b = \underbrace{[s,1]}_{\Phi^{T}(s)} + \underbrace{
\begin{bmatrix}
a \\
b
\end{bmatrix}
}_{w}={\Phi^{T}(s)w}
$$
其中，$\underbrace{[s,1]}_{\Phi^{T}(s)}$这部分被称为状态s的特征向量，$w$表示参数向量。这是一个用于对状态价值函数进行近似的一条直线。
我们同样可以采用曲线的方式来对状态价值函数进行拟合，那么就有如下的形式:
$$
v_{\pi}(s) \approx as^{2} + bs + c = \underbrace{[s^{2},s,1]}_{\Phi^{T}(s)} + \underbrace{
\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}
}_{w}={\Phi^{T}(s)w}
$$
尽管特征向量${\Phi^{T}(s)}$和参数向量$w$的维度提高了，但是它们对状态价值函数的拟合也更近似了。
值得注意的是，尽管$v_{\pi}(s)$对于状态$s$是非线性的函数，它对参数$w$仍然是线性的。
我们同样可以使用更多分量的向量来表示状态$s$的特征，从而提升拟合的精度。如果我们缺乏对于特征的认识，也可以使用神经网络让网络学习特征，自动拟合点。
另一个重要的问题是如何找到最优的参数向量（即如何更好的拟合），我们可以通过优化如下的目标函数来找到最优的$w$:
$$
J_1=\sum_{i=1}^{{|S|}}{\|{\Phi^{T}(s)w}-v_{\pi}(s_{i})\|}^{2}={\|
\begin{bmatrix}
{\Phi^{T}(s_1)} \\
... \\
{\Phi^{T}(s_{|S|})}
\end{bmatrix}
w-
\begin{bmatrix}
{v_{\pi}(s_1)} \\
... \\
{v_{\pi}(s_{|S|})}
\end{bmatrix}
\|}^{2}={\|{\Phi w-v_{\pi}}\|}^{2}
$$
>这是策略评估问题中的均方误差损失函数。其中，$J_1$表示策略评估的损失函数，$\Phi$是状态特征向量的转置矩阵，$w$是待求的值函数的权重向量，$v_{\pi}(s_i)$表示在策略$\pi$下状态$s_i$的真实价值。该损失函数的目标是最小化预测值与真实值之间的均方误差，从而得到最优的权重向量$w$。

>最小二乘法是一种通过最小化预测值与真实值之间的均方误差来学习模型参数的方法。在这个公式中，我们希望通过学习一个权重向量$w$来逼近真实的状态价值函数$v_{\pi}$。
为了实现这个目标，我们首先需要将每个状态$s_i$映射到一个特征向量$\Phi(s_i)$上，这个特征向量也被称为设计矩阵。我们可以手工设计特征向量，也可以通过神经网络等方式自动学习到特征向量。
假设我们有$|S|$个状态，那么我们可以将所有状态的特征向量组合成一个矩阵$\Phi$，其中每一行对应一个状态。类似地，我们将所有状态的真实价值组成一个向量$v_{\pi}$。那么，我们的目标就是找到一个权重向量$w$，使得$\Phi w \approx v_{\pi}$，即预测值与真实值之间的均方误差最小。
为了实现这个目标，我们可以使用最小二乘法。具体来说，我们需要最小化损失函数$J_1$，其中$J_1$定义为所有状态的预测值与真实值之间的均方误差之和。在公式中，$J_1$的第一个等号展开了损失函数的表达式，第二个等号将所有状态的特征向量和真实价值组合成了矩阵形式，第三个等号展开了矩阵形式的表达式，将其转化为向量形式。
最小化损失函数$J_1$的过程可以通过求解$J_1$对$w$的梯度为零得到。具体来说，我们可以对$J_1$关于$w$的梯度进行求导，得到一个关于$w$的线性方程组。通过解这个方程组，我们就可以得到最优的权重向量$w$，使得预测值与真实值之间的均方误差最小。
总之，这个公式体现了最小二乘法的思想，通过最小化预测值与真实值之间的均方误差来学习模型参数。

>要求该损失函数的最小值，可以使用最小二乘法来求解。最小二乘法是一种经典的线性回归方法，可以通过最小化均方误差来求解最优解。
具体来说，我们可以将损失函数$J_1$对权重向量$w$求偏导数，令其为0，即：
$$
\frac{\partial J_1}{\partial w} = 2\Phi^T(\Phi w - v_\pi) = 0
$$
解得最优权重向量为：
$$
w^{*} = (\Phi^T \Phi)^{-1} \Phi^T v_\pi
$$
其中，$(\Phi^T \Phi)^{-1}$表示$\Phi^T \Phi$的逆矩阵。
需要注意的是，当$\Phi^T \Phi$不可逆时，最小二乘法无法求解最优解。此时，可以使用正则化方法（如岭回归、Lasso回归等）来解决。

### 目标函数的推导与计算
设$v_{\pi}{(s)}$和$\hat{v}{(s, w)}$分别是真实的状态价值和近似值。
我们的目的是要使得它们的误差尽可能小，因此值函数的评估的目标函数可以写作:
$$
J(w)=\Bbb E[(v_{\pi}(s) - {\hat{v}{(S,w)}})^{2}]
$$
<font color="red">注意到，我们这里的状态$s$是一个随机变量，且由于状态价值函数的计算是依赖于策略的，所以我们需要对损失函数求期望。</font>
>在某些情况下，我们可以将每个状态视为同等重要，这时可以对所有状态的损失函数值求平均来估计整个状态空间的期望损失。这种方法称为均方误差（Mean Squared Error, MSE），其损失函数形式如下：
$$
J(w) = \frac{1}{|S|} \sum_{s \in S}(v_\pi(s) - \hat{v}(s,w))^2
$$
其中，$|S|$表示状态空间大小。这种方法假设每个状态的出现概率相等，且每个状态的价值函数都有同样的贡献。但是，在实际情况下，不同状态的出现概率和价值函数的贡献可能是不同的，因此在某些情况下，使用均方误差可能不太合适。

然而，某些状态的重要性可能不如其他状态。例如策略可能很少访问某个状态。所以，上述方法没有考虑到马尔科夫过程的真实动态。
因此我们引入了平稳分布$(stationary\ distribution)$的概念:
设$\{d_{\pi}(s)\}_{s \in S}$是一个基于策略$\pi$的马尔科夫过程中的平稳分布，平稳分布的含义是指`在经过了长时间的运行后，智能体停留在某一个状态的停留概率。或者说经过一个长时间的运行，智能体在每个状态的次数除以运行的次数，就是状态的出现概率。`

>在策略评估问题中，我们需要对所有状态$s$求解其价值函数$v_\pi(s)$，其中$\pi$表示当前的策略。由于在实际情况下，我们并不知道每个状态的概率分布，因此我们需要对所有状态$s$取期望，将该损失函数转化为对整个状态空间的期望损失：
$$
J(w) = \sum_{s \in S} P(s) (v_\pi(s) - \hat{v}(s,w))^2
$$
其中，$P(s)$表示状态$s$出现的概率。由于通常情况下，状态出现的概率是未知的，因此我们可以将其近似为在策略$\pi$下，状态$s$的出现概率，即：
$$
P(s) \approx \frac{n(s)}{N}
$$
其中，$n(s)$表示在样本中状态$s$出现的次数，$N$表示样本总数。这样，我们就可以通过对样本中所有状态的损失函数值求平均来近似估计整个状态空间的期望损失，从而求解最优权重向量$w$。

#### 证明智能体的策略收敛至平稳分布
要证明智能体的策略会收敛到平稳分布，需要满足马尔可夫链的某些条件，包括可约性、非周期性和遍历性等。

首先，我们需要证明策略迭代算法产生的策略序列是一个可约、非周期、遍历的马尔可夫链。可约性指的是马尔可夫链中任何状态都能够到达其他状态，非周期性指的是马尔可夫链中不存在循环的长度，即不存在周期性的行为。遍历性则要求马尔可夫链中所有状态都是遍历状态。

在策略迭代算法中，每次更新策略后，都需要对新的策略进行评估和改进。具体来说，我们需要证明每次策略改进都会使策略序列变得更接近平稳分布。

设 $v_\pi$ 为当前策略 $\pi$ 的值函数，$v_{\pi'}$ 为改进后的策略 $\pi'$ 的值函数，$\rho_{\pi}$ 和 $\rho_{\pi'}$ 分别为策略 $\pi$ 和 $\pi'$ 的状态分布。根据贝尔曼方程，有：
$$v_\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')]$$
$$v_{\pi'}(s) = \sum_{a \in A} \pi'(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_{\pi'}(s')]$$
因为策略 $\pi'$ 是由策略 $\pi$ 改进而来，所以对于任意状态 $s$，都有：
$$v_{\pi'}(s) \geq v_\pi(s)$$
接下来，我们定义一个矩阵 $P$，其元素 $P_{ss'}$ 表示在策略 $\pi$ 下，从状态 $s$ 转移到状态 $s'$ 的概率：
$$P_{ss'} = \sum_{a \in A} \pi(a|s) p(s'|s,a)$$
定义矩阵 $D$ 为对角矩阵，其对角线上的元素为 $\rho_{\pi}(s)$。矩阵 $D^{-1} P$ 的元素 $(D^{-1} P_{ss'})$ 表示在策略 $\pi$ 下，从状态 $s$ 转移到状态 $s'$ 的概率，并且在转移过程中状态 $s$ 的权重被除以了 $\rho{\pi}(s)$。
然后，我们将矩阵 $D^{-1} P$ 的幂次不断增加，直到其不再变化为止。
>在迭代策略评估算法中，每次更新状态值函数时，都是通过将上一次的状态值函数乘以转移概率矩阵来得到的。也就是说，每次更新后状态值函数的变化量是由上一次状态值函数和转移概率矩阵共同决定的。当幂次不断增加时，状态值函数会不断地被更新，直到达到一定程度后，状态值函数的变化量会越来越小，最终收敛于真实的状态值函数。这个过程类似于数学中的极限，因此矩阵幂次不断增加，状态值函数会不再变化。

根据马尔科夫链的性质，状态转移矩阵P的n次幂表示n步转移后的概率分布。因此，我们可以得到：
$$\pi_{t+n} = \pi_t P^n$$
其中，$\pi_t$表示在时间t的状态分布，$\pi_{t+n}$表示在时间t+n的状态分布。
假设状态转移矩阵P存在平稳分布$\pi^*$，即：
$$\pi^* = \pi^* \cdot P$$
则有：
$$\lim_{n\to\infty} {\pi_t}{P^n} = \pi^{*}$$
因此，我们可以得到：
$$\lim_{n\to\infty} \pi_{t+n} = \pi^*$$
这意味着，随着时间的推移，状态分布将收敛到平稳分布。因此，如果我们能够证明策略迭代算法的状态转移矩阵P存在平稳分布，那么我们就可以证明智能体的策略会收敛到平稳分布。
> 在策略迭代算法中，状态转移矩阵P是一个固定的矩阵，而不是一个随机矩阵。因此，P本身不存在平稳分布。但是，在策略迭代算法中，我们通常会对P进行加权，使得加权后的矩阵Q存在平稳分布。具体来说，我们可以将P乘以一个对角矩阵D，其中对角线上的元素为状态权重的倒数，即$D_{ii} = 1/\rho_\pi(i)$。得到加权后的矩阵$Q = D^{-1}P$，这个矩阵就可以存在平稳分布了。

我们可以通过以下步骤证明：
1. 首先，由于$Q$是一个马尔可夫链的转移矩阵，因此它满足马尔可夫性质，即它的任意两个状态之间的转移概率只与它们本身有关，而与之前的状态无关。
2. 其次，由于$Q$是一个加权矩阵，因此它满足细致平稳条件，即对于任意两个状态$i$和$j$，它们之间的转移概率满足：
$$\pi(i)Q_{ij} = \pi(j)Q_{ji}$$
其中，$\pi(i)$和$\pi(j)$分别表示状态$i$和$j$的平稳分布概率。
3. 最后，我们需要证明加权矩阵$Q$存在平稳分布。由于$Q$是一个非负矩阵，因此根据Perron-Frobenius定理，$Q$存在一个非负的左特征向量$\pi$，对应于矩阵$Q$的最大特征值1。由于$Q$是一个细致平稳矩阵，因此该特征向量就是$Q$的平稳分布。因此，我们证明了加权矩阵$Q$存在平稳分布，即矩阵$D^{-1}P$存在平稳分布。

#### 书中给出的平稳分布证明过程
假设$d_0 \in \Bbb R^{S}$是状态的初始概率分布向量。例如，如果总是选择状态$s$作为起始状态，则有$d_0(s)=1$，$d_0$的其他项都为0。
设$d_k$为从$d_0$开始的$k$步后的概率分布向量，那么就有:
$$
d_k^{T}=d_0^{T}P_{\pi}^{k}
$$
左乘$d_0$的原因在于，$P_{\pi}^{k}$的第$i$列表示了状态$i$经过$k$步后到达每个状态的概率。
当我们考虑马尔科夫过程的长期行为时，它在某些条件下是成立的:
$$
P_{\pi}^{k} \rightarrow W = 1_nd_{\pi}^T,\quad as \quad k \rightarrow \infty
$$
> 这个公式说明了在马尔可夫决策过程中，当$k$趋近于无穷大时，从任意状态$i$出发，经过$k$步后到达任意状态$j$的概率$P_{\pi}^{k}(i,j)$会收敛到一个稳定的值$W_j$。其中$W$是一个向量，表示从任意状态$i$出发，经过无限步后到达各个状态$j$的概率。$d_{\pi}$是一个向量，表示在策略$\pi$下，各个状态被访问的概率。因此，$W$可以看做是$d_{\pi}$的转置，即$W=1_nd_{\pi}^T$。

如果上述公式成立，那么就有:
$$
d_k^{T}=d_0^{T}P_{\pi}^{k} \rightarrow d_0^{T}W = d_0^{T}1_nd_{\pi}^T = d_{\pi}^{k} \quad k \rightarrow \infty
$$
>这个公式说明了在马尔可夫决策过程中，当$k$趋近于无穷大时，从任意状态$i$出发，经过$k$步后到达各个状态$j$的概率$d_k^{T}$会收敛到一个稳定的值$d_{\pi}^{k}$。其中$d_0^{T}$是一个向量，表示起始状态为各个状态$i$时的概率。$W$是一个向量，表示从任意状态$i$出发，经过无限步后到达各个状态$j$的概率。$d_{\pi}^{k}$是一个向量，表示在策略$\pi$下，经过$k$步后到达各个状态$j$的概率。公式右边的$d_0^{T}W$表示在策略$\pi$下，从任意状态出发，经过无限步后到达各个状态$j$的概率，即$d_{\pi}^{k}$。因此，当$k$趋近于无穷大时，$d_k^{T}$会收敛到$d_{\pi}^{k}$，即策略$\pi$下经过$k$步后到达各个状态$j$的概率。
这意味着状态分布收敛到常数值$d_{\pi}$，这被称为平稳分布。平稳分布取决于马尔可夫过程的动态，因此取决于策略$\pi$。平稳分布与初始分布$d_0$无关，即无论智能体从那个状态开始经过足够长的时间后，它所处的状态都可以用平稳分布来描述。

$d_{\pi}$的值究竟是多少？因为$d_k^{T} = d_{k-1}^{T}P_{\pi}$以及$d_k^{T}, d_{k-1}^{T} \rightarrow d_{\pi}$。所以有:
$$
d_{\pi}^{T}=d_{\pi}^{T}P_{\pi}
$$
因此，$d_{\pi}$是$P_{\pi}$的特征值为1的特征向量。
- **Q：那么，在什么样的条件下平稳分布存在呢？**
- A：具有唯一平稳分布的一类一般马尔科夫过程是正则马尔科夫过程。要定义正则的概念，则需要首先了解以下概念。
如果对于某个有限的整数$k$，有$[P\pi]_{ij}^{k}>0$，则称状态$j$对于状态$i$来说是可进入的，这意味着智能体从状态$i$出发经过有限次的步数即可到达状态$j$。
如果状态$i$和状态$j$是相互可访问的，则称这两个状态为可通信的。
如果所有状态都相互通信，则马尔科夫过程称为不可约过程。换而言之，从任意状态开始的智能体总是可以在有限步数内到达任何其他状态。
如果一个马尔科夫过程不可约，且存在一个状态使得$[P\pi]_{ii}>0$，则该马尔科夫过程称为正则过程。$[P\pi]_{ii}>0$表明智能体可以从状态$i$开始经历一步到达状态$i$。
对于正则过程，存在唯一平稳分布$d_{\pi}^{T}=d_{\pi}^{T}P_{\pi}$
> 正则过程存在唯一平稳分布的原因是基于它的马尔可夫链具有一些特殊的性质。具体来说，正则过程的转移矩阵是可逆的，也就是说，它的每个状态都可以通过一系列转移到达任何其他状态。这意味着该马尔可夫链是非周期的、可约的，且不含非常返态。
由于该马尔可夫链是可逆的，我们可以通过计算该链的平稳分布来获得该马尔可夫链的唯一平稳分布。另外，由于正则过程是可约的且不含非常返态，因此其平稳分布是唯一的。
需要注意的是，正则过程存在唯一平稳分布的条件是有限状态且可逆。在无限状态的情况下，存在多个平稳分布的情况。

>一个具有有限状态空间的马尔科夫过程存在唯一平稳分布，当且仅当它是不可约的、正则的和具有可数的状态。其中，不可约意味着所有状态都是可达的，即从任意状态i可以到达状态j；正则意味着从任意状态i，存在一个整数k，使得在k步之后，状态i可以到达所有其他状态。

正则过程的转移矩阵是可逆的，这是因为正则过程是不可约的且具有唯一平稳分布。具体证明如下：

假设 $P$ 是一个 $n \times n$ 的正则转移矩阵，即对于某个正整数 $m$，$P^m$ 中的所有元素都是正的。那么我们定义 $v$ 是一个 $n$ 维列向量，其中每个元素都为 $1/n$，即 $v_i = 1/n$。显然 $v$ 是一个概率分布向量，因为其所有元素都是非负的并且和为 $1$。

接下来，我们证明 $v$ 是 $P$ 的唯一平稳分布，即对于任意初始概率分布向量 $u$，其极限分布都将收敛到 $v$。

我们可以将 $u$ 表示为 $u = \sum_{i=1}^n a_i v_i$ 的形式，其中 $a_i$ 是一个实数。根据转移矩阵的定义，我们可以得到：
$$
\lim_{k\to\infty} P^k u = \lim_{k\to\infty} \sum_{i=1}^n a_i P^k v_i
$$
由于 $P$ 是可逆矩阵，因此存在一个逆矩阵 $P^{-1}$。因此，我们可以将 $P^k v_i$ 表示为 $P^k v_i = P^{k-1} (P v_i)$ 的形式。由于 $v$ 是 $P$ 的平稳分布，因此 $Pv = v$，因此有 $P^k v_i = v$。因此，我们可以将 $\lim_{k\to\infty} P^k u$ 重写为：
$$
\lim_{k\to\infty} \sum_{i=1}^n a_i P^k v_i = \lim_{k\to\infty} \sum_{i=1}^n a_i v = v \sum_{i=1}^n a_i = v
$$
因此，对于任意初始概率分布向量 $u$，其极限分布都将收敛到 $v$，因此 $v$ 是唯一平稳分布。此外，由于 $v$ 是唯一平稳分布，因此 $P$ 的任意两个平稳分布 $v_1$ 和 $v_2$ 必须相同，因此 $P$ 的转移矩阵是可逆的。
$$
\begin{bmatrix}
\pi_1 & \pi_2 & \cdots & \pi_n \\
\pi_1 & \pi_2 & \cdots & \pi_n \\
\vdots & \vdots & \ddots & \vdots \\
\pi_1 & \pi_2 & \cdots & \pi_n \\
\end{bmatrix}
$$
>基本上是对的。具体来说，正则马尔可夫过程满足从任意状态出发到其他任意状态都有一个大于0的概率，即矩阵的每个元素都大于0。而根据一个著名的定理——高斯-约旦消元法，一个矩阵可逆当且仅当它的每个列向量都是线性无关的，或者说没有一个列向量可以表示为其它列向量的线性组合。因此，如果一个马尔科夫过程的状态转移概率矩阵的所有列向量都是线性无关的，那么这个矩阵就是可逆的，从而存在唯一平稳分布。

- **Q:什么样的策略会导致平稳分布？**
- A:探索性的策略。例如epsilon greedy policy，这是因为这样的策略会使得每个行动都有一个正数的概率被采取，因此当系统模型允许时，状态可以在epsilon greedy策略下相互通信。

### 优化算法
首先回顾一下上面的目标函数:
$$
J(w)=\Bbb E[(v_{\pi}(s) - {\hat{v}{(S,w)}})^{2}]
$$
如果想要优化这个函数，可以采用梯度下降的方法:
$$
\begin{align}
w_{k+1}&=w_k-\alpha_k \nabla_w \Bbb E[(v_{\pi}(S)-\hat{v}(S,w))^{2}] \\
&=w_k-\alpha_k \Bbb E[\nabla_w(v_{\pi}(S)-\hat{v}(S,w))^{2}] \\
&=w_k-2\alpha_k \Bbb E[(v_{\pi}(S)-\hat{v}(S,w))(-\nabla_w \hat{v}(S,w))] \\
&=w_k+2\alpha_k \Bbb E[(v_{\pi}(S)-\hat{v}(S,w))\nabla_w \hat{v}(S,w)]
\end{align}
$$
不失一般性的，我们可以将系数2给合并到步长因子$\alpha$中。由于上述的梯度下降算法需要计算期望，因此我们可以利用随机梯度下降的方式去掉期望，得到如下算法:
$$
w_{t+1}=w_{t}+\alpha_t (v_{\pi}(s_t)-\hat{v}(S,w))\nabla_w \hat{v}(S,w)
$$
其中，$s_t$表示状态空间$S$中的一个采样。
然而，上述这个公式是无法实现的。因为它需要我们的真实状态值$v_{\pi}(s)$。如果我能够知道真实的$v_{\pi}(s)$，就没有必要再去最小化目标函数来找到一个合适的对$v_{\pi}(s)$的估计了。
<font color="red">这里的关键还是在于，我们并不知道状态转移概率矩阵$P$。如果我们知道这个概率矩阵，则可以直接利用求解$\pi P = \pi$这个方程来得到相对应的马尔科夫过程的平稳分布。如果我们得到了平稳分布，就可以直接代入到目标函数中直接求解。</font>
因此我们可以采取一些方式来估计这个$v_{\pi}(s)$，那么不可避免的就需要谈到无模型的学习算法。因为我们无从得知状态转移信息，就只能使用无模型的方法来对$v_{\pi}(s)$进行一个估计。
假设我们有一个状态序列:$(s_0,r_1,s_1,r_2...)$
那么，我们可以采用蒙特卡洛算法，以当前状态$s_t$为起点计算$return$，用它来估计$v_{\pi}(s)$。那么就有:
$$
w_{t+1}=w_{t}+\alpha_t (g_t-\hat{v}(S,w))\nabla_w \hat{v}(S,w)
$$
理所当然的，我们还可以采用TD学习算法来估计$v_{\pi}(s)$，那么就有:
$$
w_{t+1}=w_{t}+\alpha_t (r_{t+1}+\gamma \hat{v}(s_{t+1},w)-\hat{v}(S,w))\nabla_w \hat{v}(S,w)
$$
### 线性函数近似
上述TD算法包含了一个重要的问题，那就是我们如何选取$\hat{v}(S,w)$。有两个可行的办法，第一种是使用神经网络作为非线性逼近器，其中输入为状态，输出为$\hat{v}(S,w)$，网络参数为$w$。第二种办法则是使用线性函数:
$$
\hat{v}(S,w)=\Phi^T(s)w
$$
其中,$\Phi(s)$是一个关于状态$s$的特征向量，两个向量的维度相同。在线性函数的情况下，梯度为:
$$
\nabla_w \hat{v}(S,w) = \Phi(s)
$$
代入上述的TD学习算法估计$v_{\pi}(s)$的公式，则有:
$$
w_{t+1}=w_{t}+\alpha_t (r_{t+1}+\gamma \Phi^T(s_{t+1})w_t-\hat{v}(S,w))\Phi(s)
$$
这就是线性函数逼近的TD学习算法。

### TD算法的理论分析
在上面的内容中，从优化目标函数开始:
$$
J(w)=\Bbb E[(v_{\pi}(s) - {\hat{v}{(S,w)}})^{2}]
$$
为了优化这个目标函数，我们首先采用梯度下降的方式，引入了随机梯度下降算法。然后，我们为了估计出$(v_{\pi}(s))$的近似值，分别采用了蒙特卡洛以及TD学习来对它进行一个估计。
在TD学习中，我们最后得到的公式为:
$$
w_{t+1}=w_{t}+\alpha_t (r_{t+1}+\gamma \hat{v}(s_{t+1},w)-\hat{v}(S,w))\nabla_w \hat{v}(S,w)
$$
但是这个公式在数学上并不是严谨的。实际上它并没有真正意义上对目标函数最小化。因此，我们在此对TD学习进行理论分析，证明这个算法为什么可行以及解决了哪些数学问题。
$$
w_{t+1}=w_{t}+\alpha_t \Bbb E(r_{t+1}+\gamma \Phi^T(s_{t+1})w_t-\hat{v}(S,w)\Phi(s))
$$
其中，期望是关于随机变量$s_t,s_{t+1},r_{t+1}$的。假设$s_t$的分布是一个平稳分布$d_{\pi}$，则上述公式的算法是确定性的，因为随机变量$s_t,s_{t+1},r_{t+1}$在计算后都会消失。
> 我来进一步解释一下。
首先，这个公式是强化学习算法中的一个更新规则，具体来说是值迭代算法中的更新规则。在这个公式中，$w$ 是值函数的参数向量，它被更新为 $w_{t+1}$，$\alpha_t$ 是学习率，$\Phi(s)$ 是状态 $s$ 的特征向量，$\hat{v}(S,w)$ 是对于状态 $S$ 的值函数的一个估计值，$\gamma$ 是折扣因子。在该公式中，我们的目标是最小化 $\Bbb E[\delta^2]$，其中 $\delta$ 是预测误差，即 $\delta = r_{t+1}+\gamma \Phi^T(s_{t+1})w_t-\hat{v}(S,w)\Phi(s)$。因此，我们将 $\delta$ 的期望嵌入到更新规则中，即 $\Bbb E[\delta] = \Bbb E[r_{t+1}+\gamma \Phi^T(s_{t+1})w_t-\hat{v}(S,w)\Phi(s)]$，得到以下更新规则：
$$
w_{t+1}=w_{t}+\alpha_t \Bbb E[\delta] \Phi(s)
$$
接下来，我们来解释为什么在这个公式中，由于期望是关于随机变量 $s_t,s_{t+1},r_{t+1}$ 的，所以公式是确定性的。
首先，我们需要理解期望的含义。期望是一个统计量，它代表着随机变量在多次试验中取值的平均数。因此，当我们计算期望时，我们需要考虑多次试验中随机变量可能取到的所有取值，以及每个取值发生的概率。在强化学习中，我们的环境是动态的，我们的智能体与环境的交互是通过多次试验实现的，每次试验中，智能体与环境的状态、动作和奖励都是随机变量。因此，我们需要计算期望来考虑多次试验中智能体和环境的随机性。
其次，对于平稳分布 $d_{\pi}$，它是指在智能体使用策略 $\pi$ 的情况下，当智能体在不同的状态上停留的时间趋于无穷大时，状态的分布将趋于稳定，即不再随时间变化而变化。因此，对于一个平稳分布，我们可以通过采样来近似计算随机变量的期望值。
最后，由于在这个更新规则中，期望是关于随机变量 $s_t,s_{t+1},r_{t+1}$ 的，但是我们已经假设了 $s_t$ 的分布是一个平稳分布。所以对于任意时刻$t$，$s_t$的分布都是$d_{\pi}$。因此，我们可以用$d_{\pi}$来计算公式中的期望值。此外，由于这是一个确定性算法，因此在计算完期望值之后，公式中所有的随机变量$s_t,s_{t+1},r_{t+1}$都会消失，公式变成一个确定的表达式，可以直接使用。这就是上述句子的含义。
具体而言，公式的含义是：用$w_t$近似值函数$v(S_t,w_t)$来计算状态$S_t$的价值，然后使用$r_{t+1}+\gamma v(S_{t+1},w_t)$来估计状态$S_t$的回报，用这个估计值和$v(S_t,w_t)$的差来更新权重向量$w$，从而逐步改进价值函数的近似。其中，$\alpha_t$是学习率，$\Phi(s)$是状态$s$的特征向量，$\gamma$是折扣因子。

---
### 函数近似的Sarsa算法
为了寻找最优的策略，我们需要估计行为价值。因此我们将值函数近似引入Sarsa算法来估计行为价值。
在Sarsa函数逼近的每次迭代中包含两个步骤，首先是策略评估，即对当前的行为价值进行评估。 其次是策略改进，即根据当前的行为价值找到一个软贪婪策略。\
Sarsa函数逼近和Q表式的Sarsa算法区别仅在第一步的行为价值估计上有差异。具体的来说，函数逼近式的Sarsa算法的行为价值是使用$w$参数化的函数$q(s,a,w)$来描述的，使用$q(s,a,w)$来代替了原先的$q(s,a)$。函数逼近式的Sarsa算法更新规则是:
$$
w_{t+1}=w_t + \alpha \big[R_{t+1}+\gamma Q(S_{t+1},A_{t+1},w_{t})-Q(S_t,A_t,w_t)\big]\nabla_w Q(S_t,A_t,w_t)
$$
其中，$w_t$表示第$t$次迭代中的权重，$Q(S_t,A_t,w_t)$是函数逼近得到的状态-动作值函数，$\alpha$是学习率，$R_{t+1}$是在状态$S_t$采取动作$A_t$后得到的即时奖励，$\gamma$是折扣因子，$S_{t+1}$和$A_{t+1}$是下一个状态和动作。$\nabla_w Q(S_t,A_t,w_t)$表示状态-动作值函数在当前状态和动作下关于权重$w$的梯度。该更新规则使用当前状态-动作对$(S_t,A_t)$的估计来更新权重$w$，以逼近真实的状态-动作值函数$Q^*$。
其中，$Q$可以由以下公式得到:
$$
Q(s,a)=\phi(s,a)^Tw 
$$
其中，$\phi(s,a)$ 是状态-动作对 $(s,a)$ 的特征向量，$w$ 是线性函数逼近的参数向量。它的结果是一个标量，因此也可以写成$Q(s,a)=w^T\phi(s,a) $

### 函数近似的Q-learning算法
类似于Sarsa，Q-learning也可以用值函数近似来代替原先的Q表，函数逼近式的Q-learning算法的更新规则为：
$$
w_{t+1} = w_t + \alpha_t \left( R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a', w_t) - Q(S_t, A_t, w_t) \right) \nabla_{w_t} Q(S_t, A_t; w_t)
$$
其中，$w_t$为$t$时刻的权重向量，$S_t$为$t$时刻的状态，$A_t$为$t$时刻的动作，$R_{t+1}$为$t+1$时刻的奖励，$\alpha_t$为$t$时刻的学习率，$\gamma$为折扣因子，$Q(S_t, A_t; w_t)$为状态-动作值函数的估计值，$\max_{a'} Q(S_{t+1}, a'; w_t)$为下一个状态所有可能动作的状态-动作值函数的最大值。
更新规则中的$\nabla_{w_t} Q(S_t, A_t; w_t)$表示在当前权重向量下，对状态-动作值函数的估计值$Q(S_t, A_t; w_t)$求关于$w_t$的梯度。通过不断迭代更新$w_t$，可以逐渐优化状态-动作值函数的估计，从而得到最优的策略。

### Deep Q-learning
我们可以在Q-learning中引入深度神经网络，得到深度Q-learning或者说Deep Q-learning(DQN)。它的思想类似于函数近似的Q-learning算法，但是它的数学公式和实现和它有着本质上的不同。
DQN学习的目标函数是平均方误差（mean-squared error, MSE）损失函数，其具体形式为：
$$
\mathcal{L}(w)=\mathbb{E}_{(s,a,r,s')\sim \mathcal{D}}\left[(\underbrace{(r+\gamma\max_{a'}Q(s',a';w^-)}_{y}-Q(s,a;w))^2\right]
$$
其中，$\mathcal{D}$是经验回放缓存中保存的样本集合，$w$是当前Q网络的参数，$w^-$是目标Q网络的参数，$s,a,r,s'$是经验回放缓存中的一个样本，$\gamma$是折扣因子。

这个目标函数的含义是，用当前Q网络的输出$Q(s,a;w)$来逼近真实的行为价值$Q^*(s,a)$，即最小化当前Q网络的预测值与真实值之间的平均方差。其中，真实值被用$r+\gamma\max_{a'}Q(s',a';w^-)$来估计，它表示从下一个状态$s'$开始采取最优动作$a'$后能够得到的最大收益，再加上当前时刻的即时奖励$r$。
>经验回放缓存（Experience Replay）是深度强化学习（Deep Reinforcement Learning）中常用的一种技术，用于存储智能体（Agent）在环境中的经验数据，并随机地从中选择一些样本进行训练。
\
在传统的强化学习中，智能体会不断地与环境交互，并将当前的状态、行为、奖励和下一个状态存储在内存中，然后在每个时间步进行学习。但是，这种方法存在两个问题：一是样本之间的相关性较强，二是可能会过多地利用最新的数据。
\
经验回放缓存通过将智能体的经验数据存储在一个缓冲区中，并随机抽样进行训练，解决了上述问题。具体地，当智能体与环境交互时，它将当前的状态、行为、奖励和下一个状态存储在经验回放缓存中。在训练时，智能体从经验回放缓存中随机选择一批数据进行训练，这样可以减少样本之间的相关性，同时也可以避免过度依赖最新的数据。
经验回放缓存的引入使得深度强化学习算法更加稳定和高效，是深度强化学习中不可或缺的一部分。

为了优化上述的目标函数，我们需要计算它关于$w$的梯度。值得注意的是，$w$不仅出现在了当前$Q$网络中，也出现在了目标$Q$网络中。为了方便计算，我们可以在计算梯度时假设$y$这部分的$w$是固定的(至少在一段时间内是固定的)。这里我的公式已经将其改成了$Q\ network$和$main\ network$的形式。所以就有:
$$
\mathcal{L}(w)=\mathbb{E}_{(s,a,r,s')\sim \mathcal{D}}\left[(\underbrace{(r+\gamma\max_{a'}Q(s',a';w^-)}_{y}-Q(s,a;w))^2\right]
$$
在这个公式中，$w^-$是一个固定的值。因此对上式求梯度则有:
首先，我们可以把目标$y$代入到损失函数中，得到：
$$
\begin{aligned} \mathcal{L}(w) &= \mathbb{E}_{(s,a,r,s')\sim \mathcal{D}}\left[(y-Q(s,a;w))^2\right] \\ &= \mathbb{E}_{(s,a,r,s')\sim \mathcal{D}}\left[\left(r + \gamma\max_{a'}Q(s',a';w^-) - Q(s,a,w)\right)^2\right] \end{aligned}
$$
然后我们对损失函数求梯度，得到：
$$
\nabla_w \mathcal{L}(w) = \mathbb{E}_{(s,a,r,s')\sim \mathcal{D}}\left[2\left(r + \gamma\max_{a'}Q(s',a';w^-) - Q(s,a;w)\right)\nabla_w Q(s,a;w)\right]
$$
其中$\nabla_w Q(s,a;w)$是$Q$函数关于参数$w$的梯度。
DQN的基本思想就是利用上式中的梯度最小化目标函数$\mathcal{L}(w)$。
#### 深度强化学习中使用的重要技术
1. 经验回放，在我们经过若干次的迭代获得了一些经验样本以后，我们并不按照收集的顺序使用这些样本。而是将其存储在一个数据集中，这个数据集称为回放缓存。我们从中随机的抽取一些样本来训练神经网络。特别的，让$(s,a,r,s^{'})$是一种经验，以及让$\mathbb B=\{(s,a,r,s^{'})\}$是回放缓存区。每次训练神经网络时，我们都可以从回放缓冲区中抽取一小批随机样本，样本的抽取或者称为经验重放，应该遵循均匀分布。

为什么经验回放在DQN中是必要的？为什么回放必须要遵循均匀分布?
>经验回放在DQN中是必要的，因为它可以使得模型在学习时更加稳定和高效。具体来说，经验回放可以破解时间上相邻的样本之间存在的相关性问题，避免模型因为样本之间存在相关性而导致训练不稳定的情况。此外，经验回放还可以提高数据的利用率，因为模型可以多次利用之前的经验来进行训练。
回放必须遵循均匀分布，是为了使得模型在学习过程中不会过度关注某些经验，而是平衡地利用所有经验来进行训练。如果回放的样本分布不均匀，模型可能会过度关注某些样本，导致对其他样本的学习不充分，最终影响模型的性能。因此，遵循均匀分布的经验回放可以有效地平衡样本的利用，提高模型的稳定性和泛化能力。

从另一个角度解释。首先，为了很好的定义目标函数，我们必须指定$S,A,R,S^{'}$的概率分布。$R$和$S^{'}$的分布由系统模型决定，棘手的部分是$S$以及$A$的分布。假设经验样本按照策略$\pi_b$进行收集，那么A的分布就是$\pi_b(s)~A$。如果我们将状态-行为对$(S,A)$视为单个变量而不是两个随机变量，那么我们就可以去除样本$(S,A,R,S^{'})$对于策略$\pi_b$的依赖。因为一旦$(S,A)$给定，$R$和$S^{'}$则完全由系统模型决定，假设状态-行为对$(S,A)$的分布是均匀的。
其次，尽管我们假设状态-行为对的样本是均匀分布的，但实际上由于它们是由行为策略产生的，因此并不是均匀分布。为了了打破结果样本之间的相关性以满足均匀分布的假设，因此我们使用经验重放技术，从回放缓冲区中均匀的抽取样本。这就是为什么经验回放是必要的，以及为什么经验回复是一致的数学原因。随机抽样的好处是每个经验样本可以被多次使用，可以提高数据的效率，当我们的数据量有限时，这一点尤其重要。

2. 另一项技术是使用两个网络，主网络和目标网络。它们的作用在计算梯度的公式中已经有过解释，接下来会阐述它们的实现细节。假设$w$和$w_T$分别代表主网络和目标网络的参数，它们一开始是相同的。
在每次迭代中，我们从回放缓冲区中抽取一小批样本$\{(S,A,R,s^{'})\}$。网络的输入包括状态$s$和动作$a$，而目标输出是$y_T = r + \gamma\max_{a'}Q(s',a';w_T)$。然后，我们直接通过一小批样本$\{s,a,y_T\}$而不是单个样本最小化TD误差或者称为损失函数$(y-\hat{q}(s,a,w))^{2}$来改善效率以及稳定性。注意，在更新主网络参数时，目标输出是基于目标网络计算的。
主网络的参数在每次迭代中都会更新，相比之下，在计算梯度时，目标网络被设置为每迭代一定的次数就会将$w$赋值给$w^T$以满足$w^T$固定的假设。

# 策略梯度
函数近似的思想不仅可以用来表示状态/动作值，还可以用于表示策略。
在之前的学习中，我们的策略都是用一个表的形式来表示的，所有状态下的动作概率存储在$\pi (a\mid s)$中，表中的每一项都由一个状态和一个动作进行索引。
然而，我们的策略同样可以使用一个函数的形式来表示为$\pi (a\mid s, \theta)$，其中$\theta$就是类似我们之前学习的$w$，它们都是参数向量。
当我们将策略表示成函数时，同样可以通过优化某些标量的度量来找到最优策略。但是在这之前我们需要思考的问题是，如何定义最优策略？
当我们使用表格式存储策略时，最优策略指的是对于任意状态的状态价值都*不小于*其他策略下对应状态的状态价值。而当我们使用函数来表示时，最优策略则由$\theta$和函数结构来决定。因此策略如果能够最大化某些标量的度量，则将其定义为最优策略。
我们知晓了如何定义最优策略，那么我们如何对策略进行改善呢？
在表格式的策略中，我们只需要对特定状态-行为对的概率进行更新，就达到了改善策略的目的。然而，在函数式的策略中，我们通过更新$\theta$这个参数向量的取值来更新策略。
策略梯度法的基本思想如下，假设$J(\theta)$是定义最优策略的标量度量，基于梯度算法的度量优化可以得到最优策略:
$$
\theta_{t+1}=\theta_{t}+\alpha \nabla_{\theta}J(\theta_t)
$$
其中，$\nabla_{\theta}J$是J相对于$\theta$的梯度，$t$为时间步，$\alpha$为优化速率。
如果一个策略由一个函数表示，我们应该使用什么样的指标来定义最优性？接下来介绍几种流行的指标。
1. 平均状态值，或称平均值。假设我们有:
$$
v_{\pi}=[...,v_{\pi}(s),...]^T \in \Bbb R^{|s|}
d_{\pi}=[...,d_{\pi}(s),...]^T \in \Bbb R^{|s|}
$$
$v_{\pi}$表示我们所有状态的状态价值，而$d_{\pi}$则是对应状态的权重。那么平均值可以定义为:
$$
\begin{align}
\overline{v}_{\pi} &= d_{\pi}^Tv_{\pi} \\
&= \sum_{s}{d_{\pi}(s)v_{\pi}(s)} \\
&= \Bbb E[v_{\pi}(s)]
\end{align}
$$
其中,$d_{\pi}$可以是状态数量的均值，这表示每个状态具有相同的权重。但是根据上一章值函数近似的内容我们可以了解到，我们可以使用平稳分布来给不同的状态分配不同的权重。

2. 平均单步奖励，或称平均奖励。假设我们有:
$$
r_{\pi}=[...,r_{\pi}(s),...]^T \in \Bbb R^{|s|}
$$
这代表我们每一步的即时奖励，因此有:
$$
r_{\pi}(s) = \sum_{a}{\pi(a|s)r(s,a)}
$$
那么我们的平均单步奖励就是:
$$
\begin{align}
\overline{r}_{\pi} &= d_{\pi}^Tr_{\pi} \\
&= \sum_{s}{d_{\pi}(s)r_{\pi}(s)} \\
&= \Bbb E[r_{\pi}(s)]
\end{align}
$$

3. 特定状态下的状态价值。对于某些任务，我们只能从一个特定的状态$s_0$开始。在这样的条件下我们只关注从$s_0$开始的长期收益。在这样的条件下我们同样可以把它看作是第一种度量，只不过我们给$s_0$的权重为1，其余全部为0.

对于上述的度量，需要补充一点内容。
首先，由于所有的度量都是基于策略$\pi$的，而策略$\pi$又是由$\theta$这个参数向量进行参数化的函数，所以这些度量也都是关于$\theta$的函数。因此，不同的$\theta$可以产生不同的度量值。我们可以通过寻找$\theta$的最优值来最大化这些度量。
其次，$r_{\pi}$相对来说更加短视，因为它只考虑即时奖励。
再次之，策略梯度的一个复杂之处在于。度量的定义和我们的折现率$\gamma$也有一定的关系。因为$\gamma$的取值同样会影响到状态的价值。

## 度量的梯度
对于上面介绍的指标，我们可以使用基于梯度的方法来最大化它们。
$$
\nabla_{\theta}J(\theta)=
\begin{bmatrix}
\frac{\partial{J(\theta)}}{\partial{\theta_1}} \\
\vdots \\
\frac{\partial{J(\theta)}}{\partial{\theta_n}}
\end{bmatrix}
$$
参数$\theta$采用下式更新:
$$
\theta_{t+1} = \theta_{t} + \alpha \nabla_{\theta}J(\theta)
$$
如果使用起始状态的状态价值作为对目标函数大小进行度量的标量，则对应的策略目标函数为:
$$
\begin{align}
J(\theta)&=\Bbb E_{\pi_{\theta}}[r] \\
&= \sum_{s \in S}{d(s)}\sum_{a \in A}{\pi_{\theta}(s,a)R_{s,a}}
\end{align}
$$
如果对该策略目标函数进行求梯度，则结果为:
$$
\begin{align}
\nabla_{\theta}J(\theta)&=\sum_{s \in S}{d(s)}\sum_{a \in A}{\nabla_{\theta}\pi_{\theta}(s,a)R_{s,a}} \\
&=\sum_{s \in S}{d(s)}\sum_{a \in A}{\pi_{\theta}(s,a)\nabla_{\theta}log\pi_{\theta}(s,a)R_{s,a}}\\
&=\Bbb E_{\pi_{\theta}}[\nabla_{\theta}log\pi_{\theta}(s,a)r]
\end{align}
$$
