# 变分内在控制

## 摘要

本文中引入了一种新的无监督强化学习方法来发现可用于智能体的内在选项集合。该集合通过<font color="red">最大化智能体能够可靠到达的不同状态的数量</font>来进行学习，并通过选项集合与选项终止状态之间的交互信息来衡量。为此，文中采用了两种策略梯度的算法。一种是创建选项的显式嵌入空间，另一种是隐式表示选项。这些算法还提供了一个明确的权力测量，在给定状态下可以被权力最大化的智能体使用。该算法在函数近似上的扩展性很好，文中在一系列任务上展示了该算法的适用性。

>这里大概的意思是，内在选项集合其实指的就是在某个状态下能够采取的动作的集合。例如考虑在Montezuma's revenge环境中，虽然看起来环境定义的动作空间包含了18个动作。但是在某些状态下包含了许多“无效动作”，例如在面向墙壁的时候向墙壁的方向移动，或者在上下楼梯时采取向左向右的动作，在没有获得武器的时候选择"Fire"动作等等。通过最大化智能体能够可靠到达的不同状态的数量，并采用动作集合与动作状态之间的交互信息来衡量。
>
>文中采取了两种策略梯度的算法，一个创建动作的显式嵌入空间，另一个则隐式表示选项。个人的理解是，第一个创建可能指的是创建了一个"显式的、可以直接观察和理解的"动作表达方式，例如使用一个特定形状的张量来表示一个动作，这种表达方式通常可以理解和解释。因为每个动作都有一个明确的、可观察的表示。
>
>而"隐式表达动作"可能指的是一种"隐式的、不容易直接观察和理解的"动作表达方式，例如使用一个数字来表达一个动作(比如Gym库的Atari游戏中就常用数字0表示"No operation"这个动作)。
>
>这些算法提供了一个明确的权力测量，在给定状态下可以被权力最大化的智能体使用。
>
>这个"权力最大化"，个人猜测可能指的是那些在给定状态下，尽可能地增加其影响环境状态的能力或可能性的智能体。换句话说，这种智能体会选择那些能够使其在环境中获得更多控制权的动作或策略。
>
>举个例子，假设一个智能体在一个迷宫中，它的目标是找到出口。在这个环境中，智能体可以选择向上、向下、向左或向右移动。如果我们使用一个权力最大化的代理，那么它会选择那些能够使其到达更多不同位置的动作，因为这样可以增加其在环境中的控制力。例如，如果智能体在一个十字路口，那么它可能会选择向右走，因为这样可以使其到达更多的位置，而不是选择向上走，因为那样可能只能到达一个死胡同。
>
>这种权力最大化的策略可以帮助智能体更有效地探索环境，并找到达到目标的最优策略。
>
>这种策略同时能够在一定程度上解决“无效动作”的问题，因为它倾向于选择那些能够到达更多不同状态的动作。然而其缺点在于，智能体容易陷入过度探索。因为它鼓励智能体选择那些能够到达更多状态的动作，智能体可能花费大量时间和资源探索环境而忽略那些直接带来奖励的动作。
>
>其次，权力最大化的策略可能会导致智能体陷入“探索-利用”的困境。在强化学习中，智能体需要在探索新的动作和利用已知的奖励之间找到一个平衡。如果智能体过于关注权力最大化，那么它可能会过度探索，而忽视了利用已知的奖励。
>
>最后，权力最大化的策略可能会导致智能体忽视那些短期内看起来不太有用，但长期来看可能非常有价值的动作。因为这种策略倾向于选择那些能够立即带来更多状态的动作，所以它可能会忽视那些需要长期投入才能看到回报的动作。
>
>因此，虽然权力最大化的策略在一些情况下可能非常有效，但在其他情况下，它可能并不是最优的策略。在实际应用中，可能需要根据具体的任务和环境来选择最合适的策略。