# 变分内在控制

## 摘要

本文中引入了一种新的无监督强化学习方法来发现可用于智能体的内在选项集合。该集合通过<font color="red">最大化智能体能够可靠到达的不同状态的数量</font>来进行学习，并通过选项集合与选项终止状态之间的交互信息来衡量。为此，文中采用了两种策略梯度的算法。一种是创建选项的显式嵌入空间，另一种是隐式表示选项。这些算法还提供了一个明确的权力测量，在给定状态下可以被权力最大化的智能体使用。该算法在函数近似上的扩展性很好，文中在一系列任务上展示了该算法的适用性。

>这里大概的意思是，内在选项集合其实指的就是在某个状态下能够采取的动作的集合。例如考虑在Montezuma's revenge环境中，虽然看起来环境定义的动作空间包含了18个动作。但是在某些状态下包含了许多“无效动作”，例如在面向墙壁的时候向墙壁的方向移动，或者在上下楼梯时采取向左向右的动作，在没有获得武器的时候选择"Fire"动作等等。通过最大化智能体能够可靠到达的不同状态的数量，并采用动作集合与动作状态之间的交互信息来衡量。
>
>文中采取了两种策略梯度的算法，一个创建动作的显式嵌入空间，另一个则隐式表示选项。个人的理解是，第一个创建可能指的是创建了一个"显式的、可以直接观察和理解的"动作表达方式，例如使用一个特定形状的张量来表示一个动作，这种表达方式通常可以理解和解释。因为每个动作都有一个明确的、可观察的表示。
>
>而"隐式表达动作"可能指的是一种"隐式的、不容易直接观察和理解的"动作表达方式，例如使用一个数字来表达一个动作(比如Gym库的Atari游戏中就常用数字0表示"No operation"这个动作)。
>
>这些算法提供了一个明确的权力测量，在给定状态下可以被权力最大化的智能体使用。
>
>权力，简单来说可以看作是智能体对于环境的掌控程度。权力越大，智能体越有能力根据自身的行动控制环境。
>
>这个"权力最大化"，个人猜测可能指的是那些在给定状态下，尽可能地增加其影响环境状态的能力或可能性的智能体。换句话说，这种智能体会选择那些能够使其在环境中获得更多控制权的动作或策略。
>
>举个例子，假设一个智能体在一个迷宫中，它的目标是找到出口。在这个环境中，智能体可以选择向上、向下、向左或向右移动。如果我们使用一个权力最大化的代理，那么它会选择那些能够使其到达更多不同位置的动作，因为这样可以增加其在环境中的控制力。例如，如果智能体在一个十字路口，那么它可能会选择向右走，因为这样可以使其到达更多的位置，而不是选择向上走，因为那样可能只能到达一个死胡同。
>
>这种权力最大化的策略可以帮助智能体更有效地探索环境，并找到达到目标的最优策略。
>
>这种策略同时能够在一定程度上解决“无效动作”的问题，因为它倾向于选择那些能够到达更多不同状态的动作。然而其缺点在于，智能体容易陷入过度探索。因为它鼓励智能体选择那些能够到达更多状态的动作，智能体可能花费大量时间和资源探索环境而忽略那些直接带来奖励的动作。
>
>其次，权力最大化的策略可能会导致智能体陷入“探索-利用”的困境。在强化学习中，智能体需要在探索新的动作和利用已知的奖励之间找到一个平衡。如果智能体过于关注权力最大化，那么它可能会过度探索，而忽视了利用已知的奖励。
>
>最后，权力最大化的策略可能会导致智能体忽视那些短期内看起来不太有用，但长期来看可能非常有价值的动作。因为这种策略倾向于选择那些能够立即带来更多状态的动作，所以它可能会忽视那些需要长期投入才能看到回报的动作。
>
>因此，虽然权力最大化的策略在一些情况下可能非常有效，但在其他情况下，它可能并不是最优的策略。在实际应用中，可能需要根据具体的任务和环境来选择最合适的策略。

## 引言

在本论文中，我们的目标是回答一个问题，在给定的一个状态下，什么样的内在选择是可用的。也就是说，什么样的动作是在有意义的影响环境的。我们将动作定义为具有终止状态的策略，并主要关心它所带来的结果---终止时它们在环境中到达什么样的状态。智能体可用的动作集合与智能体的意图无关，它是智能体可能实现的所有动作的集合。文中工作的目的在于提供一种算法，利用信息论的学习标准和训练程序，尽可能多的发现内在动作。

有别于传统的动作学习方法，传统方法的目标在于找到少量对特定的任务有用的动作。将自身限制在较小的动作空间中，使得信用分配和长时间间隔的规划更容易。尽管文中的算法使得动作空间大大扩展，但是对更大的动作空间进行操作实际上是有用的。首先，动作的数量仍然远远小于所有动作序列的数量，因为动作是根据其最终状态进行区分的，许多动作序列可以达到相同的状态。其次，我们的目标是学习良好的动作特征嵌入，相似的动作在特征空间中接近，可以依赖泛化性的力量。在这样的嵌入空间中，一个规划器只需要选择具有足够相似影响的选项的这个空间的邻域。

> "信用分配"是一个在强化学习中常用的概念，主要是指如何将智能体从环境中接收到的奖励（或惩罚）分配给它之前执行的一系列动作。你可以将它想象成一个问题：哪一个动作或动作序列实际上导致了后续的奖励或惩罚？如何分配这个奖励或惩罚对于学习过程来说至关重要，因为它决定了我们的算法如何调整智能体的行为策略。
>
> "长时间间隔的规划"则是指智能体需要预测并规划在较长的时间步骤里如何执行动作。例如，在许多复杂的任务中，智能体可能需要考虑不仅仅是下一步会发生什么，而是需要考虑接下来几十步，甚至更多步骤中会发生什么。这样的规划任务通常更为复杂，因为它需要对未来的环境状态和可能的奖励进行预测。
>
> 限制较小选项空间，可以算法更轻松地进行信用分配，因为它只需要关注一小部分可能的动作选项。同时，较长时间间隔的规划也变得更容易，因为智能体只需要考虑一小部分可能的未来状态，而不是所有可能的 future trajectories。

Schaul等人在2015年提出了目标和状态嵌入的概念，以及实现这些目标的通用价值函数。这项工作允许智能体有效的表示对许多目标的控制，并对新的目标进行泛化。然而，这些目标被视为默认已给出。本文拓展了这项工作，并提供了一种保留嵌入性质的目标(动作)学习机制。我们的算法至少具有两种使用场景，一种是经典的强化学习场景，目标是最大化环境提供的奖励，正如上面所解释的。在这样的情况下，智能体可以将外部奖励和内部控制最大化目标相结合，倾向于学习高奖励选项，而不是均匀的学习代表控制的选项。

>在这里，"外部奖励"通常指的是环境给出的奖励，例如，智能体完成一个任务或达到一个目标时获得的奖励。而"内部控制最大化"则是指智能体尝试最大化其能够到达的不同状态的数量，也就是说，智能体会选择那些能够使其到达更多不同状态的动作。
>
>将这两者结合起来，意味着智能体在做决策时，会同时考虑这两个因素。也就是说，智能体会在获取环境奖励和到达具有更多动作的状态之间进行权衡。这样的策略可以帮助智能体更有效地探索环境，并找到达到目标的最优策略。

第二种情况是，智能体的长期目标是达到一种拥有最大的可用内在动作集的状态---授权目标(个人的理解是目标是"能够授予智能体最大权力"的)。这组动作由智能体知道如何使用的动作组成。

请注意，这并不是所有选项的理论集合：如果智能体无法学习如何做某事，那么能够做这件事对智能体来说就没有用处。因此，为了最大化权力，智能体需要同时学习如何控制环境 - 它需要发现可用的选项。实际上，智能体应该不只是寻求根据当前能力最能控制的状态，而是寻求预期在学习后将获得最大控制力的状态。因此，能够学习可用选项是变得更有权力的基础。

> 这段文字的关键点在于强调，我们不应该仅仅着眼于智能体当前掌握的选项或者当前能够达到的最大控制力。因为这只是基于智能体当前的知识和技能进行的局限性的考虑。
>
> 相反，我们应该更关注那些智能体预期在未来学习和发展后能够掌握和达到的选项和控制力。这意味着，我们需要关注那些智能体在学习和掌握新技能后能够到达的状态，以及通过学习和发展能够打开的新的可能性。这就需要智能体具有学习和发现新的可用选项的能力。
>
> 因此，这段文字主张的是一种更前瞻性的、对未来具有开展性的视角。它鼓励我们关注智能体的潜力，以及如何通过学习和发展来实现这种潜力，而不是仅仅满足于现状。

我们将其与最常用的内在动机目标，即通过学习前后压缩经验的数量差异来衡量并最大化模型学习进度的数量的内在动机目标进行比较。授权目标与此有着根本的不同:主要目标并不是理解或者预测观测结果，而是控制环境。智能体常常可以在不太了解的情况下很好的控制一个环境，如规范的无模型强化学习算法所示，其中智能体只对以行动为条件的预期回报建模，专注于这样的理解可能会极大的分散和损害智能体，从而降低其所实现的控制。

我们的算法可以看作是学习表示一个智能体的内在控制空间。开发这一空间应该被视为获取对完成多种不同任务有用的普遍知识，如最大化外部或内部奖励。这类似于数据处理中的无监督学习，目标是找到对其他任务有用的数据表示。然而，这里的关键区别在于，我们并不是简单的寻找表示，而是了解智能体可以选择遵循的显式策略。此外，该算法显式的估计了它在不同状态下的控制量---直观的说，就是可靠的可达状态的总数，并因此可用于授权最大化的智能体。

无监督学习最常见的标准是数据可能性。对于给定的数据集，可以根据这个度量来比较各种算法。目标还没有这种普遍建立的衡量方法来比较智能体中的无监督学习性能。主要的困难在于，在无监督学习中，数据是已知的，但在控制中，智能体存在于环境中，需要在环境中行动以发现它包含什么状态和动力学。然而，我们应该能够比骄傲智能体在不同状态下实现的内在控制和授权的数量。就像无监督学习有多种方法和目标一样，我们可以用无监督控制设计多种方法和目标。数据可能性和授权都是信息度量：可能性度量描述数据所需的信息量，授权衡量行动选择与最终状态之间的互信息。

> 在强化学习中，一个动作和最终状态之间的互信息可以用来衡量这个动作对最终状态的影响程度。具体地说，如果一个动作确定之后，最终状态的不确定性大大降低，那么我们可以说这个动作和最终状态之间有高度的互信息。反之，如果一个动作确定之后，最终状态的不确定性几乎没有改变，那么我们可以说这个动作和最终状态之间的互信息很低。

本文的结构如下，首先，我们正式介绍了内在控制的概念，并从互信息原理推导出内在控制的概念。在那之后，我们描述了我们的算法与显式选项的内在控制，并从实验部分论证其可能性。最后，我们谈到了隐含选项的内在控制，并证明其扩展效果更好。最后，我们简要的讨论了该方法的优点和可能的扩展。

## 内在控制和互信息原理

在本节中，我们将解释我们如何表示内在选项和我们优化的相应目标。

我们将一个选项定义为空间中的一个元素$\Omega$和一个关联的策略$\pi(a|s,\Omega)$，该策略在遵循$\Omega$时在状态$s$下选择动作$a$。策略$\pi$有一个特殊的终止操作，该操作终止选项并生成最终状态$s_f$。

考虑以下的$\Omega$的示例空间。

1. $\Omega$取有限数个值，即$\Omega \in \{1, ..., n\}$。这是最简单的情况，其中每一个$i$遵循一个单独的策略$\pi_i$。
2. $\Omega$是一个长度为$n$的二进制向量。这使得其包含了$2^n$种可能性的组合。
3. $\Omega \in R^d$是一个$d$维实向量，这里的选项空间是无限的。预期中对于$\Omega_s$附近的选项也会采取相似的策略。

我们需要表达出关于要考虑选项空间中哪些区域的知识。假设我们从状态$s_0$开始并遵循选项$\Omega$。由于环境和策略通常是随机的，我们可能在不同的时间终止于不同的最终状态。因此，策略定义了一个概率分布$p^J(s_f|s_0,\Omega)$。现在考虑两个不同的方案，如果它们的最终状态非常相似，从本质上讲，它们不应该被视为彼此不同。那么我们该如何表达我们对给定状态下有效内在选项集合的认识呢？

> 这段话的内容是关于选项在状态空间的分布和对策略影响的讨论。讨论的内容包括了选项如何影响最终状态，以及如何衡量和比较不同选项的影响。其中提到的一个重要观点是，如果两个选项导致了非常相似的状态，那么从本质上讲，这两个选项并非不同。

为了帮助回答这个问题，考虑一个离散选项的例子。有三个选项$\Omega_1,\Omega_2,\Omega_3$。假设$\Omega_1$总是指向状态$s_1$，而$\Omega_2$和$\Omega_3$总是指向状态$s_2$。那么我们可以说，我们实际上有两个内在的选择$\Omega_1、(\Omega_2,\Omega_3)$。如果我们为了最大化行为多样性而选择这些选项，那么我们会有一半的时间选择$\Omega_1$，一半的时间选择$(\Omega_2,\Omega_3)$中的任何一个。在这个例子中，$\Omega_2$和$\Omega_3$的相对选择频率无关紧要。我们用一个概率分布$p^C(\Omega|s_0)$来表示这些选择，我们称之为可控性分布。

从直观上来说，为了最大化内在控制，我们应该选择$\Omega_s$，即最大化最终状态的多样性。而对于给定的$\Omega$，则尽可能精确的控制随后的最终状态是什么。前者可以用数学方法表示为熵$H(s_f)=-\sum_{s_f}{p(s_f|s_0)\log{(p(s_f|s_0)}}$，其中$p(s_f|s_0)=\sum_{\Omega}{p^J(s_f|s_0,\Omega)p^C(\Omega|s_0)}$。对于给定的$\Omega$，后者可以表示为负的对数概率$-\log{p^J(s_f|s_0,\Omega)}$(给定$\Omega$时指定最终状态所需的比特数)。然后需要对$\Omega$和$s_f$求均值。将这两个量相减，就得到了我们希望优化的目标--选项和最终状态在概率分布$p(\Omega,s_f|s_0)=p^J(s_f|s_0,\Omega)p^C(\Omega|s_0)$之间的互信息量$I(\Omega,s_f|s_0)$。

> 对这段话的内容进行逐个解释
>
> 首先，为了最大化内在控制，我们应该选择使得最终状态多样性最大化的$\Omega$值。也就是说，在可选的选项中，我们应该选择能够导致更多不同最终状态出现的选项(也就是说尽可能的选择不确定的选项来执行，通过训练来增强对环境的控制能力)。而对于已经选择的特定选项，我们希望尽可能准确的控制最终状态，使其朝着我们期望的方向发展。
>
> 一个选项可能对应着多个最终状态，因为在某些情况下一个选项的执行可能会导致不同的结果或最终状态。
>
> 那么第一个考虑的是，如何衡量我们对于一个选项的控制程度？或者说，我们怎么确定我们对于一个选项的确定程度。显然就是通过熵的方式来衡量不确定性。具体来说，可以针对一个选项进行如下步骤:
>
> 1. 计算该选项下所有可能的最终状态的概率分布:对于给定的初始化状态$s_0$和选项$\Omega$，我们可以计算每个最终状态$s_f$的条件概率。这个概率描述了在给定初始状态和选择了选项$\Omega$的情况下，最终状态$s_f$发生的可能性。
> 2. 计算最终状态的熵:使用第一步中得到的概率分布，可以计算最终状态的熵$H(s_f)$。这个熵描述了在给定初始状态和选项的情况下，最终状态$s_f$的不确定性或多样性。换而言之，熵越大则说明最终状态的不确定性越高。
>
> 通过这样的方式，我们就可以衡量对于一个选项的控制程度。在实际操作中为了最大化内在控制，我们应该选择那些熵较大的选项。
>
> 而对于给定的一个选项$\Omega$，我们希望可以通过某种方式来衡量该选项下对最终状态的控制程度。文中采用的方式是通过信息熵来衡量该选项下对最终状态的控制程度。具体来说，对于一个概率为$p$的事件发生，需要$\log{\frac{1}{p}}$的比特的信息来描述或者确定这个事件的结果，它表示了为了确定这个事件发生所需要的平均比特数。
>
> 而对于给定选项$\Omega$的情况下，我们希望衡量确定最终状态$s_f$所需的信息量，也就是确定最终状态$s_f$所需要的平均比特数。因此使用信息熵来表示这个信息量，其中$p^J(s_f|s_0,\Omega)$就是遵循选项$\Omega$的情况下，从初始状态$s_0$到达各个最终状态$s_f$的概率分布。
>
> 如果在给定选项$\Omega$的情况下，某个最终状态$s_f$的概率