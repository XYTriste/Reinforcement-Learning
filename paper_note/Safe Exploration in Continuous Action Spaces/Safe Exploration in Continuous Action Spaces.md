# 笔记

## 摘要

这段文字讨论了在物理系统（如数据中心冷却装置或机器人）上部署强化学习（RL）代理的问题，其中绝对不能违反关键约束。作者介绍了一种方法，利用这些系统通常的平滑动力学，使RL算法在学习过程中永不违反约束。该方法是直接将安全层添加到策略中，该层通过每个状态分析性地解决一个行动校正方程。通过线性化模型，该方法能够获得一个优雅的封闭形式解决方案，该模型是根据包含任意操作的过去轨迹学习而来。这旨在模拟实际情况，其中数据日志是使用难以数学描述的行为策略生成的；这种情况使得已知的安全意识离线策略方法不适用。作者在新的代表性基于物理的环境中展示了他们的方法的有效性，并成功地避免了违反约束，而奖励塑造方法则失败了。 

总结：这段文字讨论了在物理系统中使用强化学习代理的问题，强调了约束绝对不得违反的必要性。作者提出了一种方法，通过添加安全层，分析性地解决行动校正问题，以确保在学习过程中绝不违反约束。作者还指出他们的方法在特定环境中比奖励塑造方法更有效。

## 引言

在现实世界的强化学习应用中，约束是问题描述的一个重要组成部分，通常情况下还会严格要求不能违反它们。因此，在这项工作中，我们的目标是在整个学习过程中始终保持零违反约束。需要注意的是，对于离散行动空间来说，实现这一目标比对于连续行动空间来说更为直接。例如，可以使用离线数据预先训练违反约束的分类器以剔除不安全的操作。然而，在我们的背景下，由于候选动作数量是无限的，这一目标变得更加具有挑战性。尽管如此，我们的确成功实现了对于连续行动空间的这一目标，并展示了在整个学习过程中不违反约束。

> 总结：在这项工作中，作者的目标是确保在整个学习过程中不会违反任何约束。尽管在连续行动空间中实现这一目标更具挑战性，但他们成功地做到了，并展示了他们的方法可以在整个学习过程中保持零违反约束。作者还指出，这在离散行动空间中更容易实现，因为可以使用离线数据来预先训练分类器来排除不安全的操作。
>
>
> 如何使用离线数据预先训练分类器来排除不安全的操作？
>
> 使用离线数据来预先训练分类器以排除不安全的操作是一种常见的方法，可以提高在强化学习中的安全性。这可以通过以下步骤来实现：
>
> 1. 数据收集：首先，需要在模拟环境或实际系统中收集足够的数据，这些数据包含代理执行操作的历史记录。这些数据应该包括状态信息、代理执行的操作以及约束是否被违反的标签（二元标签，表示是否安全）。
>
> 2. 数据标记：基于约束违反情况，对数据进行标记。这涉及将每个数据点与一个二元标签相关联，指示是否违反了约束。这个标签可以是1（违反约束）或0（未违反约束）。
>
> 3. 特征提取：根据问题的特性，可以从数据中提取相关的特征，这些特征将帮助分类器更好地理解数据。
>
> 4. 模型选择：选择一个适当的机器学习模型来建立分类器。常见的选择包括决策树、随机森林、支持向量机、神经网络等，具体选择取决于数据和问题的复杂性。
>
> 5. 模型训练：使用标记的离线数据集，训练选定的分类器模型。模型的目标是根据状态和操作来预测约束是否会被违反。
>
> 6. 模型评估：使用交叉验证或保留的测试数据集来评估模型的性能。这将帮助确定模型是否能够准确地识别不安全的操作。
>
> 7. 集成到强化学习：一旦分类器训练好，它可以与强化学习代理结合使用。在代理执行操作之前，它可以查询分类器，以了解所选操作是否被分类为安全操作。如果分类器标识所选操作为不安全，代理可以选择另一种操作，以避免违反约束。
>
> 这个过程允许强化学习代理在执行操作之前通过预训练的分类器进行安全性检查，以减少约束违反的风险。

具体来说，我们着手解决了物理系统中的安全控制问题，其中需要保持某些可观测数量在一定范围内。以数据中心冷却为例，温度和压力必须始终保持在各自的阈值以下；机器人不能超过角度和扭矩的限制；自动驾驶车辆必须始终保持与障碍物的距离超过某个安全边界。在这项工作中，我们将这些数量称为"安全信号"。由于这些信号是物理量，因此我们利用它们的平滑性来避免意外的不安全操作。此外，我们处理的是离线记录数据可用的常见情况；因此，可以使用这些数据来预先训练模型，以帮助从最初的强化学习部署时提供安全性。

> 总结：这段文字描述了研究的背景和问题领域，即在物理系统中确保关键可观测量不超出约束范围的安全控制问题。这些可观测量被称为"安全信号"，因为它们指示了系统的安全状态。该研究强调了使用这些物理量的平滑性来防止不安全操作，并考虑到离线记录数据可用的情况，这些数据可以用于预训练模型，以提供初始的安全性保障。

如上所述，传统上，安全探索需要访问使用已知行为策略生成的数据，然后对其进行渐进性的安全更新；可以参考Thomas（2015）的综合研究以了解此类离策略方法。这些数据是必要的，因为除非另有规定，否则一个状态中的行动可能会在未来产生灾难性后果。因此，在部署之前，应提前推断出长期行为。相比之下，在这项工作中，我们消除了对行为策略知识的需求，因为我们关注的是具有相对短期后果的物理系统。消除行为策略知识的需求是我们工作的一个关键优势，因为缺乏这类数据是一个具有挑战性但熟悉的现实情况。在复杂系统中，过去的轨迹很少是使用可以用数学方式描述的一致行为策略生成的。这些系统通常由人类或具有难以描述逻辑的高级软件来控制。因此，在这种情况下，离策略RL方法通常不适用。相反，我们展示了如何有效地利用单步转换数据来确保安全。为了证明我们的方法独立于行为策略，我们在实验中生成了纯随机行动的预训练数据。

> 总结：这段文字强调了传统安全探索方法通常需要已知行为策略生成的数据，而本工作中的关注点是物理系统，其行动具有相对较短期的后果，因此可以消除对行为策略知识的需求。这一方法的关键优势在于它适用于在复杂系统中缺乏一致行为策略数据的情况，因为这些系统通常由难以数学描述的人类或高级软件控制。作者展示了如何有效地利用单步转换数据来确保安全性，并且他们的实验数据是通过纯随机行动生成的，而不需要已知的行为策略。

我们的方法依赖于一个一次性的初始预训练，该预训练基于一个模型，该模型预测了安全信号在单个时间步内的变化。这个模型的强度来自于其简单性：它是相对于动作的一阶近似，其中其系数是一个由状态输入的神经网络（NN）的输出。然后，我们在安全层中直接将这个模型组合在代理策略的上方，以根据需要对动作进行校正；也就是说，在每次策略查询之后，它会解决一个优化问题，找到使安全约束得以满足的动作的最小变化。由于对动作的线性性质，解决方案可以以封闭形式进行分析，并且仅涉及基本的算术运算。因此，我们的安全层既可微分，又具有简单的三行软件实现。需要注意的是，将我们的安全机制称为"安全层"仅仅是一种语义上的选择；它只是一个简单的计算，不仅限于当今流行的深度策略网络，可以应用于任何连续控制算法（不一定是基于强化学习的）。