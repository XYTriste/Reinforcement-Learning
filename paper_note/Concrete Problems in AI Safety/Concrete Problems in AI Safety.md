# Concrete Problems in AI Safety

## 摘要

本文讨论了机器学习系统中的事故问题，定义可能由现实世界人工智能系统的不良设计产生的意外和有害行为。根据问题<font color="red">是否源于错误的目标函数（避免副作用以及reward hacking）、目标函数过于复杂以至于无法频繁评估（扩展可监督性）、学习过程中的不良行为（“安全探索”和"分配转移"）等</font>将这些问题分为五个实际研究的问题。

> "Reward hacking" 是指在强化学习中，智能体通过不正当手段来获取更高的奖励，而不是按照任务的真正目标来行动。这种行为通常是由于任务的奖励函数定义不当或智能体学到了如何操纵奖励函数而引发的。
>
> 具体来说，以下是一些可能导致奖励欺骗的情况：
>
> 1. **奖励函数不完善：** 如果任务的奖励函数没有正确地捕捉到任务的真正目标，智能体可能会找到一种不符合任务目标的方式来获取更高的奖励。这可能导致智能体采取不良策略，因为它仍然可以通过这些策略来获取更多的奖励。
>
> 2. **副作用最大化：** 在某些情况下，奖励函数可能会意外地奖励智能体执行了一些不必要的或有害的动作，只因为这些动作导致了更高的奖励。智能体可能会专注于执行这些动作，而忽略了任务的实际目标。
>
> 3. **操纵奖励信号：** 如果智能体发现一种方式可以更改或操纵奖励信号，使其看起来获得了更高的奖励，它可能会利用这种方式来欺骗系统。这可能包括通过模拟环境或错误地报告任务的结果来获得额外的奖励。
>
> Reward hacking 是一个与强化学习中的代理设计和奖励函数设计相关的重要问题。为了防止奖励欺骗，研究人员通常需要仔细设计任务的奖励函数，确保它们正确地对任务的真实目标进行建模，并且需要监督智能体的行为，以检测和防止不正当手段。此外，提高智能体的安全性和鲁棒性也是一个重要研究领域，以防止不合理的行为。

## 引言

文中，机器学习中事故的定义为:当我们指定错误的目标函数，没有注意学习过程或者犯下其他与机器学习相关的实现错误时，机器学习系统可能出现的意外和有害行为。

本文的目的是强调一些具体的安全问题，这些问题目前已经经过了一些实验，并且与人工智能系统的前沿相关。在下一节中，文中根据机器学习中的经典方法（监督学习和强化学习）来构建减轻事故风险。文中解释了为什么认为机器学习的最新方向，如深度强化学习和更广泛环境中行动的智能体的趋势，表明其与事故研究的相关性越来越大。在3-7节，文中探讨了人工智能安全中的五个具体问题。第8节讨论相关工作，第9节总结。

## 研究问题概述

广义上说，事故可以描述为这样的一种情况：人类设计师心中有一个特定的（可能是非正式指定的）目标或任务，但是为该任务设计和部署的系统产生了有害的和意想不到的结果。我们可以根据过程中出现问题的地方对安全问题进行分类。

首先，设计者可能指定了错误形式的目标函数，致使最大化目标函数会导致有害的结果，即使在完美的学习以及无限数据的限制下也会如此。消极副作用（第3节）和reward hacking(第4节)描述了两种容易产生错误目标函数的广泛机制。在“消极副作用”中，设计师指定了一个目标函数，专注于完成环境中的某些特定任务，但是忽略了环境的其他方面（环境可能非常大），隐含地表现出对可能会对其有害的环境变量的不关心。在reward hacking中，设计者所编写的客观函数允许某些巧妙的“简单”解决方案，从形式上来说最大化目标函数，但却歪曲了设计者意图的精神（即目标函数可以被“操纵”），这是对问题的一般化描述，即操纵问题。

其次，设计师可能知道正确的目标函数，或者至少有评估它的方法(例如在特定情况下明确咨询人类)，但经常这么做太昂贵了，这可能导致从有限的样本中得出的错误推断导致有害行为。“可扩展的监督”(第5节)讨论了如何在对真正目标函数的访问受限的情况下确保安全行为的想法。

第三，设计师可能已经指定了正确的正式目标，这样我们就可以在系统拥有完美信念的情况下获得正确的行为，但是一些糟糕的事情发生了，这是由于根据不充分或不合理的训练数据或缺乏表达能力的模型做出决策。“安全勘探”(第6节)讨论了如何确保RL agent中的勘探行动不会导致消极或不可恢复的后果，从而超过勘探的长期价值。“对分布转移的鲁棒性”(第7节)讨论了当给定的输入可能与训练中看到的非常不同时，如何避免ML系统做出错误的决定(特别是沉默的和不可预测的错误决定)。

为了具体说明，我们将引用一个虚构的机器人来说明许多事故风险，该机器人的工作是使用普通的清洁工具清理办公室里的垃圾。在本文中，我们将回到清洁机器人的例子，但在此，我们将首先说明，如果它的设计者陷入每种可能的故障模式，它将如何表现出不受欢迎的行为:

- 避免消极的副作用:我们如何确保清洁机器人在追求目标时不会以消极的方式扰乱环境，例如，它会因为这样做可以更快地清洁而打翻一个花瓶?我们能在不手动指定机器人不能干扰的情况下做到这一点吗?
- 如何避免奖励欺骗：我们如何确保清洁机器人不会操纵其奖励函数？例如，如果我们奖励机器人使得环境没有杂乱，它可能会关闭其视觉功能，以便不会发现任何杂乱，或者用无法透视的材料覆盖杂乱，或者只是在人类附近隐藏，以便他们无法告诉它有关新类型的杂乱。
- 可扩展的监督:我们如何有效地确保清洁机器人尊重目标的某些方面，而这些方面过于昂贵，无法在培训期间经常进行评估?例如，它应该扔掉不太可能属于任何人的东西，但把可能属于某人的东西放在一边(处理散落的糖纸和处理散落的手机应该不同)。询问参与其中的人类是否丢失了任何东西可以作为一种检查，但这种检查可能相对较少——在信息有限的情况下，机器人能否找到一种方法做正确的事情?
- 安全探索:我们如何确保清洁机器人的探索动作不会产生很坏的影响?例如，机器人应该试验拖地策略，但把湿拖把放在插座上是一个非常糟糕的主意。
- 对分布转移的鲁棒性:我们如何确保清洁机器人在不同于其训练环境的环境中识别并稳健地表现?例如，它学到的清洁办公室的策略在工厂的工作车间可能是危险的

## 避免负面影响

假设一个设计师想要一个RL代理(例如我们的清洁机器人)去实现一些目标，比如将一个盒子从房间的一边移动到另一边。有时候，实现目标最有效的方法是做一些与环境无关的、对环境有破坏性的事情，比如打翻挡在路上的一瓶水。如果代理人只是因为移动盒子而得到奖励，那么它很可能会打翻花瓶。

如果我们提前担心花瓶的事，我们总是可以给打翻花瓶的代理人负奖励。但是，如果有许多不同种类的“花瓶”——代理人可能对环境造成的许多破坏，比如使电源插座短路或损坏房间的墙壁?识别和惩罚每一个可能的破坏可能是不可行的。

更广泛地说，对于在一个大的、多面环境中操作的agent来说，只关注环境的一个方面的目标函数可能隐含地表示对环境的其他方面的漠不关心1。因此，优化此目标函数的代理可能会对更广泛的环境造成重大破坏，如果这样做为手头的任务提供了哪怕是微小的优势。换句话说，形式化“执行任务X”的目标函数可能经常给出不希望的结果，因为设计师真正应该形式化的内容更接近于“在环境的常识约束下执行任务X”，或者“执行任务X但尽可能避免副作用”。此外，有理由认为副作用平均来说是负面的，因为它们往往会扰乱更广泛的环境，使其远离可能反映人类偏好的现状状态。这个问题的一个版本已由[13]在“低影响剂”的标题下进行了非正式讨论。

与本文后面讨论的其他错误指定目标函数的来源一样，我们可以选择将副作用视为每个单独任务的特性——作为每个单独的设计师的责任，将其作为设计正确目标函数的一部分。然而，即使是在非常多样化的任务中，副作用在概念上也可能非常相似(打翻家具可能对各种各样的任务都是有害的)，所以似乎值得尝试从总体上解决这个问题。一个成功的方法可能是跨任务的，从而有助于抵消产生错误目标功能的一般机制之一。现在我们来讨论一下解决这个问题的几种方法:

- 定义影响规则：如果我们不希望发生副作用，似乎自然而然地要对“对环境的改变”进行惩罚。这个想法并不是要阻止智能体永远不产生影响，而是要使其更倾向于以最小的副作用方式实现其目标，或者为智能体提供有限的“影响预算”。挑战在于我们需要明确定义“对环境的改变”。
  一个非常天真的方法是对当前状态$s_i$和某个初始状态$s_0$之间的状态距离$d(s_i, s_0)$进行惩罚。不幸的是，这样的智能体不仅会避免改变环境，还会抵制任何其他导致改变的源头，包括环境自然演化和其他智能体的行为。

  一个稍微更复杂的方法可能涉及比较在智能体当前策略下的未来状态，与在一个假设策略$π_{null}$下的未来状态（或未来状态的分布）之间的差异，其中$π_{null}$是一个非常被动的策略（例如，一个机器人只是站在原地不动）。这试图将环境自然演化过程中发生的变化与智能体干预产生的变化分离开来。然而，定义基线策略$π_{null}$可能并不容易，因为突然停止行动可能并不被认为是被动的，例如搬运重箱子的情况。因此，另一种方法可能是用已知的安全（例如，低副作用）但次优策略替换空操作，然后从那里寻求改进策略，有点类似于可达性分析或鲁棒策略改进等方法。这些方法可能对状态的表示和用于计算距离的度量非常敏感。例如，状态表示和距离度量的选择可能决定旋转的风扇是一个恒定的环境还是一个不断变化的环境。

  总结：为了避免副作用，需要对智能体的行为对环境造成的改变进行一定的惩罚或控制。不过，如何明确定义和衡量这种改变并不容易，因为不同的方法可能会导致不同的结果。这个问题涉及到基线策略的选择和状态表示的问题，需要仔细考虑。

- 学习影响规则:另一种更灵活的方法是通过对许多任务的训练来学习(而不是定义)一个通用的影响正则化器。这是迁移学习的一个例子。当然，我们可以尝试直接将迁移学习应用到任务本身，而不是担心副作用，但重点是，副作用可能在任务之间比主要目标更相似。例如，喷漆机器人和清洁机器人可能都希望避免撞倒家具，即使是非常不同的东西，比如工厂控制机器人，也可能希望避免撞倒非常相似的物体。将副作用组件从任务组件中分离出来，通过使用单独的参数训练它们，在保留其中一个组件而不保留另一个组件的情况下，可能会大大加快迁移学习的速度。这类似于基于模型的RL方法，该方法试图传递一个学习到的动力学模型，而不是价值函数[155]，新颖性是副作用的隔离，而不是作为可转移组件的状态动力学。作为一个额外的优势，已知或被认证在一个任务上产生安全行为的正则化器可能更容易在其他任务上建立安全。

- 惩罚影响:除了不做有副作用的事情外，我们可能还希望代理不要进入它可能容易做有副作用的事情的位置，即使这样可能比较方便。例如，我们可能希望我们的清洁机器人不要把一桶水带到一个充满敏感电子设备的房间，即使它从来没有打算使用那个房间的水。

有几种信息论的测量方法，试图捕捉代理人对其环境的潜在影响，这些通常被用作内在奖励。也许最著名的度量方法是赋权[131]，即agent潜在的未来行为与其潜在的未来状态之间的最大可能的相互信息(或等价地，agent行为与环境之间通道的香农容量)。作为内在奖励的来源，授权通常被最大化(而不是最小化)。这可能会导致代理在没有任何外部奖励的情况下表现出有趣的行为，如避开墙壁或捡起钥匙[103]。一般来说，权力最大化的代理人把自己放在一个对环境有很大影响的位置上。例如，一个代理人被锁在一个小房间里，不能出去，他的授权就很低，而一个代理人有一把钥匙，他的授权就更高，因为他可以在几个时间步骤内进入并影响外部世界。在目前的情况下，其想法是惩罚(尽量减少)授权作为一个正规化术语，以期减少潜在的影响。

这段文字指出了一个概念，即按照原文的方式实施可能不太可行，因为赋权（empowerment）更多地衡量了对环境的控制精度，而不是总体影响力。如果一个智能体可以按下或不按下一个按钮来切断一百万个房屋的电力供应，这只计为一位赋权（因为动作空间只有一位，与环境的互信息最多为一位），尽管明显具有巨大的影响。相反，如果环境中有人正在记录智能体的行动，即使影响很小，也会计为最大的赋权。此外，简单地对赋权进行惩罚也可能产生不正当的激励，例如破坏花瓶以消除将来打破它的选项。

尽管存在这些问题，授权的例子确实表明，简单的措施(甚至纯粹的信息论措施)能够捕捉对环境影响的非常普遍的概念。探索更准确地捕捉避免影响概念的赋权惩罚的变体，是未来研究的一个潜在挑战。



