# 深入研究策略梯度算法中的无效动作屏蔽

## 摘要

近年来，深度强化学习算法在许多具有挑战性的测出来游戏中取得了良好的表现。然而在游戏中，从学习策略预测的完整离散行为分布中采样的动作很可能是无效的（例如：撞墙）。在策略梯度算法中，处理这个问题的通常方法是“屏蔽”无效的动作，只从有效的动作集合中进行采样。然而，这一影响仍未得到充分的研究。在本文中，我们展示了这种实践的理论依据，从经验的角度证明了它的重要性。并通过评估不同的行动屏蔽机制，如何使用屏蔽训练后去除屏蔽，提供了进一步的见解。

## 引言

深度强化学习算法在具有挑战性的领域产生了最先进的智能体代理。由于这些游戏具有复杂的规则，所以不同状态的有效离散动作空间通常具有不同的大小。也就是说，一个状态可能有5个有效操作，而另一个状态可能有7个有效操作。为了将这些游戏描述成带有单一动作集的标准强化学习问题，之前的工作将这些离散的动作空间组合成包含所有状态的可用动作的完整离散动作空间。尽管如此完整的离散动作空间使得应用DRL算法变得更容易，但是一个问题是，从这个完整的离散行为空间中取样的行动可能对某些游戏状态无效，而这一行动不得不丢弃。

更糟糕的是，有些游戏具有非常大的完全离散的动作空间，动作采样通常是无效的。例如《Dota2》的完全离散动作空间的大小为1837080.

其中一个例子可能是购买道具，这种道具在某些游戏状态下是有效的，但在没有足够金币的情况下就会失效。为了避免在完全离散的动作空间中重复采样无效动作，最近的研究将策略梯度算法与一种称为无效动作屏蔽的技术结合在一起，这种技术“屏蔽”无效动作，只从那些有效的动作中采样。然而，据我们所知，无效动作屏蔽效应的理论基础尚未得到研究，其经验效应也未得到充分研究。本文进一步研究了博弈环境下的无效动作屏蔽，指出无效动作屏蔽所产生的梯度对应于有效的策略梯度。更有趣的是，我们表明，事实上，无效的动作屏蔽可以被看作是在计算动作概率分布时应用一个状态相关的可微函数来产生一个行为策略。

接下来，我们设计了实验来比较无效动作屏蔽相较于无效动作惩罚的性能，这是一种常见的方法，为无效行为提供负奖励，以便智能体学会通过不执行任何无效行为来最大化奖励。我们的经验表明，当无效行动的空间扩大时，无效动作屏蔽会有很好的扩展性，智能体会解决我们期望的任务。而无效动作惩罚甚至连第一个奖励都很难探索到。

然后，我们设计实验来回答两个问题：

1. 如果我们移除无效动作屏蔽，会发生什么情况？
2. 当我们使用无效动作屏蔽，但使用未屏蔽动作的概率分布更新策略梯度时，智能体的表现如何？

## 背景

简单介绍马尔科夫决策过程以及策略梯度算法的损失函数。

## 无效动作屏蔽

无效动作屏蔽是一种常见的技术，用于避免在大型离散动作空间中重复产生无效的动作。据我们所知，没有文献详细描述无效操作屏蔽的实现。现有的工作似乎只是将其作为一个辅助细节，通过几句话来描述它。此外，也没有文献提供理论依据来解释为什么它适用于策略梯度算法。在本节中，我们将研究无效动作屏蔽是如何实现的呃，并证明它确实对应于有效的策略梯度更新。更有趣的是，我们通过计算动作概率分布时应用一个状态相关的可微函数来说明它的工作原理。

首先，让我们看看离散行动通常是如何通过策略梯度算法生成的。大多数策略梯度算法使用神经网络来表示策略，该策略通常输出未归一化的分数(logits)，然后使用softmax操作或等效操作将其转换为行为概率分布，这是我们将在本文的其余部分假设的框架。考虑一个具有行为集合$A=\{a_0,a_1,a_2,a_3\}$和状态集合$S=\{s_0,s_1\}$的MDP，其中在初始状态$s_0$采取行动后，MDP立即到达终止状态$s_1$，而奖励总是$+1$。进一步考虑以$\theta=[l_0,l_1,l_2,l_3]=[1.0,1.0,1.0,1.0]$为参数的策略$\pi_{\theta}$，在本例中直接得到$\theta$作为输出对数。在$s_0$中，我们有:
$$
\begin{align}
\pi_{\theta}(\cdot|s_0)&=[\pi_{\theta}(a_0|s_0),\pi_{\theta}(a_1|s_0),\pi_{\theta}(a_2|s_0),\pi_{\theta}(a_3|s_0)] \\
&=softmax([l_0,l_1,l_2,l_3]) \\
&=[0.25,0.25,0.25,0.25] \\
&where \quad \pi_{\theta}(a_i|s_0)=\frac{\exp(l_i)}{\sum_j{\exp(l_j)}}
\end{align}
$$
此时，常规的策略梯度算法将从$\pi_{\theta}(\cdot|s_0)$中对动作进行采样。假设$a_0$是从中采样得到的动作，那么策略梯度可以通过如下方式进行计算:
$$
\begin{align}
g_{policy}&=\Bbb E_{\tau}[\nabla_{\theta}\sum_{t=0}^{T-1}\log{\pi_{\theta}(a_t|s_t)G_t}] \\
&=\nabla_{\theta}\log{\pi_{\theta}(a_0|s_0)G_0} \\
&=[0.75, -0.25, -0.25, -0.25] \\
(\nabla_{\theta}\log{softmax(\theta)_j})_i&=
\begin{cases}
(1-\frac{\exp(l_i)}{\sum_j{\exp(l_j)}}) \quad if \quad i=j \\
\frac{-\exp(l_i)}{\sum_j{\exp(l_j)}} \quad otherwise
\end{cases}
\end{align} \\
$$
现在，假设动作$a_2$对状态$s_0$无效，唯一有效的动作是$a_0,a_1,a_3$。通过"屏蔽"无效操作对应的logits，进而避免对无效操作进行采样。通常的做法是用一个大的负数$M$(例如:$M= -1 * 10^8$)来替换行为的对数来实现的。我们用掩模$\Bbb R \rightarrow \Bbb R$来表示这一过程，可以计算出重新归一化的概率分布$\pi_{\theta}^{'}(\cdot|s_0)$如下：
$$
\begin{align}
\pi_{\theta}^{'}(\cdot|s_0)
&=softmax(mask([l_0,l_1,l_2,l_3])) \\
&= softmax(mask([l_0,l_1,M,l_3])) \\
&= [\pi_{\theta}^{'}(a_0|s_0),\pi_{\theta}^{'}(a_1|s_0),\epsilon,\pi_{\theta}^{'}(a_3|s_0)] \\
&=[0.33,0.33,0.0000,0.33] \\
\end{align}
$$
其中$\epsilon$是被屏蔽无效动作的结果概率，它应该是一个很小的数字。当它负的足够大时，选择被屏蔽的无效动作的概率几乎为零。本回合结束后，策略将按照以下梯度进行更新，我们将其称为无效动作策略梯度:
$$
\begin{align}
g_{invalid \ action \ policy}&=\Bbb E_{\tau}[\nabla_{\theta}\sum_{t=0}^{T-1}{\log{\pi_{\theta}^{'}(a_t|s_t)G_t}})] \\
&=\nabla_{\theta}\log{\pi_{\theta}^{'}(a_0|s_0)G_0} \\
&=[0.67, -0.33, 0.0000, -0.33]
\end{align}
$$
这个例子强调了无效动作屏蔽似乎并不仅仅是"重新正则化概率分布"，实际上它实际的对应于无效动作的对数的梯度为零。

## 屏蔽仍然产生一个有效的策略梯度

看上去，影响动作选择的因素似乎时一个与不同的策略函数不同的外部过程，这个过程用于计算动作的屏蔽（或者称为无效动作）。实际上，我们的分析表明，无效动作屏蔽过程可以被看作是一个状态相关的可微函数，用于计算一个新的策略函数$\pi_{\theta}^{'}$。因此$g_{invalid \ action \ policy}$可以看作是$\pi_{\theta}^{'}$的策略梯度更新。

Proposition 1. $g_{invalid \ action \ policy}$是策略$\pi_{\theta}^{'}$的策略梯度

证明. 设对于任意的$s \in S$，将无效动作屏蔽过程看作是一个可微函数屏蔽，将其应用于策略$\pi_{\theta}$在给定状态$s$下的logits输出$l(s)$，则有:
$$
\begin{align}
\pi_{\theta}^{'}(\cdot|s)&=softmax(mask(l(s))) \\
mask(l(s))_{i}&=
\begin{cases}
l_i \quad if \ a_i \ is \ valid \ in \ s \\
M \quad otherwise
\end{cases}
\end{align}
$$
显然，对于logits中的元素，mask要么是一个恒等函数，要么是常数函数。由于这两类函数都是可微的，$\pi_{\theta}^{'}$对于其参数$\theta$也是可微的。即对于任意的$a \in A, s \in S$偏导数都存在。因此它满足了策略梯度定理的假设，$g_{invalid \ action \ policy}$可以看作是$\pi_{\theta}^{'}$的策略梯度更新。

请注意，mask并不是一个分段线性函数。如果我们绘制mask函数的曲线，它要么是一个恒等函数，要么是一个常数函数。这取决于状态$s$，并在$-\infty \to \infty$范围内。因此，我们将mask称为一个依赖于状态的可微函数。也就是说，给定一个向量$x$和两个状态$s$和$s^{'}$。这两个状态在可用的无效状态状态数量上可能不同。即$mask(s,x)\neq mask(s^{'},x)$。

