# 强化学习中内在动机的信息论视角：一项调查研究

## 摘要

强化学习（RL）研究领域非常活跃，涌现了大量新的贡献，特别是考虑到深度强化学习（DRL）这一新兴领域。然而，仍然有许多科学和技术挑战需要解决，其中包括抽象动作的能力或在稀疏奖励设置中探索环境的困难，而这可以通过内在动机（IM）来解决。我们提出通过基于信息论的新分类法来调查这些研究工作：我们计算重新审视惊奇、新颖性和技能学习的概念。这使我们能够识别方法的优势和劣势，并展示研究的当前展望。我们的分析表明，新颖性和惊奇可以帮助建立可转移技能的层次结构，进一步抽象环境，并使探索过程更加健壮。

附加关键词和短语：内在动机，深度强化学习，信息论，发展性学习。

## 引言

在强化学习（RL）中，代理通过试错学习，以最大化其在环境中执行的行动所带来的预期奖励[Sutton和Barto 1998]。传统上，代理最大化根据要执行的任务定义的奖励：当代理学习解决游戏时，可以是分数，或者当代理学习达到目标时，可以是距离函数。然后，奖励被视为外在的（或作为反馈），因为奖励函数是专门为任务提供的。通过外在奖励，使用深度学习整合到RL的Deep Q-network（DQN）[Mnih等人2015]在Atari游戏上取得了许多引人注目的成果，推动了深度强化学习（DRL）。

然而，尽管DRL方法最近取得了改进，但当奖励在环境中分散时，它们通常在大多数情况下都无法成功，因为代理难以学习针对目标任务的期望行为[Francois-Lavet等人2018]。此外，代理学到的行为在同一任务内以及跨许多不同任务中都很难重复使用[Francois-Lavet等人2018]。代理难以将学到的技能推广到在环境中做出高层次的决策。例如，这种技能可能是使用包含沿四个基本方向移动的原始动作去到门口；甚至是通过控制机器人模拟器MuJoCo [Todorov等人2012]中的不同关节向前移动。

另一方面，与RL不同，发展性学习[Cangelosi和Schlesinger 2018; Oudeyer和Smith 2016; Piaget和Cook 1952]基于这样一种趋势，即婴儿，或更广泛的有机体，在自发探索其环境时获得新技能[Barto 2013; Gopnik等人1999]。这通常被称为内在动机（IM），可以从内在奖励中得出。这种动机允许自主获得新知识和技能，从而使学习新任务的过程更加容易[Baldassarre和Mirolli 2013]。多年来，内在动机在强化学习中得到越来越广泛的应用，得益于重要的成果和深度学习的出现。这一范式通过使用更一般的奖励函数提供了更大的学习灵活性，允许解决仅使用外在奖励时引发的问题。典型地，内在动机提高了代理探索环境的能力，逐渐学习独立于其主要任务的技能，选择要改进的适当技能，甚至创建具有有意义属性的状态表示。此外，由于其定义的影响，内在动机不需要额外的专家监督，使其在各种环境中容易推广使用。

我们论文的范围。在本文中，我们通过一个基于信息论目标的新分类法，研究并将方法分组在一起。通过这种方式，我们重新审视了惊奇、新颖性和技能学习的概念，并展示它们可以涵盖许多研究。每个类别都以一个适合其最终心理学定义的计算目标为特征。这使我们能够定位/关联大量研究，并突显研究的重要方向。总的来说，本文研究了在DRL框架内使用IM的方式，并考虑以下几个方面：

- IM在解决DRL挑战中的作用。
- 通过少数信息论目标对当前异质工作进行分类。
- 展示每类方法的优势。
- 在每个类别内外IM在RL中的重要展望。

相关工作。有关IM的整体文献非常庞大[Barto 2013]，我们仅考虑其在DRL中的应用以及与信息理论相关的IM。因此，我们对IM的研究并不意味着穷尽。内在动机目前引起了很多关注，有几项工作对方法进行了有限的研究。[Colas等人2020]和[Amin等人2021]分别关注技能学习和探索的不同方面；[Baldassarre 2019]通过心理学、生物学和机器人学的视角研究内在动机；[Pateria等人2021]综述了包括内在和外在动机在内的层次强化学习；[Linke等人2020]实验比较了不同目标选择机制。与这些方法相比，我们通过信息论的视角研究了基于内在动机的大部分目标。我们假设我们的工作与[Schmidhuber 2008]的工作一致，后者假设有机体被希望压缩他们接收到的信息的欲望。然而，通过审查该领域的更近期的进展，我们使用信息论的工具对压缩的概念进行了形式化。

本文的结构如下。首先，我们讨论强化学习（RL），定义内在动机并解释它如何适用于RL框架（第2节）。然后，我们突出强调RL的主要当前挑战，并确定需要额外的结果（第3节）。之后，我们简要解释我们的分类（第4节），即惊奇、新颖性和技能学习，以及详细说明当前的研究如何符合它（分别在第5节、第6节和第7节）。最后，我们强调该领域的一些重要展望（第8节）。

## 定义以及背景知识

## 2.1 马尔科夫决策过程

略

## 2.2 内在激励的定义

简而言之，内在动机是指出于其固有满足感而从事某事，而不是为了从环境中获得积极的反馈[Ryan和Deci 2000]。从这个定义来看，人们可以注意到内在动机是通过与外在动机形成对比来定义的；它突显了两者范 paradigm 之间的差异。内在动机假设代理自行学习，而外在动机则假设存在一个专家/需求来监督学习过程。

根据[Singh等人2010]的观点，进化提供了一个基于个体生存的适应度函数来最大化的一般内在动机（IM）函数。例如，好奇心并不立即产生选择性优势，但使得自身具有一些选择性优势的技能的获得是可能的。更广泛地说，使用内在动机可以获得比仅使用标准强化学习更有效地实现目标的智能行为[Baldassarre 2011; Baldassarre和Mirolli 2013; Lehman和Stanley 2008]。通常情况下，一个学生因为觉得有趣而做数学作业是内在动机，而他/她的同学为了取得好成绩而做则是外在动机[Ryan和Deci 2000]。在这个情境中，内在动机的学生可能在数学方面比其他同学更成功。这对于仅使用标准强化学习方法的相关性提出了疑问。

更严格地说，[Oudeyer和Kaplan 2008]解释了对于一个自治实体而言，一项活动如果其兴趣主要取决于来自不同刺激的信息的整合或比较，而独立于它们的语义，那么它就具有内在动机。相反，外在奖励是由未知环境静态函数产生的，这个函数不依赖于代理对所考虑环境的先前经验。关键点在于代理不能对其接收到的观察的语义有任何先验知识。这里的刺激一词并不指的是感觉输入，而是更普遍地指一个系统的输出，该系统可以是独立实体内部或外部的，因此包括体内稳态变量（温度、饥饿、口渴、对性活动的吸引等）[Baldassarre 2011; Berlyne 1965]。广义上说，代理的动机可以是内在的（动机的来源），但仍然是外在的（行为的目的）。例如，当代理因为饥饿而寻找食物时，饥饿是来自代理的认知系统的刺激，因此它是内在但外在的动机。另一个例子是，一个孩子可能做家庭作业是因为他/她认为这对将来找到工作至关重要。虽然动机的来源是内在的，但真正的结果来自环境。

既然我们澄清了内在动机的概念，我们将研究如何将内在动机整合到强化学习框架中。有关内在动机的广泛概述可以在[Barto 2013]中找到。

## 包含内在奖励的RL模型

强化学习源自行为主义 [Skinner 1938]，通常使用外在奖励 [Sutton 和 Barto 1998]。然而，[Singh 等人 2010] 和 [Barto 等人 2004] 重新构建了强化学习框架以整合内在动机。我们可以区分奖励，这是环境中的事件，和奖励信号，这是对代理的内部刺激。因此，在强化学习社区中被称为奖励的实际上是奖励信号。在奖励信号类别中，有初级奖励信号和次级奖励信号之分。次级奖励信号是通过预期未来奖励计算的局部奖励信号，与值函数相关，而初级奖励信号是从MDP中接收到的标准奖励信号。

此外，与其将MDP环境视为代理完成任务的环境，该模型建议MDP环境可以分为两部分：外部部分对应于潜在任务和代理的环境；内部部分计算MDP状态和次级奖励信号，可能使用之前的相互作用。因此，我们可以将内在奖励视为从MDP环境接收到的奖励信号。MDP状态不再是外部状态，而是代理的内部状态。然而，从现在开始，我们将遵循强化学习的术语，术语奖励将指的是初级奖励信号。
图1总结了该框架：评论者位于代理的内部部分，它计算内在奖励并处理学分分配。代理可以在其内部部分合并内在奖励和外在奖励。状态包括感知和任何形式的内部上下文；在本节中，我们将这个状态称为上下文状态。决策可以是一个高层次的决策，由内部环境分解成低层次的动作。
这个概念模型将内在动机纳入MDP的形式化框架。接下来，我们将回顾这个模型在实践中是如何实现的。事实上，可以扩展RL以包括内在奖励、高层次决策和上下文状态这三个新组件。我们将在以下各节中分别研究它们。

## 内在奖励与信息论

在我们对内在动机的定义中，人们可以注意到信息的概念经常出现。这并非偶然，量化信息对生成内在奖励非常有用。在本节中，我们提供有关信息理论的基础知识，并解释如何结合内在和外在奖励。然而，我们强调内在奖励并不局限于信息度量，它们的特性主要取决于奖励函数是否符合内在动机的属性。

香农熵量化了确定随机变量值所需的平均信息量。设$X$是一个随机变量，其密度分布满足规范化和正值要求，我们定义它的熵为：
$$
H(X)=-\int_{X}{p(x)\log{p(x)dx}}
$$
换句话说，它允许量化随机变量的无序程度。当$X$遵循均匀分布时，熵最大，当$p(X)$在除一个值外的所有地方都等于零时，即为Dirac分布时，熵最小。基于此，我们还可以定义在随机变量$S$条件下的熵。它类似于经典熵，量化了在知道另一个随机变量$S$的值的情况下找到$X$所需的平均信息量：
$$
H(X|S)=-\int_{S}p(s)\int_{X}p(x|s)\log{p(x|s)}dxds
$$
互信息允许量化随机变量$X$关于另一个随机变量$Y$的信息量。它也可以被视为随机变量$Y$对随机变量$X$的无序程度的减少。互信息的定义如下：
$$
I(X;Y)=H(X)-H(X|Y)
$$
