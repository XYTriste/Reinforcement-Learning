# 笔记

## 摘要

在强化学习中，探索始终是一个关键的问题。在复杂或奖励稀疏的环境中（例如Montezuma's revenge）中进行探索是一个困难的过程，然而基于奖励的方法在探索困难环境中还是取得了一些成功，例如RND算法分配了内在奖励来鼓励智能体访问很少访问的状态。这种奖励是根据执行某个动作后下一个状态的新颖性来得到的，因此我们将这种方法称为<font color="red">下一状态奖励方法。</font>

然而这样的方法使得智能体过度关注探索已知的状态，忽略寻找未知状态。因为探索由已经访问的下一个状态驱动，这会减慢某些环境中探索的速度。

论文中提出了一种方法，用来改善找到未知状态的效率并提出了动作平衡探索，它平衡了在给定状态下选择每个动作的频率，可以看作是置信上界在深度强化学习中的扩展。

> 置信上界（Upper Confidence Bound，简称UCB）是一种用于平衡探索与利用的算法。在强化学习中，UCB算法用于选择动作，以便智能体能够在探索未知动作和利用已知高奖励动作之间找到平衡点。
>
> UCB算法基于置信上界的原理，它通过估计每个动作的奖励值，并计算每个动作的置信上界。置信上界表示对奖励值的不确定性的估计，它越大表示对奖励值的估计越不确定，智能体越有可能选择该动作进行探索。
>
> 具体来说，UCB算法根据每个动作的奖励估计值和置信上界来选择动作。在每个时间步，UCB算法会计算每个动作的置信上界，并选择具有最大置信上界的动作进行执行。这样，智能体既能够利用已知的高奖励动作，又能够在一定概率下选择未知的动作进行探索。
>
> UCB算法的一个常用形式是UCB1算法，它使用上界函数$UCB1(a) = Q(a) + c * \sqrt{(\frac{\log(t)}{N(a)}})$来计算每个动作的置信上界。其中，Q(a)是动作a的奖励估计值，N(a)是动作a被选择的次数，t是总的时间步数，c是一个控制探索程度的参数。

此外，文中还提出了行动平衡的RND算法，它结合了下一状态奖励方法（RND）以及行动平衡探索以充分利用二者。实验表明动作平衡探索在寻找未知状态方面具有较好的能力，且在困难的探索环境下能够提高RND的性能。

## 引言

常见的深度强化学习算法，例如DQN、PPO等在奖励密集的环境效果较好，但在稀疏奖励环境中往往失败（如MonteZuma's revenge）。这是因为大多数状态-行为对的环境奖励都是0，因此几乎没有更新策略的有效信息。对于这样的环境，奖励重塑是一种引入了人类专业知识，将原始稀疏问题转换为密集奖励问题的解决方案。然而，这样的方法并不具备普适性，在实际任务中要将人类知识转换为数字奖励通常是复杂的。

在最近的研究中，基于奖励的探索方法取得了巨大成功，通过使用执行某些行为得到的下一个状态来产生内在奖励，并将其与外部奖励结合起来，使得环境奖励更加密集。由于奖励由到达的下一个状态为基础进行计算，因此称为<font color="red">下一状态奖励方法</font>。还是如上面所说，这样的方法使得智能体过度关注探索已知的状态，忽略了发现未知状态，因为它是通过影响外部奖励来发挥作用的。

> 考虑一个特殊的环境，在两个网格世界的中间有一条3格的走廊连接这两个网格世界。右侧的网格世界中存在一个奖励，左侧的网格世界没有奖励。移动的奖励为0。如果通过常见的下一状态奖励方法（如RND），假设智能体处于走廊中间并决定往左边走，RND就会给出一个内在奖励鼓励智能体到达了未知状态。如果策略依赖于行为价值进行选择，那么下一次智能体回到起点时依赖行为价值选择行为，它会始终选择向左走。直到由于一直找不到奖励使得左侧所有状态的奖励衰减到0为止，才会在起点重新随机选择一个方向来行动。
>
> 一种直接的解决方法是在当前状态选择行为时就进行探索，例如使用epsilon greedy策略就具有一定的随机性，即使在每个回合的开始都会有可能走向右边的网格世界。
>
> 然而，完全依赖于幸运的行为是低效的。

基于上述要点，文中提出了一种新的探索方法，称为行动平衡探索，它专注于寻找未知状态。主要思想是<font color="red">平衡选择每个动作的频率</font>，它可以被看作是置信上界在深度强化学习中的扩展。

具体而言，在给定状态下，文中使用随机网络蒸馏模块来记录每个行为的选择频率，并为每个行为生成奖励。行为奖励的向量将直接与策略$\pi$结合以直接提高不常被选择的行为的概率。对于使用下一状态奖励方法的智能体，动作平衡探索可以避免其过度关注某个单个动作，提高发现未知状态的能力。

文中将动作平衡方法与RND相结合，提出了一种新的探索方法---动作平衡的RND算法，它可以更有效的发现未知状态，同时引导智能体更频繁的访问不熟悉的状态。文中最开始首先在一个完全没有奖励的网格世界中测试了动作平衡的RND算法，相比传统的RND算法具有一定提升。随后，文中证明了动作平衡的RND算法可以在一些难以探索的Atari游戏中提高RND的实际性能。

## 相关工作

基于计数的方法已经有很长的一段研究历程了，它们使用状态的新颖性作为内在奖励来指导探索。基于计数的方法在表格环境中易于实现且高效，但当状态空间不可枚举时，它们就不适用于大规模的问题。

为了解决这个问题，许多人提出了各自的解决方案。例如，TRPO-AE-hash算法使用SimHash对状态空间进行哈希，虽然它可以在一定程度上减少状态空间。但是这依赖于哈希算法的设计。DDQN-PC、A3C+、DQN-PixelCNN、$\phi$-EB等采用密度模型来衡量状态的访问次数。

ICM和RND使用监督学习的预测误差来衡量状态的新颖性，因为新状态由于较少的训练，预计会有更高的预测误差。“Episodic curiosity through Reachability”通过引入内存缓冲区解决了ICM的“噪音电视”的问题。NGU采取了一个额外的经验缓冲来来保存回合中经历的状态来计算内在奖励。
