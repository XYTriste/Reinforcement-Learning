# Made: Exploration via maximizing deviation from explored regions

## 摘要

在奖励稀疏的高维环境中，有效的探索仍然是具有挑战性的。低维环境中通常表格参数化是可行的，基于计数的上置信度（UCB）探索方法可以实现最低限度的近最优率。然而，如何在涉及非线性函数近似的RL任务中有效使用UCB仍然不够清楚。为了解决这个问题，文中提出了一种新的探索方法，通过最大化下一个策略与已探索区域的占用偏差。文中将这个偏差作为自适应正则器添加到标准RL的目标函数中，以平衡探索与利用。文中将新的目标与可证明的收敛算法进行配对，从而产生调整现有奖励的新内在奖励。所提出的内在奖励易于实现，并与其他现有强化学习算法相结合进行探索。作为概念的验证，文中在各种基于模型和无模型算法的表格示例上评估了新的内在奖励，显示了比仅计数策略探索的改进。

