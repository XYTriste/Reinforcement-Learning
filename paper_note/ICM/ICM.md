---
title: "Curiosity-driven exploration by self-supervised prediction -- note"
author: Yu Xia
date: May 14, 2023
output: pdf_document
---
# 笔记
## 摘要
在现实生活的场景中，因为有时候智能体的外部奖励信号非常稀少或者不存在。在这样的情况下，我们可以将好奇心作为一种内在的奖励信号，驱使智能体探索环境并学习可能有用的状态。
好奇心，在文中被表述为“智能体通过自监督逆动力学模型学习的视觉特征空间中预测其自身行为后果能力的误差”。
> 这段话的理解是，智能体通过一个自监督逆动力学模型，通过输入当前的状态$s_t$和下一个状态$s_{t+1}$，输出当前状态下的动作估计值$\hat{a}_t$

## 引言
强化学习的目标旨在获得尽可能多的环境给予的奖励，然而在实际问题中奖励信号十分稀疏，甚至外在奖励会消失或者无法构造一个有效的奖励函数。
> 一个简单的例子就是让智能体学习弹钢琴，如果只在完成一整首钢琴曲时才给予奖励。那么奖励信号是非常稀疏的，智能体可能需要非常非常久才能学习到如何弹奏一首钢琴曲。如果奖励信号设置的过于密集，例如在某个时刻按下某个钢琴键就获得奖励，那么智能体会太过于依赖这些奖励信号，从而泛化能力非常差。

因此，作者引出了好奇心的概念。在现实生活发展的历程中，使人们得以长足发展的原因或许并不只有完成任务时获得的满足感，同时也应有好奇心去驱使人们探索一个可能的新的技能。当外在奖励稀疏时，内在奖励就变得至关重要。内在奖励的形式主要可以分为两大类：
1. 鼓励智能体探索“新颖”状态
2. 鼓励智能体执行能够减少其预测自身行为结果能力（即其对环境的了解）中的错误/不确定性的行为。

衡量"新颖"这个指标需要建立一个环境状态分布的统计模型，或者用人话说就是一个状态表。而衡量预测误差/不确定性需要建立一个环境动力学模型，或者说需要建立一个状态-行为对到下一个状态的映射。显然如我之前学到的那样，<font color="red">这两个模型都很难在连续状态或连续行为空间下建立。</font>

另一个问题在于处理智能体以及环境中的随机性，智能体在执行行为时可能存在噪声。可能导致智能体在相同的状态下执行相同行为时得到不同的结果。而环境本身可能包含了一些随机性，这些随机性可能使得环境状态发生随机变化，从而增加了预测的控制和困难。

针对上述问题一个可行的解决方案是，只有当智能体遇到了一个难以预测但是“可学习”的状态时才奖励它，但是如何估计一个状态的“可学习性”本身就是一个困难的问题。

---
文中提出了一个基于智能体预测其自身行为结果的难易程度（或者说预测自身行为结果的误差大小）而产生内在奖励信号的广泛方法，即根据当前状态和执行的行为预测下一个状态。然而，文中提出的方法避免了一些之前的预测方法的陷阱：我们只预测环境中可能由代理行为或影响代理的那些变化，而忽略其余的变化。
也就是说我们并不是在原始空间中进行预测，而是<font color="red">将输入转换为特征空间，其中仅表示与代理执行行为相关的信息。我们使用自监督学习学习这个特征空间，在智能体逆动力学任务上训练神经网络，在输入智能体当前状态和下一个状态时预测行为。</font>由于只预测行为，因此神经网络输入中无需嵌入环境中不影响智能体本身变化的因素。
然后，我们使用这个特征空间来训练一个前向动态模型，该模型通过给定的当前状态和行为的特征表示，预测下一个状态的特征表示。将预测和实际之间的预测误差作为内在奖励提供给智能体，鼓励其好奇心。

> 该文中提出了一种基于智能体预测自身行为结果的难易程度来产生内在奖励信号的方法。算法流程如下：
> 1. 特征空间转换：将输入数据从原始空间转换为特征空间，其中只包含与智能体执行的行为相关的信息。这一步通过自监督学习来学习特征空间，利用逆动力学任务训练神经网络，输入为智能体的当前状态和下一个状态，预测为智能体的行为。
> 2. 前向动态模型训练：利用特征空间训练一个前向动态模型，通过给定当前状态和行为的特征表示，预测下一个状态的特征表示。
> 3. 预测误差计算：将前向动态模型的预测结果与实际下一个状态的特征表示进行比较，计算它们之间的预测误差。
> 4. 内在奖励信号生成：将预测误差作为内在奖励信号提供给智能体，作为一种鼓励智能体探索的机制，激励其好奇心。
通过这种方法，智能体通过自我预测行为结果的准确性来衡量其对环境的理解程度，并利用预测误差作为内在奖励来推动探索和学习。这种方法能够避免预测环境中不影响智能体的因素，并且在特征空间中进行预测和误差计算，从而应对高维连续状态空间的挑战。

## 好奇心探索
我们的系统分为了两个部分:
1. 训练内在奖励生成器模块，也就是刚刚所提到的动力学模型的预测问题。
2. 训练RL策略的模块，在训练策略的过程中会先将训练数据输入ICM模块进行预测推理，用前向模型的预测误差作为内在奖励。然后结合内在奖励和外在奖励一起进行策略的训练。

假设智能体在时刻$t$产生的内在好奇心奖励为$r_{t}^{i}$，外在奖励为$r_{t}^{e}$。那么第2个模块，也就是训练RL策略的模块被设定为最大化这两个奖励的综合$r_t=r_{t}^{i} + r_{t}^{e}$。其中$r_{t}^{e}$大部分情况下（并不总是）为0。
![avatar](images/figure%202.png)
从上图我们可以看出算法的流程，处于状态$s_t$的智能体通过执行当前策略$\pi$采样的动作$a_t$来与环境进行交互并终止于状态$s_{t+1}$。策略$\pi$被训练以优化由环境提供的外在奖励和内在好奇心模块(ICM)提供的内在奖励的总和。ICM将状态$s_t$和状态$s_{t+1}$编码为特征$\phi(s_t)$和特征$\phi(s_{t+1})$。并将其输入给逆向动力学模型(Inverse model)来产生预测行为$\hat{a}_t$，通过优化$\min_{\theta_{I}}{L_{I}(\hat{a}_{t},a_t)}$来训练逆向动力学模型。
而前向模型则使用$\phi(s_t)$和$a_t$为输入，预测$s_{t+1}$的特征表示$\hat{\phi}(s_{t+1})$，将预测与实际的误差作为基于好奇心的内在奖励信号。由于没有激励$\phi(s_t)$编码任何不能影响或者不受代理行为影响的环境特征，因此智能体学习探索策略对环境的不可控方面具有强健性。

我们通过使用包含了深度神经网络参数$\theta_{P}$来表示策略$\pi(s_t;\theta_P)$。给定智能体的状态$s_t$，它从中采样并得到一个行为$a_t$。参数$\theta_P$被优化以最大化期望的奖励总和:
$$
\max_{\theta_P}{\Bbb E_{\pi(s_t;\theta_P)}}[\sum_{t}{r_t}]
$$

### 预测误差作为好奇心奖励
在原始像素空间中进行预测是不值得期望的，一方面其难以预测，另一方面来说我们并不知道预测的像素是否是我们要优化的正确目标。文中给出了一个例子说明其不可行性:
>在原始感觉空间中进行预测（例如当$s_t$对应于图像时）是不期望的，这不仅是因为难以直接预测像素，而且还因为不清楚预测像素是否是要优化的正确目标。要了解原因，请考虑使用像素空间中的预测误差作为好奇心奖励。想象一个场景，智能体正在观察树叶在微风中的运动。由于对微风建模本身就很困难，因此预测每个叶子的像素位置就更难了。这意味着像素预测误差将保持较高，并且智能体将始终对叶子保持好奇。但是叶子的运动对智能体来说是无关紧要的，因此它对叶子的持续好奇是不可取的。潜在的问题是，代理不知道状态空间的某些部分根本无法建模，因此代理可能会陷入人为的好奇心陷阱并停止其探索。以表格形式记录访问状态的计数（或其对连续状态空间的扩展）的新颖性探索方案也遭受该问题。过去已经提出测量学习进度而不是预测误差作为一种解决方案（Schmidhuber，1991）。不幸的是，目前还没有已知的计算可行的机制来衡量学习的进展。

如果并不从原始像素空间中进行预测，那么应该如何定义正确的特征空间进行预测？以便提供一个较好的好奇心度量？
如果我们将所有影响智能体观测结果的来源进行划分，可以分为三种情况:
1. 可以由智能体直接控制的事物 （智能体操作的角色）
2. 智能体无法控制，但是影响智能体的事物 （地图中的怪物，陷阱等）
3. 智能体无法控制，不影响智能体的事物（地图中偶尔拂过的微风等）

一个好的好奇心特征空间应该是1和2的模型，并且不受3的影响。

### 自监督预测探索
文中所提出的特征空间可以通过训练具有两个子模块的深度神经网络来学习:
1. 第一个子模块将原始输入状态$s_t$编码为特征向量$\phi(s_t)$。
2. 第二个子模块将两个后续状态的特征编码$\phi(s_t)$和$\phi(s_{t+1})$作为输入，并预测智能体从状态$s_t$移动到状态$s_{t+1}$所采取的动作$a_t$。

训练该神经网络相当于学习函数$g$，其定义为:
$$
\hat{a}_t=g(s_t,s_{t+1};\theta_I)
$$
其中，$\hat{a}_t$是对实际行为$a_t$的估计，神经网络的参数$\theta_I$被训练以优化:
$$
\min_{\theta_I}L_{I}(\hat{a}_t,a_t)
$$
其中，$L_I$为损失函数，衡量预测行为与实际行为之间的差异。在$a_t$为离散的情况下，函数$g$的输出是所有可能行为的soft-max分布，最小化$L_I$相当于多项式分布下$\theta_I$的最大似然估计。函数$g$也被称为逆动力学模型。
除了逆动力学模型以外，文中训练了另一个神经网络，该神经网络以$\phi(s_t)、a_t$作为输入，并预测时刻$t+1$时的状态的特征编码。
$$
\hat{\phi}(s_{t+1})=f(\phi(s_t),a_t;\theta_F)
$$
其中$\hat{\phi}(s_{t+1})$表示对于${\phi}(s_{t+1})$的预估，神经网络参数$\theta_F$通过最小化损失函数$L_F$进行优化:
$$
L_F(\phi(s_t),\hat{\phi}(s_{t+1}))=\frac{1}{2}||\hat{\phi}(s_{t+1})-{\phi}(s_{t+1})||_2^2
$$
学习函数$f$也被称为前向动力学模型，内在奖励信号$r_t^i$计算为:
$$
r_t^i=\frac{\eta}{2}||\hat{\phi}(s_{t+1})-{\phi}(s_{t+1})||_2^2
$$
其中,$\eta > 0$被称为比例因子，显然这个因子如果越大，内在奖励信号也就越大。
整合逆向动力学模型和前向动力学模型，就形成了我们的ICM（Intrinsic Curiosity Module）模块。上面的式子可以整合写为:
$$
\min_{\theta_P,\theta_I,\theta_F}=\bigg[-\lambda \Bbb E_{\pi(s_t;\theta_P)}[\sum_{t}{r_t}]+(1-\beta)L_I+\beta L_F \bigg]
$$
其中，$0 \leq \beta \leq 1$是衡量模型逆损失和正向损失的标量，$\lambda > 0$是衡量策略梯度损失重要性与学习内在信号奖励重要性的标量。