# 通过约束优化获得内在奖励

## 摘要

最先进的强化学习（RL）算法通常使用随机采样（例如，ε-贪婪法）来进行探索，但这种方法在像《蒙特祖马的复仇》这样的难以探索的任务上会失败。为了应对探索的挑战，先前的工作通过在代理访问新颖状态时给予奖励来激励探索。这种内在奖励（也称为探索奖励或好奇心）通常在难以探索的任务上表现出色。然而，在探索难度较低的任务上，代理会被内在奖励分散注意力，并进行不必要的探索，即使有足够的任务（也称为外在）奖励可用。因此，这种过度好奇的代理比只有任务奖励训练的代理表现更差。这种在不同任务上的表现不一致性阻碍了内在奖励与RL算法的广泛应用。我们提出了一种称为外在-内在策略优化（EIPO）的原则性约束优化程序，它自动调整内在奖励的重要性：当探索不必要时，它会抑制内在奖励，并在探索必要时增加它。结果是不需要手动调整内在奖励与任务奖励之间平衡的优越探索。在61个ATARI游戏上的一致性表现增益验证了我们的主张。

> 这段文字介绍了一种新的强化学习算法，名为外在-内在策略优化（EIPO）。这种算法旨在解决传统RL算法在探索方面的不足，尤其是在难以探索的任务中。传统方法通过给予代理访问新颖状态的内在奖励来激励探索，但这在探索难度较低的任务中会导致代理进行不必要的探索，从而影响性能。EIPO通过自动调整内在奖励的重要性来解决这个问题，从而在不同任务中实现一致的性能提升。该算法在六十一个ATARI游戏中的测试表明，它能够在不需要手动调整奖励平衡的情况下提供优越的探索性能。相关的代码已经开源。

## 引言

强化学习（RL）的目标[1]是找到从状态到动作的映射（即策略），以最大化奖励。在每次学习迭代中，代理都面临一个问题：是否已经获得了最大可能的奖励？在许多实际问题中，最大可获得的奖励是未知的。即使最大可获得的奖励是已知的，如果当前策略不是最优的，那么代理面临另一个问题：是应该花时间改进其当前策略以获得更高的奖励（利用），还是应该尝试不同的策略以期发现潜在的更高奖励（探索）？过早的开发就像陷入局部最优一样，阻止了代理进行探索。另一方面，过多的探索可能会分散注意力，阻止代理完善一个好的策略。因此，解决探索-利用困境[1]对于数据/时间效率的策略学习至关重要。

在简单的决策问题中，动作不影响状态（例如，老虎机或有上下文的老虎机[2]），已知有证据的最优算法用于平衡探索与开发[3,2]。然而，在通常使用RL的一般设置中，这样的算法是未知的。在缺乏既符合理论又实用的方法的情况下，最先进的RL算法依赖于启发式探索策略，如给动作添加噪声或随机采样次优动作（例如，ε-贪婪法）。然而，这样的策略在奖励稀疏的情况下失败，因为不频繁的奖励阻碍了策略的改进。其中一个著名的任务是臭名昭著的ATARI游戏，《蒙特祖马的复仇》[4]

> 这段文字指出，在某些简单的决策问题中，比如老虎机或有上下文的老虎机，已经存在可以证明是最优的算法来平衡探索和开发。但是，在更一般的强化学习环境中，这样的算法还未被发现。由于缺乏既有理论基础又在实践中表现良好的方法，现有的先进RL算法通常采用启发式探索策略，比如给动作添加噪声或采用随机采样的方式选择次优动作。这些策略在奖励稀疏的环境中往往不起作用，因为不常出现的奖励会妨碍策略的改进。《蒙特祖马的复仇》就是一个在这种环境下策略改进受阻的典型例子。

稀疏奖励问题可以通过在任务奖励（或者外部奖励$r^E$）中添加由智能体自身产生的密集探索奖励（或者说内在奖励$r^i$）来解决。内在奖励鼓励智能体访问新的状态，这增加了遇到外在奖励状态的机会。以往的研究表明，相较于单纯的优化外在奖励$(\lambda=0)$相比，联合优化外在奖励和内在奖励$(r^E+\lambda r^i)$可以提高稀疏奖励任务的下智能体的性能。

然而，最近的一项研究发现，使用内在奖励并不一致地优于简单的探索策略，如ε-贪婪法，在ATARI游戏中[11]。这是因为混合目标$(r^E+\lambda r^i)$在$|λ| > 0$时是有偏的，优化它并不一定会产生仅就外在奖励而言的最优策略[12]。图1a用一个玩具示例说明了这个问题。这里绿色三角形是代理，蓝色/粉色圆圈分别表示外在奖励和内在奖励的位置。在训练开始时，所有状态都是新颖的，并提供内在奖励的来源（即粉色圆圈）。这使得积累内在奖励变得容易，代理可能会利用这一点来优化其目标，即最大化内在奖励和外在奖励的总和。然而，这样的优化可能导致局部最大值：代理可能会沿着底部的走廊向右移动，实际上分散了代理追求顶部的蓝色任务奖励的注意力。在这个例子中，由于找到任务奖励并不困难，如果只最大化外在奖励$（λ = 0）$，则会获得更好的性能。然而，问题在于，在大多数环境中，人们事先并不知道如何最优地权衡内在奖励和外在奖励（即选择$λ$）。

> 这段文字讨论了内在奖励在强化学习中的应用问题。尽管内在奖励被设计来帮助代理在探索困难的环境中找到奖励，但研究表明它们并不总是比简单的探索策略（如ε-贪婪法）更有效。这是因为当内在奖励与外在奖励混合时（通过λ参数加权），优化这个混合目标并不总能得到最优的外在奖励策略。这可能导致代理陷入局部最优，忽视了更重要的外在奖励。在某些情况下，如果外在奖励容易获得，那么仅优化外在奖励可能会得到更好的结果。然而，大多数情况下，我们事先不知道如何最优地在内在奖励和外在奖励之间做出权衡。

通常的做法是进行广泛的超参数搜索，以找到最佳的$λ$值，因为不同的λ值适用于不同的任务（见图4）。此外，随着智能体在任务上的进步，最佳的探索-利用权衡可能会变化，一个恒定的$λ$可能在整个训练过程中并不是最优的。在训练的初期阶段，可能更倾向于探索。一旦智能体能够获得一些任务奖励，它可能更倾向于利用这些奖励而不是进一步探索。探索-利用权衡的确切动态是任务依赖的，而且针对每个任务进行调整既繁琐、不受欢迎，通常也是计算上不可行的。因此，先前的工作在训练期间使用固定的$λ$，我们的实验揭示这是次优的。

我们提出了一种优化策略，减轻了在训练过程中手动调整外在奖励和内在奖励相对重要性的需要。我们的方法在内在奖励有助于探索时利用其偏差，并在内在奖励不帮助积累更高外在奖励时减轻这种偏差。这是通过使用一个外在最优性约束来实现的，<font color="red">该约束强制在优化混合目标后获得的外在奖励等于仅最大化外在奖励的最优策略积累的外在奖励。在一般设置中强制执行外在最优性约束是不可行的，因为最优的外在奖励是未知的。</font>我们设计了一种称为外在-内在策略优化（EIPO）的实用算法，该算法使用近似方法解决这个约束优化问题（第3节）

> 本文段落讨论了在强化学习中，如何平衡探索和利用的问题。传统上，需要对超参数λ进行广泛搜索，以找到适合特定任务的最佳值。然而，随着训练的进行，最佳的探索-利用平衡可能会改变，因此一个固定的λ值可能不是最佳选择。作者指出，针对每个任务调整λ既费时又不实际。为了解决这个问题，作者提出了一种新的优化策略，即外在-内在策略优化（EIPO），<font color="red">它通过一个外在最优性约束来自动调整外在和内在奖励的相对重要性，从而避免了手动调整的需要。这种方法在探索有助于提高性能时利用内在奖励的偏差，并在探索不再有益时减少这种偏差。</font>

<font color="green">简而言之，文中提出的方法其实就是：如果我们使用了内在奖励引导智能体进行探索。那么针对不同的环境，我们需要给予智能体不同的内在奖励权重来鼓励它访问新颖状态。对于较为简单就能探索到外在奖励状态的环境，不需要将内在奖励权重设置的过大(不要将$\lambda$设置的太大)，而对于难以探索到外在奖励的环境，则需要给出一个较大的权重来促使其更多的通过内在奖励访问新颖状态。 然而，这样的方法意味着针对不同的环境我们需要具体的调整$\lambda$。因此文中提出了一种方法，为$\lambda$的大小设置了一条约束：当智能体获得混合奖励（外在奖励+内在奖励）时，确保混合奖励中外在奖励的部分等同于最大化外在奖励策略时获得的奖励相等。在不牺牲外在奖励的前提下，利用内在奖励促进探索。</font>

原则上我们可以将EIPO应用于任何内在奖励方法，但我们使用最先进的随机网络提炼（RND）[9]来评估性能。图1b展示了在两款ATARI游戏上的初步结果：（i）Montezuma's Revenge - 其中与RND（红色）联合优化的结果显著优于仅使用外在奖励优化的PPO策略[13]（黑色）；（ii）James Bond - 其中PPO策略显著优于RND。这些结果加强了这样一个观点：内在奖励引入的偏差在某些游戏中有帮助，但在其他游戏中则有害。我们的算法EIPO（蓝色）在两款游戏中均匹敌最佳算法，显示出它可以根据需要利用内在奖励。在61款ATARI游戏上的结果加强了这一发现。此外，在某些游戏中，EIPO的表现超过了多个强大的基线，无论是否使用内在奖励，这表明我们的方法不仅可以减轻由内在奖励偏差引起的潜在性能下降，还可以提升性能，超越当前的最先进水平。

## 相关工作

考虑一个由状态空间$S$，行为空间$A$以及外在奖励函数$R^E:S \times A\rightarrow\Bbb{R}$组成的离散时间马尔科夫决策过程，分别使用$E$和$I$表示外在奖励和内在奖励分量。外在奖励函数指的是实际任务模板（例如游戏得分等）。

智能体从初始状态分布$\rho_0:S\rightarrow\Bbb{R}$采样的初始状态$s_0$开始，在每个时间步$t$，智能体从环境中得到一个状态$s_t$，在策略$\pi$中中采样行动$a_t$并执行动作，并通过状态转移函数$\tau(s_{t+1}|s_t,a_t)$转移到下一个状态$s_{t+1}$。智能体的目标是利用与环境的交互作用找到最优策略$\pi$，使得外在目标奖励最大化：
$$
\begin{align}
\max_{\pi}J^E(\pi),\quad where\ J^E(\pi)=\Bbb{E}_{s_0,a_0,...,\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t}r_t^{E}]
\\
s_0\sim \rho_0,a_t\sim\pi(a|s_t),s_{t+1}\sim\tau(s_{t+1}|s_t,a_t)\ \forall t>0
\end{align}
\tag{1}
$$
而结合了内在奖励的探索策略通过提供内在奖励鼓励智能体探索，内在奖励会鼓励智能体访问新颖状态，利用内在奖励函数$R^I:S\times A \rightarrow \Bbb{R}$，优化的目标函数为:
$$
\max_{\pi \in \Pi}J_{E+I}(\pi),\quad where\ J_{E+I}(\pi)=\Bbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma^t(r_t^E+\lambda r_t^I)]
\tag{2}
$$
其中，$\lambda$是内在奖励权重。最先进的基于内在奖励的探索策略通常使用近端策略优化(PPO)来优化该式中的目标函数。

## 减轻内在奖励的偏差

根据上式$(2)$，简单的最大化外在奖励和内在奖励的总和并不能使得一个策略也最大化外在奖励，即:
$$
argmax_{\pi_{E+I}\in\Pi}J_{E+I}(\pi_{E+I})\neq argmax_{\pi_{E}\in\Pi}J_{E}(\pi_{E})
$$
对于混合奖励目标函数下的最优策略$\pi_{E+I}^{*}=argmax_{\pi_{E+I}}J_{E+I}(\pi_{E+I})$而言，它可能是目标函数$J_E$的次优策略。

由于智能体的表现最终仅由外在奖励来衡量，因此我们提出一个强制性的外在最优约束，来保证混合奖励下的最优策略$\pi_{E+I}^{*}=argmax_{\pi_{E+I}}J_{E+I}(\pi_{E+I})$与最优的外在奖励策略$\pi_{E}^{*}=argmax_{\pi_{E}}J_{E}(\pi_{E})$产生的奖励一样多。即：
$$
\begin{align}
\max_{\pi_{E+I}\in\Pi}&J_{E+I}(\pi_{E+I}) \\
subject\ to\ &J_E(\pi_{E+I})-\max_{\pi_E}J_{E}(\pi_E)=0 \quad(extrinsic\ optimality\ constraint)
\end{align}
\tag{3}
$$
该优化问题的求解可以看作是提出了一个使$J_{E+I}$最大化的策略$\pi_{E+I}$，然后在给定外部最优性约束的情况下检验所提出的$\pi_{E+I}$是否可行。

<font color="red">然而，这一目标函数是难以求解的，因为评估外在最优性约束要求我们知道$\max_{\pi_E}J_{E}(\pi_E)$，但是这正是我们要求解的。</font>

### 对偶目标函数：无约束最小-最大化问题

要将上述带约束的优化问题转换为无约束问题，我们可以引入拉格朗日乘子（Lagrange multiplier），这样可以将约束条件整合到目标函数中。对于给定的问题，我们可以定义拉格朗日函数 ( L ) 如下：
$$
L(\pi_{E+I}, \lambda) = J_{E+I}(\pi_{E+I}) + \lambda \left( J_E(\pi_{E+I}) - \max_{\pi_E}J_{E}(\pi_E) \right)
$$
其中，$\lambda$ 是拉格朗日乘子。无约束的优化问题就是找到 $\pi_{E+I}$ 和 $\lambda$使得 $L$最大化。

对于对偶问题，我们首先需要定义对偶函数 $g(\lambda)$ 为原始拉格朗日函数$L$ 关于$\pi_{E+I}$的最大值:
$$
g(\lambda) = \max_{\pi_{E+I}\in\Pi} L(\pi_{E+I}, \lambda)
$$
然后，对偶问题就是最小化对偶函数$g(\lambda) $关于 $\lambda$：
$$
\min_{\lambda \geq 0} g(\lambda)
$$
> 在优化理论中，对于最大化问题的拉格朗日函数 $L(\pi_{E+I}, \lambda)$，我们定义对偶函数 $g(\lambda)$ 为拉格朗日函数关于原始变量 $\pi_{E+I}$ 的最大值。这样定义的原因是我们希望找到一个对于所有可能的原始变量 $\pi_{E+I}$ 都有效的最差情况下界。
>
> 对偶函数 $g(\lambda)$ 反映了在给定拉格朗日乘子 $\lambda$ 的情况下，原始问题的最优值的一个下界。当约束条件得到满足时，拉格朗日函数 $L(\pi_{E+I}, \lambda)$ 与原始目标函数 $J_{E+I}(\pi_{E+I})$ 相等；当约束条件不满足时，拉格朗日乘子的作用会使得 $L(\pi_{E+I}, \lambda)$ 小于原始目标函数。
>
> 因此，对于每一个固定的 $\lambda$，我们通过最大化 $L(\pi_{E+I}, \lambda)$ 来找到这个下界。然后，对偶问题就是在所有这些可能的下界中找到最大的一个，即最小化 $g(\lambda)$。这样，对偶问题提供了原始问题最优值的一个下界。
>
> 弱对偶性告诉我们，对于任何可行的原始变量 $\pi_{E+I}$ 和任何非负的拉格朗日乘子 $\lambda$，对偶函数的值 $g(\lambda)$ 总是小于或等于原始问题的最优值。在满足KKT条件等特定条件下，我们可以得到强对偶性，意味着对偶问题的最优值与原始问题的最优值相等。
>
> 因此，对偶函数 $g(\lambda)$ 是原拉格朗日函数 $L$ 取最大值，因为我们希望找到在所有可能的 $\pi_{E+I}$ 下的最差情况下界，然后通过最小化这个下界来逼近原始问题的最优解。

> 理解对偶函数定义为原拉格朗日函数求极大值的关键在于理解拉格朗日对偶性的本质。拉格朗日对偶性是一种从原始优化问题（primal problem）导出另一个问题（对偶问题，dual problem）的方法，这个对偶问题在某些条件下可以给出原始问题的最优值的下界。
>
> 原始问题是：
>
> $$
> \begin{align*}
> \max_{\pi_{E+I}\in\Pi} & \quad J_{E+I}(\pi_{E+I}) \\
> \text{subject to} & \quad J_E(\pi_{E+I}) - \max_{\pi_E}J_{E}(\pi_E) = 0 \quad (\text{extrinsic optimality constraint})
> \end{align*}
> $$
>
> 拉格朗日函数 $L(\pi_{E+I}, \lambda)$ 是原始问题的目标函数和约束条件的线性组合，其中 $\lambda$ 是拉格朗日乘子：
>
> $$
> L(\pi_{E+I}, \lambda) = J_{E+I}(\pi_{E+I}) - \lambda \left( J_E(\pi_{E+I}) - \max_{\pi_E}J_{E}(\pi_E) \right)
> $$
>
> 对偶函数 $g(\lambda)$ 是拉格朗日函数 $L(\pi_{E+I}, \lambda)$ 对 $\pi_{E+I}$ 的最大化：
>
> $$
> g(\lambda) = \max_{\pi_{E+I}\in\Pi} L(\pi_{E+I}, \lambda)
> $$
>
> 为什么要最大化 $L(\pi_{E+I}, \lambda)$ 呢？因为我们想要找到在给定 $\lambda$ 下，原始问题目标函数的最佳（最大）可能值，同时考虑到约束条件。如果 $\pi_{E+I}$ 是原始问题的可行解，则拉格朗日函数在这点的值等于原始问题的目标函数值。如果 $\pi_{E+I}$ 不满足原始问题的约束，则拉格朗日函数的值会因为 $\lambda$ 的惩罚项而减小。
>
> 通过对每个 $\lambda$ 最大化 $L(\pi_{E+I}, \lambda)$，我们得到了一个关于 $\lambda$ 的函数 $g(\lambda)$，它表示在考虑约束的情况下，原始问题目标函数的最大可能值。这个值是原始问题最优值的一个下界，因为它可能来自于不满足原始约束的 $\pi_{E+I}$。
>
> 最后，对偶问题是找到这些下界中最紧的一个，即找到使 $g(\lambda)$ 最小的 $\lambda$：
>
> $$
> \min_{\lambda \geq 0} g(\lambda)
> $$
>
> 这个最小化过程实际上是在寻找最佳的拉格朗日乘子 $\lambda$，使得对偶函数 $g(\lambda)$ 给出最接近原始问题最优值的下界。如果原始问题和对偶问题满足强对偶性条件，那么对偶问题的最优值将与原始问题的最优值相等。

> 对偶函数和原始函数之间的关系是通过拉格朗日对偶性框架建立的。在这个框架中，我们从原始优化问题（通常包含约束）出发，构造一个拉格朗日函数，然后通过这个拉格朗日函数定义对偶函数，从而形成对偶问题。下面详细解释这个过程：
>
> 1. **原始问题**：这是我们开始的优化问题，它可能包含目标函数和一些约束。例如，原始问题可能是最大化某个目标函数 $J_{E+I}(\pi_{E+I})$，同时满足一些约束条件。
>
> 2. **拉格朗日函数**：为了处理原始问题中的约束，我们引入拉格朗日乘子 $\lambda$，并构造拉格朗日函数 $L(\pi_{E+I}, \lambda)$，它将原始问题的目标函数和约束条件结合在一起。拉格朗日函数是原始变量 $\pi_{E+I}$ 和拉格朗日乘子 $\lambda$ 的函数。
>
> 3. **对偶函数**：对偶函数 $g(\lambda)$ 是通过对拉格朗日函数 $L(\pi_{E+I}, \lambda)$ 在原始变量 $\pi_{E+I}$ 上进行最大化得到的。对每个固定的 $\lambda$，我们找到使 $L(\pi_{E+I}, \lambda)$ 最大的 $\pi_{E+I}$。这个最大化过程给出了在给定 $\lambda$ 下原始问题目标函数的一个下界。对偶函数 $g(\lambda)$ 是 $\lambda$ 的函数，它表示了所有这些下界中的最大值。
>
> 4. **对偶问题**：一旦我们有了对偶函数 $g(\lambda)$，我们就可以定义对偶问题，即最小化 $g(\lambda)$。对偶问题是一个关于 $\lambda$ 的优化问题，它寻找最紧的下界，即最接近原始问题最优值的下界。
>
> 关系总结：
>
> - 拉格朗日函数是一个桥梁，它结合了原始问题的目标函数和约束条件。
> - 对偶函数是通过最大化拉格朗日函数得到的，它为每个 $\lambda$ 提供了原始问题最优值的一个下界。
> - 对偶问题是最小化对偶函数，它寻找最紧的下界，可能在某些条件下与原始问题的最优值相等。
>
> 这种关系允许我们通过解决对偶问题来间接地解决原始问题，尤其是在原始问题难以直接求解时。在某些条件下（如满足强对偶性条件），对偶问题的最优解将给出原始问题的最优解。

为了将原始的约束优化问题转换为一个无约束的最小化问题，我们可以引入拉格朗日乘子（Lagrange multipliers）。在这个情况下，我们有一个外部优化性能指标$J_{E+I}$ 和一个约束条件，即外部性能指标$J_E$ 必须等于其最大可能值。

首先，我们定义拉格朗日函数 \( L \) 来结合原始问题中的目标函数和约束条件：

$$
L(\pi_{E+I}, \lambda) = J_{E+I}(\pi_{E+I}) - \lambda \left( J_E(\pi_{E+I}) - \max_{\pi_E}J_{E}(\pi_E) \right)
$$

其中 $\lambda$是拉格朗日乘子。然后，我们将原始的最大化问题转换为最小化拉格朗日函数的问题：

$$
\min_{\pi_{E+I}\in\Pi} \max_{\lambda \geq 0} L(\pi_{E+I}, \lambda)
$$

然而，我们通常对拉格朗日函数的对偶函数更感兴趣，因为它可以提供原始问题的下界。对偶函数$g$ 是通过最小化拉格朗日函数 $L$相对于原始变量$ \pi_{E+I}$ 来定义的：

$$
g(\lambda) = \min_{\pi_{E+I}\in\Pi} L(\pi_{E+I}, \lambda)
$$

对偶问题随后可以被表述为最大化对偶函数 $g$：

$$
\max_{\lambda \geq 0} g(\lambda)
$$

这个过程被称为拉格朗日对偶性，它允许我们通过解决对偶问题来得到原始问题的下界。对偶问题通常更容易解决，因为它是一个无约束的优化问题，并且在某些条件下（例如，当原始问题是凸的时候），对偶问题的解会给出原始问题的最优解。

在你的特定情况下，我们需要首先确定$J_{E+I}$和$ J_E $ 的具体形式，以及它们是否满足凸性条件，以便准确地应用上述过程。如果这些函数是凸的，并且满足Slater's condition，那么原始问题和对偶问题的解将会相同。

---

以下是书中的公式推导：

对于式$(3)$中的原始优化问题，它的拉格朗日对偶问题为：
$$
\min_{\lambda \in \Bbb{R}^+}[\max_{\pi_{E+I} \in \Pi}{J_{E+I}(\pi_{E+I})+\lambda(J_E(\pi_{E+I})-\max_{\pi_E \in \Pi}{J_E{(\pi_E)}})}]
\tag{4}
$$
我们通过合并$J_{E+I}(\pi)$和$J_E{(\pi)}$来重写式$(4)$:
$$
J_{E+I}^{\lambda}:=J_{E+I}(\pi_{E+I})+\lambda J_{E}(\pi_{E+I})=\Bbb{E}_{\pi_{E+I}}[\sum_{t=0}^{\infty}\gamma^{t}(1+\lambda)r^{E}(s_t,a_t)+r^I(s_t,a_t)]
$$
重写后的式子提供了对于$\lambda$的一个直观解释：更大的值对应着外在奖励（即利用）的增加。将$J_{E+I}^{\lambda}(\pi_{E+I})$代入式$(4)$，重新排列项得到以下最小值问题：
$$
\min_{\lambda \in \Bbb{R}^+}{[\max_{\pi_{E+I}\in \Pi}{\min_{\pi_E \in \Pi}{J_{E+I}^{\lambda}(\pi_{E+I})-\lambda J_E(\pi_E)}}]}
\tag{5}
$$

### 对偶目标函数优化算法

我们现在描述一种算法来解决式$(5)$中提到的$\pi_E$、$\pi_{E+I}$、$\lambda$的问题。

#### 外在奖励策略$\pi_E$

$\pi_E$是通过最小化子问题进行优化的，它可以重写为一个最大化问题：
$$
{\min_{\pi_E \in \Pi}{J_{E+I}^{\lambda}(\pi_{E+I})-\lambda J_E(\pi_E)}} \rightarrow {\max_{\pi_E \in \Pi}}{\ \lambda J_E(\pi_E)} - {J_{E+I}^{\lambda}(\pi_{E+I})}
\tag{6}
$$
上式$(6)$的主要问题在于，估计目标函数$J_E(\pi_E)$和$J_{E+I}^{\lambda}(\pi_{E+I})$需要分别使用两个策略来采样轨迹，如果使用策略梯度方法（如PPO），这意味着要从训练期间的每次迭代中从两个独立策略中采样轨迹，这样做的效率非常低。

如果我们假设两个策略是相似的，那么我们就可以使用一个策略$\pi_{E+I}$的轨迹来近似另一个策略$\pi_{E}$的回报。

#### performance difference lemma

性能差异引理（Performance Difference Lemma）在强化学习中是用来量化两个策略之间性能差异的工具。它的表达式如下：

$$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \sum_{a} (\pi'(a|s) - \pi(a|s)) Q^{\pi}(s,a) $$

这里：

- $J(\pi)$ 表示策略 $\pi$ 的性能，通常是长期回报的期望值。
- $d^{\pi'}(s)$ 是在策略 $\pi'$ 下访问状态 $s$ 的折扣后的访问频率。
- $\pi(a|s)$ 和 $\pi'(a|s)$ 分别是在状态 $s$ 下策略 $\pi$ 和 $\pi'$ 选择动作 $a$ 的概率。
- $Q^{\pi}(s,a)$ 是在策略 $\pi$ 下，从状态 $s$ 开始并采取动作 $a$ 的预期回报的动作价值函数。

现在，让我们来推导这个引理：

1. 首先，我们定义一个策略 $\pi$ 的状态价值函数 $V^{\pi}(s)$，它是从状态 $s$ 开始并遵循策略 $\pi$ 所获得的预期回报。

2. 状态价值函数可以通过动作价值函数 $Q^{\pi}(s,a)$ 来表达，即：

$$ V^{\pi}(s) = \sum_{a} \pi(a|s) Q^{\pi}(s,a) $$

3. 策略的性能 $J(\pi)$ 可以通过其状态价值函数来表达，特别是从初始状态分布 $p(s_0)$ 开始的期望值：

$$ J(\pi) = \sum_{s_0} p(s_0) V^{\pi}(s_0) $$

4. 考虑两个策略 $\pi$ 和 $\pi'$，我们想要计算它们性能的差异 $J(\pi') - J(\pi)$。我们可以将 $J(\pi')$ 展开为：

$$ J(\pi') = \sum_{s} d^{\pi'}(s) V^{\pi'}(s) $$

5. 然后，我们将 $V^{\pi'}(s)$ 替换为 $Q^{\pi'}(s,a)$ 的表达式，并利用 $Q^{\pi}(s,a)$ 来连接 $\pi$ 和 $\pi'$：

$$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \left( \sum_{a} \pi'(a|s) Q^{\pi'}(s,a) - \sum_{a} \pi(a|s) Q^{\pi}(s,a) \right) $$

6. 由于 $Q^{\pi'}(s,a)$ 和 $Q^{\pi}(s,a)$ 之间可能很难直接比较，我们可以使用一个重要的恒等式，即对于任何策略 $\pi$ 和所有的 $s$ 和 $a$，都有：

$$ Q^{\pi}(s,a) - V^{\pi}(s) = A^{\pi}(s,a) $$

其中 $A^{\pi}(s,a)$ 是动作 $a$ 在状态 $s$ 下相对于策略 $\pi$ 的优势函数。

7. 使用这个优势函数，我们可以重写性能差异为：

$$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \sum_{a} (\pi'(a|s) - \pi(a|s)) A^{\pi}(s,a) $$

8. 最后，我们注意到优势函数 $A^{\pi}(s,a)$ 可以用 $Q^{\pi}(s,a)$ 来表示，因为 $V^{\pi}(s)$ 是 $Q^{\pi}(s,a)$ 在策略 $\pi$ 下的期望值。这样我们就得到了性能差异引理的最终形式：

$$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \sum_{a} (\pi'(a|s) - \pi(a|s)) Q^{\pi}(s,a) $$

这个引理提供了一个量化策略改进的强大工具，它说明了通过增加在某状态下选择更好动作的概率，可以提高策略的整体性能。

> 上述过程基本上是正确的，但在第5步和第7步之间，需要一个更明确的过渡来说明如何从 $Q^{\pi'}(s,a)$ 到 $Q^{\pi}(s,a)$。这个过渡涉及到将 $Q^{\pi'}(s,a)$ 替换为 $Q^{\pi}(s,a)$ 加上策略 $\pi'$ 和 $\pi$ 之间的优势函数差异。这里是详细的解释：
>
> 在第5步中，我们有：
>
> $$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \left( \sum_{a} \pi'(a|s) Q^{\pi'}(s,a) - \sum_{a} \pi(a|s) Q^{\pi}(s,a) \right) $$
>
> 为了将 $Q^{\pi'}(s,a)$ 替换为 $Q^{\pi}(s,a)$，我们需要考虑两个策略之间的优势函数 $A^{\pi}(s,a)$。我们知道：
>
> $$ A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s) $$
>
> 因此，我们可以将 $Q^{\pi'}(s,a)$ 表达为：
>
> $$ Q^{\pi'}(s,a) = A^{\pi}(s,a) + V^{\pi}(s) $$
>
> 但是，我们需要注意的是，这里的 $V^{\pi}(s)$ 是在策略 $\pi$ 下的状态价值函数，而我们需要的是在策略 $\pi'$ 下的状态价值函数 $V^{\pi'}(s)$。为了解决这个问题，我们可以使用策略改进定理，它指出如果策略 $\pi'$ 在所有状态 $s$ 下都至少和策略 $\pi$ 一样好，那么：
>
> $$ Q^{\pi'}(s,a) \geq V^{\pi}(s) $$
>
> 对于所有的 $s$ 和 $a$。这意味着 $A^{\pi}(s,a)$ 是非负的，当我们从 $\pi$ 转换到 $\pi'$ 时。
>
> 现在，我们可以将 $Q^{\pi'}(s,a)$ 替换为 $Q^{\pi}(s,a)$ 和 $A^{\pi}(s,a)$ 的关系，并将其代入 $J(\pi')$ 的表达式中：
>
> $$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \left( \sum_{a} \pi'(a|s) (A^{\pi}(s,a) + V^{\pi}(s)) - \sum_{a} \pi(a|s) Q^{\pi}(s,a) \right) $$
>
> 由于 $V^{\pi}(s)$ 是 $Q^{\pi}(s,a)$ 在策略 $\pi$ 下的期望值，我们可以将其从第一项中提取出来，并且由于 $\sum_{a} \pi'(a|s) = 1$，我们可以得到：
>
> $$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \left( V^{\pi}(s) + \sum_{a} \pi'(a|s) A^{\pi}(s,a) - V^{\pi}(s) \right) $$
>
> 这简化为：
>
> $$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \sum_{a} \pi'(a|s) A^{\pi}(s,a) $$
>
> 但是，为了得到性能差异引理的最终形式，我们需要考虑 $\pi'(a|s) - \pi(a|s)$ 而不仅仅是 $\pi'(a|s)$。这是因为我们对比的是两个策略之间的差异，而不仅仅是新策略 $\pi'$。因此，我们得到：
>
> $$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \sum_{a} (\pi'(a|s) - \pi(a|s)) A^{\pi}(s,a) $$
>
> 最后，由于 $A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$，并且 $V^{\pi}(s)$ 是 $Q^{\pi}(s,a)$ 在策略 $\pi$ 下的期望值，我们可以将 $A^{\pi}(s,a)$ 替换为 $Q^{\pi}(s,a)$，得到性能差异引理的最终形式：
>
> $$ J(\pi') - J(\pi) = \sum_{s} d^{\pi'}(s) \sum_{a} (\pi'(a|s) - \pi(a|s)) Q^{\pi}(s,a) $$
>
> 这样，我们就完成了从 $Q^{\pi'}(s,a)$ 到 $Q^{\pi}(s,a)$ 的转换，并且得到了性能差异引理的正确表达式。

那么，根据性能差异引理，目标函数$\lambda J_E(\pi_E)-J_{E+I}^{\lambda}(\pi_{E+I})$可以重写为:
$$
\begin{align}
\lambda J_E(\pi_E)-J_{E+I}^{\lambda}(\pi_{E+I})&=\Bbb{E}_{\pi_E}[\sum_{t=0}^{\infty}{\gamma^tU_{\min}^{\pi_{E+I}}(s_t,a_t)}] \\
where\ U_{\min}^{\pi_{E+I}}(s_t,a_t) &:= \lambda r_t^E + \gamma V_{E+I}^{\pi_{E+I}}(s_{t+1})-V_{E+I}^{\pi_{E+I}}(s_t) \\
V_{E+I}^{\pi_{E+I}}(s_t) &:=\Bbb{E}_{\pi_{E+I}}[\sum_{t=0}^{\infty}{\gamma^t(r_t^E+r_t^I)|s_0=s_t}]
\tag{7}
\end{align}
$$
接着，在相似性假设的前提下，可以得到上式中的下界：
$$
\begin{align}
\Bbb{E}_{\pi_E}[\sum_{t=0}^{\infty}{\gamma^tU_{\min}^{\pi_{E+I}}(s_t,a_t)}] \geq \Bbb{E}_{\pi_{E+I}}[\sum_{t=0}^{\infty}{\gamma^t \min{\{\frac{\pi_{E}(a_t,s_t)}{\pi_{E+I}(a_t,s_t)}U_{\min}^{\pi_{E+I}}(s_t,a_t), clip(\frac{\pi_{E}(a_t,s_t)}{\pi_{E+I}(a_t,s_t)},1-\epsilon,1+\epsilon)U_{\min}^{\pi_{E+I}}(s_t,a_t)\}}}]
\tag{8}
\end{align}
$$
其中，$\epsilon \in [0,1]$是一个阈值。直观的说，这个截断的目标惩罚了与策略$\pi_{E+I}$不同的策略$\pi_E$，因为过大或者过小的$\frac{\pi_{E}(a_t,s_t)}{\pi_{E+I}(a_t,s_t)}$会被截断。

根据上述内容，我们就可以计算出式$(5)$中的最小化部分了，现在我们需要考虑这个子问题：
$$
{\max_{\pi_{E+I}\in \Pi}{{J_{E+I}^{\lambda}(\pi_{E+I})-\lambda J_E(\pi_E)}}}
\tag{9}
$$
我们同样依赖近似来推导出智能体目标的下界：
$$
\begin{align}
{{J_{E+I}^{\lambda}(\pi_{E+I})-\lambda J_E(\pi_E)}} \geq \Bbb{E}_{\pi_{E}}[\sum_{t=0}^{\infty}{\gamma^t \min{\{\frac{\pi_{E+I}(a_t,s_t)}{\pi_{E}(a_t,s_t)}U_{\max}^{\pi_{E}}(s_t,a_t), clip(\frac{\pi_{E+I}(a_t,s_t)}{\pi_{E}(a_t,s_t)},1-\epsilon,1+\epsilon)U_{\max}^{\pi_{E}}(s_t,a_t)\}}}]
\tag{10}
\end{align}
$$
