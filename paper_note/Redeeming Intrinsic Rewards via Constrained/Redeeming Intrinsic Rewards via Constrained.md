# 通过约束优化获得内在奖励

## 摘要

最先进的强化学习（RL）算法通常使用随机采样（例如，ε-贪婪法）来进行探索，但这种方法在像《蒙特祖马的复仇》这样的难以探索的任务上会失败。为了应对探索的挑战，先前的工作通过在代理访问新颖状态时给予奖励来激励探索。这种内在奖励（也称为探索奖励或好奇心）通常在难以探索的任务上表现出色。然而，在探索难度较低的任务上，代理会被内在奖励分散注意力，并进行不必要的探索，即使有足够的任务（也称为外在）奖励可用。因此，这种过度好奇的代理比只有任务奖励训练的代理表现更差。这种在不同任务上的表现不一致性阻碍了内在奖励与RL算法的广泛应用。我们提出了一种称为外在-内在策略优化（EIPO）的原则性约束优化程序，它自动调整内在奖励的重要性：当探索不必要时，它会抑制内在奖励，并在探索必要时增加它。结果是不需要手动调整内在奖励与任务奖励之间平衡的优越探索。在61个ATARI游戏上的一致性表现增益验证了我们的主张。

> 这段文字介绍了一种新的强化学习算法，名为外在-内在策略优化（EIPO）。这种算法旨在解决传统RL算法在探索方面的不足，尤其是在难以探索的任务中。传统方法通过给予代理访问新颖状态的内在奖励来激励探索，但这在探索难度较低的任务中会导致代理进行不必要的探索，从而影响性能。EIPO通过自动调整内在奖励的重要性来解决这个问题，从而在不同任务中实现一致的性能提升。该算法在六十一个ATARI游戏中的测试表明，它能够在不需要手动调整奖励平衡的情况下提供优越的探索性能。相关的代码已经开源。

## 引言

强化学习（RL）的目标[1]是找到从状态到动作的映射（即策略），以最大化奖励。在每次学习迭代中，代理都面临一个问题：是否已经获得了最大可能的奖励？在许多实际问题中，最大可获得的奖励是未知的。即使最大可获得的奖励是已知的，如果当前策略不是最优的，那么代理面临另一个问题：是应该花时间改进其当前策略以获得更高的奖励（利用），还是应该尝试不同的策略以期发现潜在的更高奖励（探索）？过早的开发就像陷入局部最优一样，阻止了代理进行探索。另一方面，过多的探索可能会分散注意力，阻止代理完善一个好的策略。因此，解决探索-利用困境[1]对于数据/时间效率的策略学习至关重要。

在简单的决策问题中，动作不影响状态（例如，老虎机或有上下文的老虎机[2]），已知有证据的最优算法用于平衡探索与开发[3,2]。然而，在通常使用RL的一般设置中，这样的算法是未知的。在缺乏既符合理论又实用的方法的情况下，最先进的RL算法依赖于启发式探索策略，如给动作添加噪声或随机采样次优动作（例如，ε-贪婪法）。然而，这样的策略在奖励稀疏的情况下失败，因为不频繁的奖励阻碍了策略的改进。其中一个著名的任务是臭名昭著的ATARI游戏，《蒙特祖马的复仇》[4]

> 这段文字指出，在某些简单的决策问题中，比如老虎机或有上下文的老虎机，已经存在可以证明是最优的算法来平衡探索和开发。但是，在更一般的强化学习环境中，这样的算法还未被发现。由于缺乏既有理论基础又在实践中表现良好的方法，现有的先进RL算法通常采用启发式探索策略，比如给动作添加噪声或采用随机采样的方式选择次优动作。这些策略在奖励稀疏的环境中往往不起作用，因为不常出现的奖励会妨碍策略的改进。《蒙特祖马的复仇》就是一个在这种环境下策略改进受阻的典型例子。

稀疏奖励问题可以通过在任务奖励（或者外部奖励$r^E$）中添加由智能体自身产生的密集探索奖励（或者说内在奖励$r^i$）来解决。内在奖励鼓励智能体访问新的状态，这增加了遇到外在奖励状态的机会。以往的研究表明，相较于单纯的优化外在奖励$(\lambda=0)$相比，联合优化外在奖励和内在奖励$(r^E+\lambda r^i)$可以提高稀疏奖励任务的下智能体的性能。

然而，最近的一项研究发现，使用内在奖励并不一致地优于简单的探索策略，如ε-贪婪法，在ATARI游戏中[11]。这是因为混合目标$(r^E+\lambda r^i)$在$|λ| > 0$时是有偏的，优化它并不一定会产生仅就外在奖励而言的最优策略[12]。图1a用一个玩具示例说明了这个问题。这里绿色三角形是代理，蓝色/粉色圆圈分别表示外在奖励和内在奖励的位置。在训练开始时，所有状态都是新颖的，并提供内在奖励的来源（即粉色圆圈）。这使得积累内在奖励变得容易，代理可能会利用这一点来优化其目标，即最大化内在奖励和外在奖励的总和。然而，这样的优化可能导致局部最大值：代理可能会沿着底部的走廊向右移动，实际上分散了代理追求顶部的蓝色任务奖励的注意力。在这个例子中，由于找到任务奖励并不困难，如果只最大化外在奖励$（λ = 0）$，则会获得更好的性能。然而，问题在于，在大多数环境中，人们事先并不知道如何最优地权衡内在奖励和外在奖励（即选择$λ$）。

> 这段文字讨论了内在奖励在强化学习中的应用问题。尽管内在奖励被设计来帮助代理在探索困难的环境中找到奖励，但研究表明它们并不总是比简单的探索策略（如ε-贪婪法）更有效。这是因为当内在奖励与外在奖励混合时（通过λ参数加权），优化这个混合目标并不总能得到最优的外在奖励策略。这可能导致代理陷入局部最优，忽视了更重要的外在奖励。在某些情况下，如果外在奖励容易获得，那么仅优化外在奖励可能会得到更好的结果。然而，大多数情况下，我们事先不知道如何最优地在内在奖励和外在奖励之间做出权衡。

通常的做法是进行广泛的超参数搜索，以找到最佳的$λ$值，因为不同的λ值适用于不同的任务（见图4）。此外，随着智能体在任务上的进步，最佳的探索-利用权衡可能会变化，一个恒定的$λ$可能在整个训练过程中并不是最优的。在训练的初期阶段，可能更倾向于探索。一旦智能体能够获得一些任务奖励，它可能更倾向于利用这些奖励而不是进一步探索。探索-利用权衡的确切动态是任务依赖的，而且针对每个任务进行调整既繁琐、不受欢迎，通常也是计算上不可行的。因此，先前的工作在训练期间使用固定的$λ$，我们的实验揭示这是次优的。

我们提出了一种优化策略，减轻了在训练过程中手动调整外在奖励和内在奖励相对重要性的需要。我们的方法在内在奖励有助于探索时利用其偏差，并在内在奖励不帮助积累更高外在奖励时减轻这种偏差。这是通过使用一个外在最优性约束来实现的，<font color="red">该约束强制在优化混合目标后获得的外在奖励等于仅最大化外在奖励的最优策略积累的外在奖励。在一般设置中强制执行外在最优性约束是不可行的，因为最优的外在奖励是未知的。</font>我们设计了一种称为外在-内在策略优化（EIPO）的实用算法，该算法使用近似方法解决这个约束优化问题（第3节）

> 本文段落讨论了在强化学习中，如何平衡探索和利用的问题。传统上，需要对超参数λ进行广泛搜索，以找到适合特定任务的最佳值。然而，随着训练的进行，最佳的探索-利用平衡可能会改变，因此一个固定的λ值可能不是最佳选择。作者指出，针对每个任务调整λ既费时又不实际。为了解决这个问题，作者提出了一种新的优化策略，即外在-内在策略优化（EIPO），<font color="red">它通过一个外在最优性约束来自动调整外在和内在奖励的相对重要性，从而避免了手动调整的需要。这种方法在探索有助于提高性能时利用内在奖励的偏差，并在探索不再有益时减少这种偏差。</font>

<font color="green">简而言之，文中提出的方法其实就是：如果我们使用了内在奖励引导智能体进行探索。那么针对不同的环境，我们需要给予智能体不同的内在奖励权重来鼓励它访问新颖状态。对于较为简单就能探索到外在奖励状态的环境，不需要将内在奖励权重设置的过大(不要将$\lambda$设置的太大)，而对于难以探索到外在奖励的环境，则需要给出一个较大的权重来促使其更多的通过内在奖励访问新颖状态。 然而，这样的方法意味着针对不同的环境我们需要具体的调整$\lambda$。因此文中提出了一种方法，为$\lambda$的大小设置了一条约束：当智能体获得混合奖励（外在奖励+内在奖励）时，确保混合奖励中外在奖励的部分等同于最大化外在奖励策略时获得的奖励相等。在不牺牲外在奖励的前提下，利用内在奖励促进探索。</font>

原则上我们可以将EIPO应用于任何内在奖励方法，但我们使用最先进的随机网络提炼（RND）[9]来评估性能。图1b展示了在两款ATARI游戏上的初步结果：（i）Montezuma's Revenge - 其中与RND（红色）联合优化的结果显著优于仅使用外在奖励优化的PPO策略[13]（黑色）；（ii）James Bond - 其中PPO策略显著优于RND。这些结果加强了这样一个观点：内在奖励引入的偏差在某些游戏中有帮助，但在其他游戏中则有害。我们的算法EIPO（蓝色）在两款游戏中均匹敌最佳算法，显示出它可以根据需要利用内在奖励。在61款ATARI游戏上的结果加强了这一发现。此外，在某些游戏中，EIPO的表现超过了多个强大的基线，无论是否使用内在奖励，这表明我们的方法不仅可以减轻由内在奖励偏差引起的潜在性能下降，还可以提升性能，超越当前的最先进水平。

## 相关工作

考虑一个由状态空间$S$，行为空间$A$以及外在奖励函数$R^E:S \times A\rightarrow\Bbb{R}$组成的离散时间马尔科夫决策过程，分别使用$E$和$I$表示外在奖励和内在奖励分量。外在奖励函数指的是实际任务模板（例如游戏得分等）。

智能体从初始状态分布$\rho_0:S\rightarrow\Bbb{R}$采样的初始状态$s_0$开始，在每个时间步$t$，智能体从环境中得到一个状态$s_t$，在策略$\pi$中中采样行动$a_t$并执行动作，并通过状态转移函数$\tau(s_{t+1}|s_t,a_t)$转移到下一个状态$s_{t+1}$。智能体的目标是利用与环境的交互作用找到最优策略$\pi$，使得外在目标奖励最大化：
$$
\begin{align}
\max_{\pi}J^E(\pi),\quad where\ J^E(\pi)=\Bbb{E}_{s_0,a_0,...,\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t}r_t^{E}]
\\
s_0\sim \rho_0,a_t\sim\pi(a|s_t),s_{t+1}\sim\tau(s_{t+1}|s_t,a_t)\ \forall t>0
\end{align}
\tag{1}
$$
而结合了内在奖励的探索策略通过提供内在奖励鼓励智能体探索，内在奖励会鼓励智能体访问新颖状态，利用内在奖励函数$R^I:S\times A \rightarrow \Bbb{R}$，优化的目标函数为:
$$
\max_{\pi \in \Pi}J_{E+I}(\pi),\quad where\ J_{E+I}(\pi)=\Bbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma^t(r_t^E+\lambda r_t^I)]
\tag{2}
$$
其中，$\lambda$是内在奖励权重。最先进的基于内在奖励的探索策略通常使用近端策略优化(PPO)来优化该式中的目标函数。

## 减轻内在奖励的偏差

根据上式$(2)$，简单的最大化外在奖励和内在奖励的总和并不能使得一个策略也最大化外在奖励，即:
$$
argmax_{\pi_{E+I}\in\Pi}J_{E+I}(\pi_{E+I})\neq argmax_{\pi_{E}\in\Pi}J_{E}(\pi_{E})
$$
对于混合奖励目标函数下的最优策略$\pi_{E+I}^{*}=argmax_{\pi_{E+I}}J_{E+I}(\pi_{E+I})$而言，它可能是目标函数$J_E$的次优策略。

由于智能体的表现最终仅由外在奖励来衡量，因此我们提出一个强制性的外在最优约束，来保证混合奖励下的最优策略$\pi_{E+I}^{*}=argmax_{\pi_{E+I}}J_{E+I}(\pi_{E+I})$与最优的外在奖励策略$\pi_{E}^{*}=argmax_{\pi_{E}}J_{E}(\pi_{E})$产生的奖励一样多。即：
$$
\begin{align}
\max_{\pi_{E+I}\in\Pi}&J_{E+I}(\pi_{E+I}) \\
subject\ to\ &J_E(\pi_{E+I})-\max_{\pi_E}J_{E}(\pi_E)=0 \quad(extrinsic\ optimality\ constraint)
\end{align}
\tag{3}
$$
该优化问题的求解可以看作是提出了一个使$J_{E+I}$最大化的策略$\pi_{E+I}$，然后在给定外部最优性约束的情况下检验所提出的$\pi_{E+I}$是否可行。

<font color="red">然而，这一目标函数是难以求解的，因为评估外在最优性约束要求我们知道$\max_{\pi_E}J_{E}(\pi_E)$，但是这正是我们要求解的。</font>

### 对偶目标函数：无约束最小-最大化问题

要将上述带约束的优化问题转换为无约束问题，我们可以引入拉格朗日乘子（Lagrange multiplier），这样可以将约束条件整合到目标函数中。对于给定的问题，我们可以定义拉格朗日函数 ( L ) 如下：
$$
L(\pi_{E+I}, \lambda) = J_{E+I}(\pi_{E+I}) + \lambda \left( J_E(\pi_{E+I}) - \max_{\pi_E}J_{E}(\pi_E) \right)
$$
其中，$\lambda$ 是拉格朗日乘子。无约束的优化问题就是找到 $\pi_{E+I}$ 和 $\lambda$使得 $L$最大化。

对于对偶问题，我们首先需要定义对偶函数 $g(\lambda)$ 为原始拉格朗日函数$L$ 关于$\pi_{E+I}$的最大值:
$$
[ g(\lambda) = \max_{\pi_{E+I}\in\Pi} L(\pi_{E+I}, \lambda) ]
$$
然后，对偶问题就是最小化对偶函数$g(\lambda) $关于 $\lambda$：
$$
\min_{\lambda \geq 0} g(\lambda)
$$
这是因为原始问题是一个最大化问题，所以对偶问题是一个最小化问题。