---
title: "EXPLORATION BY RANDOM NETWORK DISTILLATION -- note"
author: Yu Xia
date: May 14, 2023
output: pdf_document
---
# RND笔记
## RND概述
强化学习关注的核心问题是，智能体如何通过环境给出的奖励信号更新策略来获取最大累计奖励。
而我上一篇看过的ICM论文中指出，在有些现实决策问题中，奖励可能是稀疏的，甚至完全没有奖励。为了引导智能体高效的探索状态和行为空间，我们需要设计一个特别的探索机制。
ICM的思想就是，我们引入了一个内在奖励(intrinsic reward)机制，通过基于预测误差的方式来衡量一个智能体对于状态-行为对的了解程度，或者说这个状态行为对的新颖性，来给予智能体一定的奖励。
区别于环境给出的外在奖励(extrinsic reward)，内在奖励的获取是由智能体的探索机制所给出的。因此我们也用这样的方式来区分外在奖励和内在奖励。
我们希望通过增加了额外的内在奖励来实现下面两种目的:
1. 对状态空间的探索，激励智能体探索更多的新颖状态(novel state)。
2. 对状态动作空间的探索，激励智能体执行有利于减少环境不确定性的动作（需要评估状态行为对$(s,a)$的新颖性）。

<font color="red">RND算法对应的则是第一种目的，即激励智能体探索更多的新颖状态。</font>
首先，我们需要给出新颖性的定义:
- 对于某个状态$s$，在智能体之前访问过的所有状态中，如果与$s$类似的状态数量越少，我们就称状态$s$越新颖。
- 某个状态$s$越新颖，通常对应智能体对状态$s$的认知不够充分，需要智能体在之后与环境交互时，更多的探索状态$s$的邻近区域。因此，这种特别设计的探索机制就会给予该状态更大的内在奖励。

那么，状态$s$的新颖性如何度量呢？
一类经典方法是使用状态在某个预测问题（通常是监督学习问题）上的预测误差来度量新颖性。根据在监督学习中神经网络拟合数据集的特性，如果智能体在某个状态$s$上的预测误差越大，近似的说明状态$s$附近的状态空间上智能体之前的访问次数少，从而说明状态$s$的新颖性较大。
例如之前我们所看到的ICM算法，就是通过学习一个前向动力学模型(或者说一个状态转移函数)，根据神经网络在给定状态行为对的条件下，对于下一状态$s_{t+1}$的预测误差来衡量一个状态行为对的新颖性。
但是ICM存在的问题在于:
1. 在大规模问题上，环境的前向动力学模型很复杂，加上神经网络的容量有限，导致在状态行为空间上某些区域访问次数很大时，预测误差仍然可能很大。
2. 在有些环境上，环境的状态转移函数是随机函数，例如包含了noisy-tv属性的环境，智能体不可能通过正常的神经网络准确的预测下一状态。

---
以下是论文笔记部分

## 摘要
我们为深度强化学习方法引入了一种探索奖励，该奖励由一个固定随机初始化的神经网络给出的观测特征的误差来计算。
文中还介绍了一种灵活结合内在奖励和外在奖励的方法，称为随机网络蒸馏（RND）奖励。

## 引言
首先仍然是说明，当实际环境中的奖励稀疏时，智能体难以通过采取随机行动序列来找到这些奖励。
强化学习中的最新发展似乎表明了，如果要解决最具有挑战性的任务，需要处理通过并行运行多个环境副本并获得大量的经验样本。因此我们希望引入能够匹配大量经验的探索方法。
然而，基于预测误差最大化进行探索的智能体往往会被预测问题的答案来自于输入的一个随机函数所吸引。例如，如果预测问题是在给定当前观测以及智能体行为的情况下预测下一个观测结果，那么试图最大化该预测误差的智能体将倾向于寻找随机变换。
这样的观察结果促使人们采用量化预测的相对改进而不是绝对误差的方法。
而文中提出了一种替代解决的方案，通过使用预测问题定义探索奖励，答案是输入的确定性函数。即在当前观测值的基础上预测一个固定的随机初始化神经网络的输出。

## 方法
### 探索奖励
探索奖励是一种鼓励智能体进行探索的方法，即使在环境奖励较少的情况下。他们的做法是使用一个新的奖励$r_t=e_t+i_t$来替代$e_t$,其中$i_t$是与时间$t$的转换相关的探索奖励。
为了鼓励智能体探索新的状态，新的状态的$i_t$要高于经常访问的状态。基于计数的探索方法就是这样的奖励的一个例子，将$i_t$定义为一个状态访问计数$n(s)$的递减函数。例如$i_t=\frac{1}{n_t(s)}$等。但是如果在连续状态空间的情况下，则需对其进行推广，使用状态密度估计的变化来作为探索奖励。这样的话可以得出关于所有状态的密度计数，即使是对于那些没有访问过但是访问过近似状态的状态。
另一种方式就是将$i_t$定义为与智能体转换相关的问题的预测误差，这类问题的一般例子就包括我们之前所看到的ICM。

### 随机网络蒸馏
本文介绍了一种不同的方法，其中预测问题是随机生成的。这涉及到两个神经网络:<font color="red">
1. 一个固定的和随机初始化的目标网络，它设置预测问题。
2. 一个预测网络，由智能体收集的数据进行训练。

</font>

目标网络将观测映射到一个空间$f:\mathcal{O} \rightarrow \Bbb R^k$，而预测网络$\hat{f}: \mathcal{O} \rightarrow \Bbb R^k$通过梯度下降训练，使得关于参数$\theta_{\hat{f}}的$MSE$||\hat{f}(x;\theta)-f(x)||$最小化。
这个过程将一个随机初始化的神经网络提炼成一个训练好的神经网络，对于与预测器所训练的状态不同的新状态，预测误差预计会更高。

#### 预测误差的来源
通常来说，预测误差可能来自于以下因素:
1. 训练数据的数量，当预测者很少看到类似的例子时（认知不确定性），预测误差很高。
2. 特性转换。由于目标函数是随机的（任意不确定性），预测误差很高。随机转换是前向动力学预测误差的一个来源。
3. 模型表达能力不足。预测误差很高，因为缺少必要的信息，或者模型类太有限，无法适应目标函数的复杂性。
4. 模型的学习过程困难。由于训练所用的数据是动态产生的，或是优化方法不够好，很容易收敛到当前minibatch的局部最优，导致具体使用时的预测误差很高。

因素1使得允许人们使用预测误差作为探索奖励。在实践中预测误差由所有这些因素组合引起，并不是这些因素都是可取的。
RND消除了因素2和3，因为目标网络可以被选择为确定性的，并且在预测网络的模型类内。