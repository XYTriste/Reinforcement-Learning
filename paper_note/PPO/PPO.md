# PPO算法

## 简述（By ChatGPT）

PPO（Proximal Policy Optimization）是一种策略优化方法，它的主要思想是在优化策略时，限制新策略与旧策略之间的差距，以此来防止策略更新过程中的过度更新问题。

在强化学习中，我们的目标是找到一个策略π，使得累积奖励的期望值最大。在策略梯度方法中，我们通过计算策略的梯度并沿着梯度方向更新策略来实现这个目标。然而，这种方法可能会导致策略更新过程中的过度更新问题，即新的策略可能会偏离旧的策略太远，导致性能下降。

为了解决这个问题，PPO引入了一个目标函数，该函数在优化策略时添加了一个限制条件，即新策略与旧策略之间的KL散度不能超过一个预设的阈值。这样，我们就可以在保证策略改进的同时，防止策略更新过程中的过度更新。

PPO的目标函数可以表示为：
$$
L^{CLIP}(\theta) = \hat{E}_t[min(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
$$
其中，$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$ 是新旧策略的比率，$\hat{A}_t$ 是优势函数，clip函数用于限制$r_t(\theta)$的范围在$[1-\epsilon, 1+\epsilon]$之间。

通过这种方式，PPO能够有效地平衡策略改进和策略稳定性之间的关系，从而在各种任务中都能取得良好的性能。

## PPO论文

### 摘要

文中提出了一种新的策略梯度方法用于强化学习，通过用于环境的交互来交替采样数据并使用随机梯度上升来优化智能体的目标函数。标准的策略梯度方法对每个数据样本执行一次梯度更新，但是文中提出了一个新的目标函数，可以实现多个小批量更新。称为近端策略优化(Proximal policy optimization, PPO)，它包含了一些信任域策略优化(Trust region policy optimization, TRPO)的一些优点。但是它们更容易实现，更通用并具有更好的样本复杂性。

### 引言

近年来人们提出了几种利用神经网络函数逼近器进行强化学习的方法，例如DQN、Vanilla 策略梯度算法以及信任区域/自然 策略梯度算法等等。然而，在开发大型模型和并行实现、数据效率和鲁棒性的方法方面仍然具有改进的空间。带有函数逼近的Q learning算法在许多简单问题上表现不佳，并且难以理解。Vanilla策略梯度方法的数据效率和鲁棒性则较低。信任域策略优化(TRPO)则相对复杂，并且与包含噪声(例如dropout)或参数共享(策略与值函数或辅助任务之间)的架构不兼容。

文中引入一种算法来改善现状，在仅使用一阶优化的情况下，实现了TRPO的数据效率和可靠性能。文中提出了一个具有剪裁概率比的新目标，形成了对策略性能的悲观估计（即下界）。为了优化策略，文中在从策略中采样数据和对采样数据执行若干次优化之间交替进行。

### 背景：策略梯度

策略梯度算法通过计算策略梯度的估计量并将其插入随机梯度上升法来工作，最常用的策略梯度估计量有：
$$
\hat{g}=\hat{\Bbb E}_t[\nabla_{\theta}\log{\pi_{\theta}(a_t|s_t)\hat{A}_t}]
$$
其中$\pi_{\theta}$是一个随机策略，$\hat{A}_{t}$是时间步$t$时优势函数的估计量。期望$\hat{\Bbb E}_{t}$表示在采样和优化之间交替的算法中，有限批样本的经验平均值。通过构造一个目标函数并使用深度学习框架中自动计算梯度的软件来进行工作，该目标函数的梯度是策略梯度估计量。通过微分该目标函数得到估计量：
$$
L^{PG}(\theta)=\hat{\Bbb{E}_t}[\log{\pi_{\theta}(a_t|s_t)}\hat{A_t}]
$$
虽然看上去使用相同的轨迹对该损失函数执行多个优化步骤很有好处，但是实际上是不合理的，且从经验上的角度来说它通常会导致破坏性的大幅度策略更新。从我个人理解的角度来说，如果使用相同的轨迹对策略进行多次更新，会让策略函数“误认为”这是一个足够好的策略。导致陷入一个局部最优解当中。DQN中引入经验回放的一个目的也是为了缓解样本之间的相关性问题，从而避免陷入局部最优解当中。

> 优势函数（Advantage Function）是在强化学习中使用的一个重要概念。它用于衡量某个动作相对于平均预期回报的优劣程度。
>
> 在强化学习中，代理（agent）需要通过与环境进行交互来学习最佳策略。代理在每个时间步选择一个动作执行，并观察环境的响应和获得的奖励。优势函数的目标是为代理提供每个动作的相对价值，以便能够进行策略改进。
>
> 优势函数通常定义为当前状态下执行某个动作的预期回报与状态的基准值（例如，状态的价值函数）之间的差异。它可以表示为以下形式：
>
> Advantage(s, a) = Q(s, a) - V(s)
>
> 其中，s表示当前状态，a表示执行的动作，Q(s, a)表示动作值函数（即在状态s下执行动作a的预期回报），而V(s)表示状态值函数（即在状态s下的预期回报）。
>
> 通过计算优势函数，代理可以评估每个动作相对于当前状态的优劣，并基于这些信息来选择最佳动作，从而改善其策略和决策过程。优势函数在许多强化学习算法中都发挥关键作用，例如，策略梯度方法中经常用于估计策略梯度辅助项。

### 信任区域方法

在TRPO中，目标函数(智能体的目标)在策略更新大小的约束下最大化，具体来说:
$$
maxmize_{\theta} \qquad \hat{\Bbb{E}_{t}}[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}}\hat{A_t}] \\
subject\quad to \quad \hat{\Bbb{E}_{t}}[KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]] \leqslant \delta
$$
其中，$\theta_{old}$是更新前的策略参数向量，在对目标函数进行线性逼近和对约束函数进行二次逼近后，利用共轭梯度算法可以有效的近似求解该问题。

> 共轭梯度算法是一种用于求解大型线性方程组的迭代方法。它在解决关于对称正定矩阵的线性方程组时表现出优良的性能。
>
> 共轭梯度算法的核心思想是通过一系列共轭方向来逐步逼近线性方程组的解。在每个迭代步骤中，共轭梯度算法利用梯度信息和前一步的搜索方向来构造一个新的搜索方向，使得新方向与前一步的搜索方向共轭。通过沿着这些共轭方向进行迭代，算法能够在较少的步骤内收敛到线性方程组的解。
>
> 共轭梯度算法的优点如下：
>
> - 内存效率：共轭梯度算法只需要存储一些向量，而不需要显式地构造矩阵。这使得算法适用于解决大规模线性方程组，节省了存储空间。
> - 迭代性能：共轭梯度算法通常收敛速度较快，特别适用于对称正定矩阵，表现出良好的迭代性能。
> - 无需直接求解矩阵：共轭梯度算法只需要通过矩阵向量乘法来计算子步骤中的向量乘积，而无需求解矩阵本身。
>
> 然而，共轭梯度算法并不适用于所有类型的线性方程组。对于非对称矩阵或非正定矩阵，共轭梯度算法的性能可能下降。此外，共轭梯度算法对于精确求解可能需要较多的迭代步骤。
>
> 总结而言，共轭梯度算法是一种用于求解对称正定线性方程组的迭代方法，具有内存效率和较快的收敛速度，但在适用性和精确性方面需注意其限制。

证明TRPO的理论实际上推荐使用惩罚而不是约束，即通过拉格朗日乘子将原问题转换为无约束优化问题：
$$
maxmize_{\theta} \quad \hat{\Bbb{E}_t}[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A_t}-\beta{KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]}]
$$

- $\theta$ 是我们希望优化的策略参数。
- $\hat{\Bbb{E}_t}$ 表示在时间步 \(t\) 上的经验采样期望。
- $\pi_{\theta}(a_t|s_t)$ 表示策略函数，给定状态 $s_t$，返回动作 $a_t$ 的概率。
- $\pi_{\theta_{old}}(a_t|s_t)$ 表示以旧策略参数（通常是上一次迭代的参数）为基础的策略函数。
- $\hat{A_t}$是在时间步 $t$ 上的优势函数估计值，它衡量在状态 $s_t$ 采取动作 $a_t$ 相对于平均奖励的优势。
- $\beta$ 是用于调节两个项之间权衡的系数。
- $KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]$ 是以 KL 散度的形式衡量新旧策略在状态 $s_t$ 上的差异。

整个表达式可以分为两部分：

第一部分 $\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A_t}$ 表示策略更新的优势项，它衡量了新策略在状态$s_t$ 采取动作 $a_t$ 相对于旧策略的优势估计值。这一项目标是最大化预期优势，并且比起上一种方法，将优势项除以旧策略的动作概率可以对策略更新进行缩放，以提高稳定性。

第二部分 $\beta{KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]}$ 是通过 KL 散度对新策略和旧策略间的差异进行惩罚。KL 散度衡量了两个概率分布之间的差异，这里衡量的是在状态 \(s_t\) 上新策略和旧策略产生动作的差异。通过引入惩罚项，TRPO算法可以控制策略更新的幅度，以确保更新尽可能小的幅度，从而保持收敛性和稳定性。

整个目标函数的目标是最大化策略的预期优势，同时限制策略更新的幅度以保持稳定性。通过调整$\beta$ 这个权衡系数以及优势项估计和KL散度的比例，可以平衡策略更新和收敛速度，并达到较好的性能。

> 个人理解：这里的无约束优化应该是使用了最优化中的类似外罚函数法的方式来将带约束的优化问题转换为无约束优化问题。罚函数法的主要思想是，将约束条件作为一个惩罚项引入到目标函数中，使其成为一个无约束的目标函数。当我们的取值在约束条件范围内（在该公式中表现为KL散度在我们设置好的范围内）时，惩罚项对目标函数的影响非常小。目标函数值大小依赖于原目标函数中的项。而当我们的取值在约束条件范围外时（在该公式中表现为KL散度超出我们设置范围），则惩罚项对目标函数的影响较大。控制目标函数的输出大小。

上述公式的成立，从直观的角度来说，源自于一个这样的事实：该优化问题中的最大KL散度在策略$\pi$的性能上形成了一个下界（即悲观界）。TRPO使用约束条件而不是惩罚，因此很难选择在不同问题中表现良好的单个$\beta$值，甚至很难在单个问题中选择，其中特征在学习过程中发生变化。文中经过实验表明，简单的选择固定的惩罚系数$\beta$并使用随机梯度下降惩罚目标方程是不够的，需要进行额外的修改。

### 截断代理目标

将$r_t(\theta)$记为新策略和旧策略下采取行动概率的比值$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$,因此有$r(\theta_{old})=1$。TRPO算法的目标是最大化以下代理目标：
$$
L^{CPI}(\theta)=\hat{\Bbb{E}_t}[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A_t}]=\hat{\Bbb E}_t[r_t(\theta)\hat{A}_t]
$$
其中，上标CPI指的是保守策略迭代。如果没有约束，$L^{CPI}$的最大化会导致过大的策略更新。文中提出一种方法，修改目标函数，以惩罚远离原目标函数的策略变化。公式如下:
$$
L^{CLIP}(\theta) = \hat{E}_t[min(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
$$
这是 **Proximal Policy Optimization（PPO）** 算法中的一个重要公式，即 PPO 的目标函数$ L^{CLIP}(\theta)$。下面是对这个公式的详细解释。

首先，我们分解并解释一下这个公式的每个部分：

- $L^{CLIP}(\theta)$ 是优化目标，也就是希望通过学习找到一个策略参数 $\theta$ 能最大化这个目标函数。
- $\hat{E}_t[]$ 表示的是对时间步 $t$ 上的期望，在强化学习中，智能体会在每个时间步骤采取行动，这是对这些行动后续结果的平均预期。
- $r_t(\theta)$ 是新策略和旧策略的比例，它告诉我们在新策略下采取行动的概率与在旧策略下采取行动的概率之比。
- $\hat{A}_t$ 是 Advantage 函数的估计。在强化学习中，优势函数衡量的是在状态 $s$ 下采取行动 $a$ 相对于平均行动的期望回报的优势。
- $clip(r_t(\theta), 1-\epsilon, 1+\epsilon)$ 是一个截断函数，将 $r_t(\theta)$ 的值限制在区间 $[1-\epsilon, 1+\epsilon]$ 内。

整个公式的含义可以理解为：

用于到达较好的策略更新，这种更新策略在 "足够小" 的更改上提供鼓励，即，如果新策略在优势函数为正（采取的行动比平均结果要好）的行动上产生更大的概率，那么它会得到奖励。相反，如果新策略在优势函数为负（采取的行动比平均结果更差）的行动上产生更大的概率，那么它会受到惩罚。

最后，$\min$ 操作则是在更新策略时提供了一种保护机制，防止策略更新的步长过大，造成学习的不稳定。

### 自适应KL惩罚系数

另一种可以作为截断代理目标的替代方法，是对KL散度使用惩罚，并调整惩罚系数，以便于我们在每次策略更新时达到KL散度的某个目标值。在实验中发现KL惩罚相比剪切代理目标的方式表现更差，但是它也是一个重要的baseline。

在该算法的最简单实例中，我们在每次更新时执行以下步骤:

- 利用小批量随机梯度下降的几个epoch，对KL惩罚目标进行优化:
  $$
  L^{KLPEN}(\theta)=\hat{\Bbb E}_t[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A_t}-\beta{KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]}]
  $$

- 计算$d=\hat{\Bbb E}_t[{KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]}]$
  - 如果$d<d_{target}/1.5$，$\beta \leftarrow \beta /2 $
  - 如果$d > d_{target}*1.5$，$\beta \leftarrow \beta * 2$

更新后的$\beta$用于下一次策略更新，使用这种方式我们偶尔会看到策略更新时KL偏离与$d_{target}$会有显著的不同。但是这些情况非常罕见，且$\beta$很快就会调整。$\beta$的初始值是另一个超参数，但在实践中并不重要。因为算法很快就会调整它。

### 算法

通过对于典型的策略梯度实现微小的更改，可以计算和区分前几节中的代理损失。对于使用自动微分的实现，只需要构造$L^{CLIP}$或$L^{KLPEN}$而不是$L^{PG}$，并且在此目标上执行多次随机梯度上升。

大部分计算方差减少优势函数估计的技术都使用学习的状态价值函数$V(s)$，例如，广义优势估计或有限视野估计。如果在使用策略和值函数之间共享参数的神经网络架构，则必须使用结合策略代理和值函数误差项的损失函数。这一目标可以通过添加熵奖励的方式来进一步的增强，以确保充分的探索。结合这些项，我们的目标是每次迭代最大化:
$$
L_{t}^{CLIP+VF+S}(\theta)=\hat{\Bbb{E}}_t[L_{t}^{CLIP}(\theta)-c_1L_t^{VF}(\theta)+c_2S[\pi_{\theta}](s_t)]
$$
其中，$c_1、c_2$均为系数，$S$表示熵增益，$L_t^{VF}$表示均方误差损失。

策略梯度实现的一种方式，非常适合与循环神经网络一起使用，它在$t$时刻时($t$远小于回合长度)运行策略并使用收集的样本进行更新。这种方式需要一个不超过时间步$t$的优势函数估计器：
$$
\hat{A}_t=-V(s_t)+r_t+\gamma{r_{t+1}}+...+\gamma^{T-t+1}r_{T-1}+\gamma^{T-t}V(s_T)
$$
其中，$t$是给定时间轨迹$[0,T]$内的时间下标，将这一选择一般化，我们可以使用截断版的广义优势估计，当$\lambda=1$时，可以简化为:
$$
\hat{A}_t=\delta_t+\gamma\lambda\delta_{t+1}+...+\gamma^{T-t+1}\delta_{T-1}\\
where \quad \delta_{t}=r_t+\gamma{V(s_{t+1})-V(s_t)}
$$
一个使用固定长度轨迹段的PPO算法如下所示，每次迭代，每个$N$(并行)参与者收集$T$个时间步长的数据。然后，我们在这些数据的$NT$个时间步上构建代理损失，并使用最小批量随机梯度下降对$K$个epochs进行优化。

![image](.\algorithm.png)