# PPO算法

## 简述（By ChatGPT）

PPO（Proximal Policy Optimization）是一种策略优化方法，它的主要思想是在优化策略时，限制新策略与旧策略之间的差距，以此来防止策略更新过程中的过度更新问题。

在强化学习中，我们的目标是找到一个策略π，使得累积奖励的期望值最大。在策略梯度方法中，我们通过计算策略的梯度并沿着梯度方向更新策略来实现这个目标。然而，这种方法可能会导致策略更新过程中的过度更新问题，即新的策略可能会偏离旧的策略太远，导致性能下降。

为了解决这个问题，PPO引入了一个目标函数，该函数在优化策略时添加了一个限制条件，即新策略与旧策略之间的KL散度不能超过一个预设的阈值。这样，我们就可以在保证策略改进的同时，防止策略更新过程中的过度更新。

PPO的目标函数可以表示为：
$$
L^{CLIP}(\theta) = \hat{E}_t[min(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
$$
其中，$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$ 是新旧策略的比率，$\hat{A}_t$ 是优势函数，clip函数用于限制$r_t(\theta)$的范围在$[1-\epsilon, 1+\epsilon]$之间。

通过这种方式，PPO能够有效地平衡策略改进和策略稳定性之间的关系，从而在各种任务中都能取得良好的性能。

## PPO论文

### 摘要

文中提出了一种新的策略梯度方法用于强化学习，通过用于环境的交互来交替采样数据并使用随机梯度上升来优化智能体的目标函数。标准的策略梯度方法对每个数据样本执行一次梯度更新，但是文中提出了一个新的目标函数，可以实现多个小批量更新。称为近端策略优化(Proximal policy optimization, PPO)，它包含了一些信任域策略优化(Trust region policy optimization, TRPO)的一些优点。但是它们更容易实现，更通用并具有更好的样本复杂性。

### 引言

近年来人们提出了几种利用神经网络函数逼近器进行强化学习的方法，例如DQN、Vanilla 策略梯度算法以及信任区域/自然 策略梯度算法等等。然而，在开发大型模型和并行实现、数据效率和鲁棒性的方法方面仍然具有改进的空间。带有函数逼近的Q learning算法在许多简单问题上表现不佳，并且难以理解。Vanilla策略梯度方法的数据效率和鲁棒性则较低。信任域策略优化(TRPO)则相对复杂，并且与包含噪声(例如dropout)或参数共享(策略与值函数或辅助任务之间)的架构不兼容。

文中引入一种算法来改善现状，在仅使用一阶优化的情况下，实现了TRPO的数据效率和可靠性能。文中提出了一个具有剪裁概率比的新目标，形成了对策略性能的悲观估计（即下界）。为了优化策略，文中在从策略中采样数据和对采样数据执行若干次优化之间交替进行。

### 背景：策略梯度

策略梯度算法通过计算策略梯度的估计量并将其插入随机梯度上升法来工作，最常用的策略梯度估计量有：
$$
\hat{g}=\hat{\Bbb E}_t[\nabla_{\theta}\log{\pi_{\theta}(a_t|s_t)\hat{A}_t}]
$$
其中$\pi_{\theta}$是一个随机策略，$\hat{A}_{t}$是时间步$t$时优势函数的估计量。期望$\hat{\Bbb E}_{t}$表示在采样和优化之间交替的算法中，有限批样本的经验平均值。通过构造一个目标函数并使用深度学习框架中自动计算梯度的软件来进行工作，该目标函数的梯度是策略梯度估计量。通过微分该目标函数得到估计量：
$$
L^{PG}(\theta)=\hat{\Bbb{E}_t}[\log{\pi_{\theta}(a_t|s_t)}\hat{A_t}]
$$
虽然看上去使用相同的轨迹对该损失函数执行多个优化步骤很有好处，但是实际上是不合理的，且从经验上的角度来说它通常会导致破坏性的大幅度策略更新。从我个人理解的角度来说，如果使用相同的轨迹对策略进行多次更新，会让策略函数“误认为”这是一个足够好的策略。导致陷入一个局部最优解当中。DQN中引入经验回放的一个目的也是为了缓解样本之间的相关性问题，从而避免陷入局部最优解当中。

> 优势函数（Advantage Function）是在强化学习中使用的一个重要概念。它用于衡量某个动作相对于平均预期回报的优劣程度。
>
> 在强化学习中，代理（agent）需要通过与环境进行交互来学习最佳策略。代理在每个时间步选择一个动作执行，并观察环境的响应和获得的奖励。优势函数的目标是为代理提供每个动作的相对价值，以便能够进行策略改进。
>
> 优势函数通常定义为当前状态下执行某个动作的预期回报与状态的基准值（例如，状态的价值函数）之间的差异。它可以表示为以下形式：
>
> Advantage(s, a) = Q(s, a) - V(s)
>
> 其中，s表示当前状态，a表示执行的动作，Q(s, a)表示动作值函数（即在状态s下执行动作a的预期回报），而V(s)表示状态值函数（即在状态s下的预期回报）。
>
> 通过计算优势函数，代理可以评估每个动作相对于当前状态的优劣，并基于这些信息来选择最佳动作，从而改善其策略和决策过程。优势函数在许多强化学习算法中都发挥关键作用，例如，策略梯度方法中经常用于估计策略梯度辅助项。

### 信任区域方法

在TRPO中，目标函数(智能体的目标)在策略更新大小的约束下最大化，具体来说:
$$
maxmize_{\theta} \qquad \hat{\Bbb{E}_{t}}[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}}\hat{A_t}] \\
subject\quad to \quad \hat{\Bbb{E}_{t}}[KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]] \leqslant \delta
$$
其中，$\theta_{old}$是更新前的策略参数向量，在对目标函数进行线性逼近和对约束函数进行二次逼近后，利用共轭梯度算法可以有效的近似求解该问题。

> 共轭梯度算法是一种用于求解大型线性方程组的迭代方法。它在解决关于对称正定矩阵的线性方程组时表现出优良的性能。
>
> 共轭梯度算法的核心思想是通过一系列共轭方向来逐步逼近线性方程组的解。在每个迭代步骤中，共轭梯度算法利用梯度信息和前一步的搜索方向来构造一个新的搜索方向，使得新方向与前一步的搜索方向共轭。通过沿着这些共轭方向进行迭代，算法能够在较少的步骤内收敛到线性方程组的解。
>
> 共轭梯度算法的优点如下：
>
> - 内存效率：共轭梯度算法只需要存储一些向量，而不需要显式地构造矩阵。这使得算法适用于解决大规模线性方程组，节省了存储空间。
> - 迭代性能：共轭梯度算法通常收敛速度较快，特别适用于对称正定矩阵，表现出良好的迭代性能。
> - 无需直接求解矩阵：共轭梯度算法只需要通过矩阵向量乘法来计算子步骤中的向量乘积，而无需求解矩阵本身。
>
> 然而，共轭梯度算法并不适用于所有类型的线性方程组。对于非对称矩阵或非正定矩阵，共轭梯度算法的性能可能下降。此外，共轭梯度算法对于精确求解可能需要较多的迭代步骤。
>
> 总结而言，共轭梯度算法是一种用于求解对称正定线性方程组的迭代方法，具有内存效率和较快的收敛速度，但在适用性和精确性方面需注意其限制。