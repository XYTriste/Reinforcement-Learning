---
title: "Reinforcement Learning alert"
author: Yu Xia
date: March 24, 2023
output: pdf_document
---

注意，策略$\pi$指的是，针对多个状态和行动，策略描述了某个状态下采取某个行动的概率。
它和状态转移概率矩阵的区别在于，状态转移概率矩阵**不涉及到行动**。
在马尔科夫决策过程中的状态价值函数$v_{\pi}(s)$，指的是状态$s$在遵循了策略${\pi}$的情况下的价值。但是需要注意的是，即使遵循了策略${\pi}$，得到的也是不止一条状态序列。因为策略描述的是**某状态下某行动发生的概率**，既然是概率，那就说明如果一个状态下不止一个行动可能，那就仍然会产生多条状态序列。
所以，对于状态价值函数$v_{\pi}(s)$我们有：
$$
\begin{align}
v_{\pi}(s) &= \Bbb E[G_t \mid S_t = s] \\
&=\Bbb E[R_{t + 1} + \gamma v(S_{t + 1}) \mid S_t = s] \\
&=\Bbb E[R_{t+1} \mid S_t = s] + \Bbb E[ \gamma v(S_{t + 1}) \mid S_t = s]
\end{align}
$$
其中$\Bbb E[R_{t+1} \mid S_t = s]$就是即时奖励的期望，这个很容易理解。假设我现在在状态$a$，采取的行动有$1、2、3$三种，发生的概率分别是$0.2,0.5,0.3$，采取这些行动获得的奖励分别是$-1, 2, 1$，那么即时奖励的期望就是$0.2 * (-1) + 0.5 * 2 + 0.3 * 1$
后面这部分$\Bbb E[ \gamma v(S_{t + 1}) \mid S_t = s]$其实挺好理解，既然确定了是由状态$s$开始的，那么下一步到达的$S_{t + 1}$则是有多种可能的，这里的期望$\Bbb E$就是对策略$\pi$下到达各个$S_{t + 1}$的概率进行加权求和。

---
行为价值函数的一般形式:
$$
q_{\pi}(s,a) = \Bbb E[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1}) \mid S_t=s,A_t=a]
$$
该函数意味着，**遵循某策略**的情况下的一个行为的价值，等于该行为获得的即时奖励加上后续行为奖励乘以折现率的和求期望。如上面所说，期望指的是该策略下该行为发生的概率。

根据上述描述，行为价值又可以分为:
$$
q_{\pi}(s,a) = R_{s}^{a}+\gamma \sum_{s^{'}\in S}P_{ss^{'}}^{a}v_{\pi}(s)
$$
说明遵循某策略的某行为的价值等于该行为获得的即时奖励加上采取该心动后到达另外状态的概率乘以该状态的奖励求和并乘以折现率。


因此，状态价值函数可以描述为：
$$
v_{\pi}(s) = \sum_{a \in A}{\pi(a\mid s)q_{\pi}(s,a)} 
$$
该函数意味着，一个状态的价值等于该状态下所有可能的行为的价值乘以它们发生的概率。

