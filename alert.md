---
title: "Reinforcement Learning alert"
author: Yu Xia
date: March 24, 2023
output: pdf_document
---

注意，策略$\pi$指的是，针对多个状态和行动，策略描述了某个状态下采取某个行动的概率。
它和状态转移概率矩阵的区别在于，状态转移概率矩阵**不涉及到行动**。
在马尔科夫决策过程中的状态价值函数$v_{\pi}(s)$，指的是状态$s$在遵循了策略${\pi}$的情况下的价值。但是需要注意的是，即使遵循了策略${\pi}$，得到的也是不止一条状态序列。因为策略描述的是**某状态下某行动发生的概率**，既然是概率，那就说明如果一个状态下不止一个行动可能，那就仍然会产生多条状态序列。
所以，对于状态价值函数$v_{\pi}(s)$我们有：
$$
\begin{align}
v_{\pi}(s) &= \Bbb E[G_t \mid S_t = s] \\
&=\Bbb E[R_{t + 1} + \gamma v(S_{t + 1}) \mid S_t = s] \\
&=\Bbb E[R_{t+1} \mid S_t = s] + \Bbb E[ \gamma v(S_{t + 1}) \mid S_t = s]
\end{align}
$$
其中$\Bbb E[R_{t+1} \mid S_t = s]$就是即时奖励的期望，这个很容易理解。假设我现在在状态$a$，采取的行动有$1、2、3$三种，发生的概率分别是$0.2,0.5,0.3$，采取这些行动获得的奖励分别是$-1, 2, 1$，那么即时奖励的期望就是$0.2 * (-1) + 0.5 * 2 + 0.3 * 1$
后面这部分$\Bbb E[ \gamma v(S_{t + 1}) \mid S_t = s]$其实挺好理解，既然确定了是由状态$s$开始的，那么下一步到达的$S_{t + 1}$则是有多种可能的，这里的期望$\Bbb E$就是对策略$\pi$下到达各个$S_{t + 1}$的概率进行加权求和。