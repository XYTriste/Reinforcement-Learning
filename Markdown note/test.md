DQN（Deep Q-Network）算法是一种基于深度神经网络的强化学习算法，旨在解决Q-learning算法在高维状态空间中面临的问题。DQN算法的流程如下：

定义输入状态$s$，动作$a$和输出$Q$值的深度神经网络$Q(s,a;\theta)$，其中$\theta$为神经网络的参数。
初始化经验回放缓冲区$D$，用于存储过去的经验。
对于每个时间步$t$，执行以下步骤：
以$\epsilon$-贪心策略从网络中获取动作$a_t$。
在环境中执行动作$a_t$，得到下一个状态$s_{t+1}$和奖励$r_t$。
将$(s_t, a_t, r_t, s_{t+1})$存储到经验回放缓冲区$D$中。
从$D$中随机抽取一个小批量的经验$(s_j, a_j, r_j, s_{j+1})$，并计算其目标Q值：
$$
\begin{cases}
r_j & \text{if } s_{j+1} \text{ is terminal},\\
r_j + \gamma \max_{a'}Q(s_{j+1}, a';\theta^-) & \text{otherwise},
\end{cases}$$
其中$\gamma$为折扣因子，$\theta^-$为目标网络的参数。
- 最小化均方误差损失函数：
$$\mathcal{L}(\theta) = \frac{1}{N}\sum_{j=1}^{N}(y_j - Q(s_j, a_j;\theta))^2$$
通过反向传播更新神经网络的参数$\theta$。
- 每$C$步将当前的参数$\theta$复制给目标网络的参数$\theta^-$。

其中，经验回放缓冲区的作用是为了让神经网络可以学习到之前的经验，以及避免连续的相关性影响学习效果。$\epsilon$-贪心策略可以平衡探索和利用之间的关系，其中$\epsilon$为探索概率。目标Q值的计算中，如果下一个状态$s_{j+1}$是终止状态，那么目标Q值就等于奖励$r_j$；否则目标Q值等于奖励加上下一步状态的最大Q值乘以折扣因子$\gamma$。由于DQN算法中采用了神经网络，导致目标函数不是凸的，因此使用随机梯度下降等优化算法进行参数更新。同时，为了进一步提高训练效率和稳定性，DQN算法还引入了目标网络，其参数


这个公式描述了当$k \rightarrow \infty$时，$k$步转移概率矩阵$P_{\pi}^{k}$收敛到一个稳态分布向量$W$的过程，并且这个稳态分布向量$W$与一个行向量$1_n d_{\pi}^T$的乘积相等。下面解释一下公式中的符号：

- $P_{\pi}^{k}$是$\pi$策略下，$k$步转移概率矩阵，其中$P_{\pi}^{k}(s, s')$表示在$\pi$策略下，从状态$s$出发$k$步后到达状态$s'$的概率。
- $W$是稳态分布向量，它是$k \rightarrow \infty$时$k$步转移概率矩阵$P_{\pi}^{k}$的极限。稳态分布向量是一个$n$维列向量，其中第$i$个元素$W_i$表示在$\pi$策略下，MDP处于状态$i$的概率。
- $1_n$是一个$n$维的全1列向量，$d_{\pi}$是一个$n$维的状态分布向量，它的每个元素$d_{\pi}(s)$表示在$\pi$策略下，MDP处于状态$s$的概率。

因此，$1_n d_{\pi}^T$实际上是一个$n \times n$的矩阵，其中每一行都等于$d_{\pi}^T$，即每个元素都是状态分布向量$d_{\pi}$。将$P_{\pi}^{k}$乘以$1_n d_{\pi}^T$的结果是一个$n$维的列向量，其中第$i$个元素表示在$\pi$策略下，从任意状态出发经过$k$步后到达状态$i$的概率，即$P_{\pi}^{k}(s, i)$的和。因此，$P_{\pi}^{k} \rightarrow W = 1_nd_{\pi}^T$意味着，当$k$趋近于无穷大时，$k$步转移概率矩阵$P_{\pi}^{k}$的行向量之和（即状态分布向量）收敛到一个稳态分布向量$W$，并且这个稳态分布向量$W$与一个行向量$1_n d_{\pi}^T$的乘积相等。

策略梯度算法的梯度形式可以表示为：
$$
\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log\pi_{\theta}(s,a) Q^{\pi_{\theta}}(s,a)\right]
$$

其中，$\pi_{\theta}$是策略函数，$\theta$是它的参数向量，$Q^{\pi_{\theta}}(s,a)$是策略$\pi_{\theta}$在状态$s$下采取动作$a$的收益期望，$\nabla_{\theta} \log\pi_{\theta}(s,a)$是策略函数的对数梯度。

这个公式的意义是，对于一个状态-动作对$(s, a)$，其对策略的贡献是由该状态-动作对的收益期望$Q^{\pi_{\theta}}(s,a)$和策略函数$\pi_{\theta}$在该状态-动作对处的对数梯度$\nabla_{\theta} \log\pi_{\theta}(s,a)$的乘积决定的。通过最大化这个期望，我们可以更新策略函数的参数，使得策略能够在当前状态下采取更优的动作。