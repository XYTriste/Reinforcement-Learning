# 神经网络
神经网络近似是一种非线性的近似，神经网络中的基本单元是一个可以进行非线性变化的神经元。通过多个这样的神经元多层排列，层间互连，最终实现复杂的非线性近似。
单个神经元在线性近似的基础上增加了一个偏置项$b$和一个非线性的整合函数$\sigma$，偏置项$b$可以认为是一个额外的数据为1的输入项的权重。单个神经元最终的输出$\hat{y}$为:
$$
\hat{y}=\sigma(z)=\sigma(w^Tx+b)
$$
非线性的整合函数$\sigma$被称为激活函数，最常用的两个家伙函数是$relu$和$tanh$。
神经网络则是由众多能够进行简单非线性近似的神经元按照一定的层次连接而成。
![avatar](./images/two%20layer%20full%20connect%20neural%20network.png)
上图是一个简单的2层神经网络架构，接受n个特征的输入数据，得到具有两个特征的输出。接收输入数据的第一层包含了16个神经元。<font color="red">输入数据可以被认为是一个神经元，但是它的输出是输入数据本身</font>，除了输入数据以外的每一个神经元都和前一层的所有神经元以一定的权重$w$进行连接，这种连接方式称为全连接，对应的神经网络为分层全连接神经网络。
分层全连接神经网络中，除了输入数据和输出层以外其他的层都称为隐藏层。显然在图中所示的神经网络中隐藏层是第一层，即包含了16个神经元的层。如果设置不同的隐藏层且隐藏层设置了不同数目的神经元，则形成了不同设置的神经网络。隐藏层越多，网络越深，其对数据的非线性近似能力越强。
对于一个多层神经网络来说，每一层的每一个神经元都有多个代表权重的参数$w$和一个偏置项$b$。如果用$n^{[l]}$表示第$l$层的神经元数量，$a_i^{[l]}$表示第$l$层的第$i$个神经元的输出，$w_{ji}^{[l]}$表示第$l$层第$j$个神经元与第$l-1$层的第$i$个神经元之间的链接权重，$b_j^{[l]}$表示第$l$层第$j$个神经元的偏置项，那么一个$L$层全连接神经网络，其参数由：
$$
<W^{[1]},b^{[1]}>,<W^{[2]},b^{[2]}>,...,<W^{[L]},b^{[L]}>
$$
其中$W^{[l]}$是一个$n^{[l]}×n^{[l-1]}$的二维矩阵，$b^{[l]}$是由$n^{[l]}$个元素构成的一维列向量。网络中第$l$层第$j$个神经元的输出$a_j^{[l]}$为:
$$
a_j^{[l]}=\sigma(z_j^{[l]})=\sigma(\sum_{i=1}^{n^{[l-1]}}{w_{ji}^{[l]}+b_j^{[l]}})
$$
如果使用矩阵的形式，那么第$l$层神经元的输出$a^{(l)}$为:
$$
a^{(l)}=\sigma(z^{[l]})=\sigma(W^{[l]}a^{[l-1]}+b^{[l]})
$$
神经网络中第$L$层的输出$a^{[L]}$也就是该网络的最终输出$\hat{y}$。当神经网络的参数确定时，给以神经网络的输入层一定的数据，通过第一个隐藏层的输出作为输入输入给第二个隐藏层，以此类推得到输出层一个确定的输出。这种由输入数据依次经过网络的各层得到输出的过程称为前向传播。
通过设计合理的目标函数，也可以利用梯度以及梯度下降算法来求解符合任务需求的参数。只不过计算梯度时需要从神经网络的输出层开始逐层计算值第一个隐藏层甚至是输入层。？