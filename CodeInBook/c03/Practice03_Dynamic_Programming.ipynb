{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习三 动态规划进行策略评估、策略迭代和价值迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 小型方格世界MDP建模"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4*4 方格状态命名\n",
    "# 状态0和15为终止状态\n",
    "#  0  1  2  3\n",
    "#  4  5  6  7\n",
    "#  8  9 10  11\n",
    "# 12 13 14  15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [i for i in range(16)] # 状态空间\n",
    "A = [\"n\", \"e\", \"s\", \"w\"] # 行为空间\n",
    "# P,R,将由dynamics动态生成\n",
    "\n",
    "ds_actions = {\"n\": -4, \"e\": 1, \"s\": 4, \"w\": -1} # 行为对状态的改变\n",
    "\n",
    "def dynamics(s, a): # 环境动力学\n",
    "    '''模拟小型方格世界的环境动力学特征\n",
    "    Args:\n",
    "        s 当前状态 int 0 - 15\n",
    "        a 行为 str in ['n','e','s','w'] 分别表示北、东、南、西\n",
    "    Returns: tuple (s_prime, reward, is_end)\n",
    "        s_prime 后续状态\n",
    "        reward 奖励值\n",
    "        is_end 是否进入终止状态\n",
    "\n",
    "    将s_prime首先初始化为s，方便进行状态更新。\n",
    "\n",
    "    首先判断是否碰到了边界，如果碰到边界则说明状态不能采取该动作。\n",
    "    如果没有碰到边界，则新的状态 = 旧的状态 + 行为改变的大小。\n",
    "    例如，旧的状态是状态2， 采取的行为是's'，那么就有:\n",
    "    s_prime = 2 + 4  (4是行为s对应的字典值)\n",
    "    那么s_prime就是6，正好对应了地图中的2向南移会到6.\n",
    "\n",
    "    如果当前状态是终止状态，那么行为获得的奖励就是0.否则是-1.\n",
    "    这里值得注意的是，如果当前状态是终止状态，前面的if else中的s_prime就不会\n",
    "    被更新了，也就是说到了终止状态，状态就不会再改变了。此时无论行为是什么，都不会采取\n",
    "    实际的行动，所以获得的奖励始终为0.\n",
    "\n",
    "    判断是否为终止状态，是的话is_end就是True，否则就是false。\n",
    "    返回:后续状态、行为的奖励、是否进入终止状态\n",
    "    '''\n",
    "    s_prime = s\n",
    "    if (s%4 == 0 and a == \"w\") or (s<4 and a == \"n\") \\\n",
    "        or ((s+1)%4 == 0 and a == \"e\") or (s > 11 and a == \"s\")\\\n",
    "        or s in [0, 15]:\n",
    "        pass\n",
    "    else:\n",
    "        ds = ds_actions[a]\n",
    "        s_prime = s + ds\n",
    "    reward = 0 if s in [0, 15] else -1\n",
    "    is_end = True if s in [0, 15] else False\n",
    "    return s_prime, reward, is_end\n",
    "\n",
    "def P(s, a, s1): # 状态转移概率函数\n",
    "    '''\n",
    "    根据环境动力学函数来计算转移概率\n",
    "    注意如果 dynamics(s, a) 中的状态s采取的行为a是可行的话（比如在状态5采取行为向\n",
    "    四个方向移动都是可行的），那么就一定会移动到新的状态且获得一个-1的奖励。\n",
    "    但是这里只计算概率，不关心奖励，所以用 _ 来占位表示得到了返回值但不使用。\n",
    "\n",
    "    返回s1 == s_prime是因为，在强化学习问题中一个状态s采取一个行为a能够转移到的\n",
    "    新状态可能是多个，比如我在学习中采取浏览手机的行为，可能会导致我接着学习状态，\n",
    "    也可能转移到继续浏览手机这样的状态。\n",
    "    而在这个迷宫中，我们在某个状态s采取行为a前往新的状态的概率是固定的。所以只有\n",
    "    在s1 == s_prime 时返回概率1， 其他都会返回0.\n",
    "    '''\n",
    "    s_prime, _, _ = dynamics(s, a)\n",
    "    return s1 == s_prime\n",
    "\n",
    "def R(s, a): # 奖励函数\n",
    "    '''\n",
    "    根据环境动力学函数得到某状态下采取行为的奖励.\n",
    "\n",
    "    只有两种可能，状态s非终止状态则返回-1，否则返回0.\n",
    "    所以其实这个函数可以改写成\n",
    "\n",
    "    return 0 if s in [0,15] else -1\n",
    "    '''\n",
    "    _, r, _ = dynamics(s, a)\n",
    "    return r\n",
    "\n",
    "gamma = 1.00\n",
    "MDP = S, A, R, P, gamma  #这里的R和P都是函数，将它们传给MDP实际上是传递了函数的地址。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_random_pi(MDP = None, V = None, s = None, a = None):\n",
    "    '''\n",
    "    均一随机策略,在任何状态下朝着能够前进的方向前进的概率均等。\n",
    "    '''\n",
    "    _, A, _, _, _ = MDP\n",
    "    n = len(A)\n",
    "    return 0 if n == 0 else 1.0/n\n",
    "\n",
    "def greedy_pi(MDP, V, s, a):\n",
    "    '''\n",
    "    贪心算法进行策略迭代，参数分别是马尔科夫决策过程的五个参数S,A,R,P,gamma\n",
    "    V 代表当前的状态价值，s和a分别表示状态和动作。\n",
    "    注：策略的更新是基于状态价值的，也就是说每一轮更新得到的策略都是基于当前\n",
    "    状态价值的最优策略。\n",
    "    某一状态下的最优策略，则是从该状态下所有可能的行为中采取行为价值最高的行为。\n",
    "    该行为可能不止一个。\n",
    "\n",
    "    首先将最大的状态价值初始化为无限小， 达到最大的状态价值对应的行为空间为空。\n",
    "\n",
    "    然后统计在当前状态下\n",
    "    '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    max_v, a_max_v = -float('inf'), []\n",
    "    for a_opt in A:# 统计后续状态的最大价值以及到达到达该状态的行为（可能不止一个）\n",
    "        s_prime, reward, _ = dynamics(s, a_opt)  #状态s采取行为a_opt后得到奖励并到达新的状态\n",
    "        v_s_prime = get_value(V, s_prime)   # 得到新的状态的状态价值\n",
    "        if v_s_prime > max_v:               # 如果新的状态的状态价值大于之前所找到的最大的状态价值\n",
    "            max_v = v_s_prime  #说明该行为是比之前的行为更优的，更新最大的后续最大状态价值\n",
    "            a_max_v = [a_opt]   # 将达到最大状态价值对应的行为设置为空\n",
    "        elif(v_s_prime == max_v):   # 达到最大状态价值的行为可能不止一个\n",
    "            a_max_v.append(a_opt)\n",
    "    n = len(a_max_v)\n",
    "    if n == 0: return 0.0\n",
    "    return 1.0/n if a in a_max_v else 0.0       # 更新策略\n",
    "\n",
    "def get_pi(Pi, s, a, MDP = None, V = None):\n",
    "    '''\n",
    "    这里的参数pi是一个具体的策略，该函数根据策略返回状态s下采取行为a的概率\n",
    "    '''\n",
    "    return Pi(MDP, V, s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数\n",
    "def get_prob(P, s, a, s1): # 获取状态转移概率\n",
    "    return P(s, a, s1)\n",
    "\n",
    "def get_reward(R, s, a): # 获取奖励值\n",
    "    return R(s, a)\n",
    "\n",
    "def set_value(V, s, v): # 设置价值字典\n",
    "    V[s] = v\n",
    "    \n",
    "def get_value(V, s): # 获取状态价值\n",
    "    return V[s]\n",
    "\n",
    "def display_V(V): # 显示状态价值\n",
    "    for i in range(16):\n",
    "        print('{0:>6.2f}'.format(V[i]),end = \" \")\n",
    "        if (i+1) % 4 == 0:\n",
    "            print(\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 策略评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q(MDP, V, s, a):\n",
    "    '''根据给定的MDP，价值函数V，计算状态行为对s,a的价值qsa\n",
    "    '''\n",
    "    S, A, R, P, gamma = MDP\n",
    "    q_sa = 0\n",
    "    for s_prime in S:\n",
    "        q_sa += get_prob(P, s, a, s_prime) * get_value(V, s_prime)\n",
    "    q_sa = get_reward(R, s,a) + gamma * q_sa\n",
    "    return q_sa\n",
    "\n",
    "def compute_v(MDP, V, Pi, s):\n",
    "    '''给定MDP下依据某一策略Pi和当前状态价值函数V计算某状态s的价值\n",
    "    '''\n",
    "    S, A, R, P, gamma = MDP\n",
    "    v_s = 0\n",
    "    for a in A:\n",
    "        v_s += get_pi(Pi, s, a, MDP, V) * compute_q(MDP, V, s, a)\n",
    "    return v_s        \n",
    "\n",
    "def update_V(MDP, V, Pi):\n",
    "    '''给定一个MDP和一个策略，更新该策略下的价值函数V\n",
    "    该函数每次都在一轮策略评估完成后调用，更新该策略下每个状态的\n",
    "    状态价值\n",
    "    '''\n",
    "    S, _, _, _, _ = MDP\n",
    "    V_prime = V.copy()\n",
    "    for s in S:\n",
    "        set_value(V_prime, s, compute_v(MDP, V_prime, Pi, s))\n",
    "    return V_prime\n",
    "\n",
    "\n",
    "def policy_evaluate(MDP, V, Pi, n):\n",
    "    '''使用n次迭代计算来评估一个MDP在给定策略Pi下的状态价值，初始时价值为V\n",
    "    '''\n",
    "    for i in range(n):\n",
    "        #print(\"====第{}次迭代====\".format(i+1))\n",
    "        V = update_V(MDP, V, Pi)\n",
    "        #display_V(V)\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00 -14.00 -20.00 -22.00 \n",
      "-14.00 -18.00 -20.00 -20.00 \n",
      "-20.00 -20.00 -18.00 -14.00 \n",
      "-22.00 -20.00 -14.00   0.00 \n",
      "\n",
      "  0.00  -1.00  -2.00  -3.00 \n",
      " -1.00  -2.00  -3.00  -2.00 \n",
      " -2.00  -3.00  -2.00  -1.00 \n",
      " -3.00  -2.00  -1.00   0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = [0  for _ in range(16)] # 状态价值\n",
    "V_pi = policy_evaluate(MDP, V, uniform_random_pi, 100)\n",
    "display_V(V_pi)\n",
    "\n",
    "V = [0  for _ in range(16)] # 状态价值\n",
    "V_pi = policy_evaluate(MDP, V, greedy_pi, 100)\n",
    "display_V(V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 策略迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iterate(MDP, V, Pi, n, m):\n",
    "    for i in range(m):\n",
    "        V = policy_evaluate(MDP, V, Pi, n)\n",
    "        Pi = greedy_pi # 第一次迭代产生新的价值函数后随机使用贪婪策略\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00  -1.00  -2.00  -3.00 \n",
      " -1.00  -2.00  -3.00  -2.00 \n",
      " -2.00  -3.00  -2.00  -1.00 \n",
      " -3.00  -2.00  -1.00   0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = [0  for _ in range(16)] # 重置状态价值\n",
    "V_pi = policy_iterate(MDP, V, greedy_pi, 1, 100)\n",
    "display_V(V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 价值迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 价值迭代得到最优状态价值过程\n",
    "def compute_v_from_max_q(MDP, V, s):\n",
    "    '''根据一个状态的下所有可能的行为价值中最大一个来确定当前状态价值\n",
    "    行为价值由行为获得的即时奖励和后续状态价值的期望获得。\n",
    "    选择了最大的行为价值意味着获得了最大的后续状态价值。\n",
    "    '''\n",
    "    S, A, R, P, gamma = MDP\n",
    "    v_s = -float('inf')\n",
    "    for a in A:\n",
    "        qsa = compute_q(MDP, V, s, a)\n",
    "        if qsa >= v_s:\n",
    "            v_s = qsa\n",
    "    return v_s\n",
    "\n",
    "def update_V_without_pi(MDP, V):\n",
    "    '''在不依赖策略的情况下直接通过后续状态的价值来更新状态价值\n",
    "    通过遍历所有状态下所有可能的行为价值，得到最大的作为状态价值\n",
    "    进行更新。\n",
    "    '''\n",
    "    S, _, _, _, _ = MDP\n",
    "    V_prime = V.copy()\n",
    "    for s in S:\n",
    "        set_value(V_prime, s, compute_v_from_max_q(MDP, V_prime, s))\n",
    "    return V_prime\n",
    "\n",
    "def value_iterate(MDP, V, n):\n",
    "    '''价值迭代\n",
    "    '''\n",
    "    for i in range(n):\n",
    "        V = update_V_without_pi(MDP, V)\n",
    "        display_V(V)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00   0.00   0.00   0.00 \n",
      "  0.00   0.00   0.00   0.00 \n",
      "  0.00   0.00   0.00   0.00 \n",
      "  0.00   0.00   0.00   0.00 \n",
      "\n",
      "  0.00  -1.00  -1.00  -1.00 \n",
      " -1.00  -1.00  -1.00  -1.00 \n",
      " -1.00  -1.00  -1.00  -1.00 \n",
      " -1.00  -1.00  -1.00   0.00 \n",
      "\n",
      "  0.00  -1.00  -2.00  -2.00 \n",
      " -1.00  -2.00  -2.00  -2.00 \n",
      " -2.00  -2.00  -2.00  -1.00 \n",
      " -2.00  -2.00  -1.00   0.00 \n",
      "\n",
      "  0.00  -1.00  -2.00  -3.00 \n",
      " -1.00  -2.00  -3.00  -2.00 \n",
      " -2.00  -3.00  -2.00  -1.00 \n",
      " -3.00  -2.00  -1.00   0.00 \n",
      "\n",
      "  0.00  -1.00  -2.00  -3.00 \n",
      " -1.00  -2.00  -3.00  -2.00 \n",
      " -2.00  -3.00  -2.00  -1.00 \n",
      " -3.00  -2.00  -1.00   0.00 \n",
      "\n",
      "  0.00  -1.00  -2.00  -3.00 \n",
      " -1.00  -2.00  -3.00  -2.00 \n",
      " -2.00  -3.00  -2.00  -1.00 \n",
      " -3.00  -2.00  -1.00   0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = [0  for _ in range(16)] # 重置状态价值\n",
    "display_V(V)\n",
    "\n",
    "V_star = value_iterate(MDP, V, 4)\n",
    "display_V(V_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(MDP, V, s):\n",
    "    '''\n",
    "    贪心策略的字符串版本\n",
    "    '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    max_v, a_max_v = -float('inf'), []\n",
    "    for a_opt in A:# 统计后续状态的最大价值以及到达到达该状态的行为（可能不止一个）\n",
    "        s_prime, reward, _ = dynamics(s, a_opt)\n",
    "        v_s_prime = get_value(V, s_prime)\n",
    "        if v_s_prime > max_v:\n",
    "            max_v = v_s_prime\n",
    "            a_max_v = a_opt\n",
    "        elif(v_s_prime == max_v):\n",
    "            a_max_v += a_opt\n",
    "    return str(a_max_v)\n",
    "\n",
    "def display_policy(policy, MDP, V):\n",
    "    S, A, P, R, gamma = MDP\n",
    "    for i in range(16):\n",
    "        print('{0:^6}'.format(policy(MDP, V, S[i])),end = \" \")\n",
    "        if (i+1) % 4 == 0:\n",
    "            print(\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nesw    w      w      sw   \n",
      "  n      nw    nesw    s    \n",
      "  n     nesw    es     s    \n",
      "  ne     e      e     nesw  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_policy(greedy_policy, MDP, V_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
